This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-10T21:37:48.755Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
langgraph.json
langstuff_multi_agent/__init__.py
langstuff_multi_agent/agent.py
langstuff_multi_agent/agents/analyst.py
langstuff_multi_agent/agents/coder.py
langstuff_multi_agent/agents/context_manager.py
langstuff_multi_agent/agents/debugger.py
langstuff_multi_agent/agents/general_assistant.py
langstuff_multi_agent/agents/life_coach.py
langstuff_multi_agent/agents/professional_coach.py
langstuff_multi_agent/agents/project_manager.py
langstuff_multi_agent/agents/researcher.py
langstuff_multi_agent/agents/supervisor.py
langstuff_multi_agent/config.py
langstuff_multi_agent/utils/tools.py
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
.env

================
File: langgraph.json
================
{
    "version": "1.0",
    "project": "LangGraph Multi-Agent AI",
    "description": "Configuration for deploying the LangGraph multi-agent AI project via LangGraph Studio.",
    "entry_point": "./langstuff_multi_agent/agent.py:graph",
    "graphs": {
      "main": "./langstuff_multi_agent/agent.py:graph"
    },
    "agents": [
      {
        "name": "DEBUGGER",
        "file": "agents/debugger.py",
        "description": "Agent responsible for debugging code."
      },
      {
        "name": "CONTEXT_MANAGER",
        "file": "agents/context_manager.py",
        "description": "Agent responsible for managing conversation context."
      },
      {
        "name": "PROJECT_MANAGER",
        "file": "agents/project_manager.py",
        "description": "Agent responsible for managing project timelines and tasks."
      },
      {
        "name": "PROFESSIONAL_COACH",
        "file": "agents/professional_coach.py",
        "description": "Agent providing professional and career guidance."
      },
      {
        "name": "LIFE_COACH",
        "file": "agents/life_coach.py",
        "description": "Agent providing lifestyle and personal advice."
      },
      {
        "name": "CODER",
        "file": "agents/coder.py",
        "description": "Agent that assists with coding tasks."
      },
      {
        "name": "ANALYST",
        "file": "agents/analyst.py",
        "description": "Agent specializing in data analysis."
      },
      {
        "name": "RESEARCHER",
        "file": "agents/researcher.py",
        "description": "Agent that gathers and summarizes research and news."
      },
      {
        "name": "GENERAL_ASSISTANT",
        "file": "agents/general_assistant.py",
        "description": "Agent for general queries and assistance."
      }
    ],
    "dependencies": [
      "langgraph>=0.0.20",
      "langchain-anthropic>=0.0.10",
      "langchain-core>=0.1.20",
      "langchain-openai>=0.0.5",
      "python-dotenv>=1.0.0",
      "tavily-python>=0.5.1",
      "langchain_community>=0.3.17",
      "./langstuff_multi_agent"
    ],
    "configuration": {
      "xai_api_key": "${XAI_API_KEY}",
      "anthropic_api_key": "${ANTHROPIC_API_KEY}",
      "openai_api_key": "${OPENAI_API_KEY}",
      "tavily_api_key": "${TAVILY_API_KEY}",
      "default_model": "grok-2-1212",
      "default_temperature": 0.4,
      "checkpointer": "MemorySaver",
      "logging": {
        "level": "INFO",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
      }
    }
  }

================
File: langstuff_multi_agent/__init__.py
================
"""
LangStuff Multi-Agent package.
A multi-agent system built with LangGraph for handling various 
specialized tasks.
"""

__version__ = "0.1.0"

================
File: langstuff_multi_agent/agent.py
================
# agent.py
"""
Main agent module that exports the graph for LangGraph Studio.
"""

from langstuff_multi_agent.agents.supervisor import supervisor_workflow

# Export the compiled graph for LangGraph Studio
graph = supervisor_workflow.compile()

================
File: langstuff_multi_agent/agents/analyst.py
================
# agents/analyst.py
"""
Analyst Agent module for data analysis and interpretation.

This module provides a workflow for analyzing data and performing
calculations using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    calc_tool
)
from langchain_anthropic import ChatAnthropic


analyst_workflow = StateGraph(MessagesState)

# Define tools for analysis tasks
tools = [search_web, python_repl, calc_tool]
tool_node = ToolNode(tools)

# Bind the LLM with analytical tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node for data analysis with a detailed system prompt
analyst_workflow.add_node(
    "analyze_data",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are an Analyst Agent. Your task is to analyze data, perform calculations, and interpret results.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Retrieve background information and data.\n"
                            "- python_repl: Run Python code to perform calculations and tests.\n"
                            "- calc_tool: Execute specific calculations and numerical analysis.\n\n"
                            "Instructions:\n"
                            "1. Review the data or query provided by the user.\n"
                            "2. Perform necessary calculations and analyze the results.\n"
                            "3. Summarize your findings in clear, concise language."
                        ),
                    }
                ]
            )
        ]
    },
)
analyst_workflow.add_node("tools", tool_node)

# Define control flow edges
analyst_workflow.add_edge(START, "analyze_data")

# Add conditional edge from analyze_data to either tools or END
analyst_workflow.add_conditional_edges(
    "analyze_data",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to analyze_data
analyst_workflow.add_edge("tools", "analyze_data")

================
File: langstuff_multi_agent/agents/coder.py
================
# agents/coder.py
"""
Coder Agent module for writing and improving code.

This module provides a workflow for code generation, debugging,
and optimization using various development tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, read_file, write_file
from langchain_anthropic import ChatAnthropic

coder_workflow = StateGraph(MessagesState)

# Define tools for coding tasks
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)

# Bind the LLM with the coding tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node for coding with a system prompt that guides code writing and debugging
coder_workflow.add_node(
    "code",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Coder Agent. Your task is to write, debug, and improve code.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Find coding examples and documentation.\n"
                            "- python_repl: Execute and test Python code snippets.\n"
                            "- read_file: Retrieve code from files.\n"
                            "- write_file: Save code modifications to files.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code or coding request.\n"
                            "2. Provide solutions, test code, and explain your reasoning.\n"
                            "3. Use the available tools to execute code and verify fixes as necessary."
                        ),
                    }
                ]
            )
        ]
    },
)
coder_workflow.add_node("tools", tool_node)

# Define control flow edges
coder_workflow.add_edge(START, "code")

# Add conditional edge from code to either tools or END
coder_workflow.add_conditional_edges(
    "code",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to code
coder_workflow.add_edge("tools", "code")

================
File: langstuff_multi_agent/agents/context_manager.py
================
# agents/context_manager.py
"""
Context Manager Agent module for tracking conversation context.

This module provides a workflow for managing conversation history
and maintaining context across interactions.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, read_file, write_file
from langchain_anthropic import ChatAnthropic

context_manager_workflow = StateGraph(MessagesState)

# Define tools for context management
tools = [search_web, read_file, write_file]
tool_node = ToolNode(tools)

# Bind the LLM with tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node that manages context with a system prompt
context_manager_workflow.add_node(
    "manage_context",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Context Manager Agent. Your task is to track and manage conversation context.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Keep track of key information and topics discussed in the conversation.\n"
                            "2. Summarize important points and decisions made.\n"
                            "3. Use read_file and write_file to store and retrieve context information.\n"
                            "4. If necessary, use search_web to gather additional context.\n"
                            "5. Ensure that the conversation stays focused and relevant."
                        ),
                    }
                ]
            )
        ]
    },
)
context_manager_workflow.add_node("tools", tool_node)

# Define the control flow edges
context_manager_workflow.add_edge(START, "manage_context")

# Add conditional edge from manage_context to either tools or END
context_manager_workflow.add_conditional_edges(
    "manage_context",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to manage_context
context_manager_workflow.add_edge("tools", "manage_context")

================
File: langstuff_multi_agent/agents/debugger.py
================
# agents/debugger.py
"""
Debugger Agent module for analyzing code and identifying errors.

This module provides a workflow for debugging code using various tools
and LLM-based analysis.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    read_file,
    write_file
)
from langchain_anthropic import ChatAnthropic

debugger_workflow = StateGraph(MessagesState)

# Define the tools available to the Debugger Agent
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)

# Define the LLM and bind the tools to it
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main agent node with a system prompt detailing the agent's role and instructions
debugger_workflow.add_node(
    "analyze_code",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Debugger Agent. Your task is to identify and analyze code errors.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- python_repl: Execute Python code.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code and identify potential errors.\n"
                            "2. Use the search_web tool to find relevant information about the error or related debugging techniques.\n"
                            "3. Use the python_repl tool to execute code snippets and test potential fixes.\n"
                            "4. If necessary, use read_file and write_file to modify the code.\n"
                            "5. Provide clear and concise explanations of the error and the debugging process."
                        ),
                    }
                ]
            )
        ]
    },
)
debugger_workflow.add_node("tools", tool_node)

# Define the control flow edges:
# 1. Start at 'analyze_code'.
# 2. If any message contains tool_calls, transition from 'analyze_code' to 'tools'.
# 3. After running tools, loop back to 'analyze_code'.
# 4. If no tool_calls remain, finish.
debugger_workflow.add_edge(START, "analyze_code")

# Add conditional edge from analyze_code to either tools or END
debugger_workflow.add_conditional_edges(
    "analyze_code",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to analyze_code
debugger_workflow.add_edge("tools", "analyze_code")

# The debugger_workflow is now complete.
# :contentReference[oaicite:0]{index=0}

================
File: langstuff_multi_agent/agents/general_assistant.py
================
# agents/general_assistant.py
"""
General Assistant Agent module for handling diverse queries.

This module provides a workflow for addressing general user requests
using a variety of tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather
from langchain_anthropic import ChatAnthropic

general_assistant_workflow = StateGraph(MessagesState)

# Define general assistant tools
tools = [search_web, get_current_weather]
tool_node = ToolNode(tools)

# Bind the LLM with the general assistant tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with a system prompt for general assistance
general_assistant_workflow.add_node(
    "assist",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a General Assistant Agent. Your task is to assist with a variety of general queries and tasks.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Provide general information and answer questions.\n"
                            "- get_current_weather: Retrieve current weather updates.\n\n"
                            "Instructions:\n"
                            "1. Understand the user's request.\n"
                            "2. Use the available tools to gather relevant information when needed.\n"
                            "3. Provide clear, concise, and helpful responses to assist the user."
                        ),
                    }
                ]
            )
        ]
    },
)
general_assistant_workflow.add_node("tools", tool_node)

# Define control flow edges
general_assistant_workflow.add_edge(START, "assist")

# Add conditional edge from assist to either tools or END
general_assistant_workflow.add_conditional_edges(
    "assist",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to assist
general_assistant_workflow.add_edge("tools", "assist")

================
File: langstuff_multi_agent/agents/life_coach.py
================
# agents/life_coach.py
"""
Life Coach Agent module for personal advice and guidance.

This module provides a workflow for offering lifestyle tips and
personal development advice using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather, calendar_tool
from langchain_anthropic import ChatAnthropic

life_coach_workflow = StateGraph(MessagesState)

# Define tools for life coaching
tools = [search_web, get_current_weather, calendar_tool]
tool_node = ToolNode(tools)

# Bind the LLM with tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with instructions for personal and lifestyle advice
life_coach_workflow.add_node(
    "life_coach",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Life Coach Agent. Your task is to provide personal advice and lifestyle tips.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up general lifestyle tips and motivational content.\n"
                            "- get_current_weather: Provide weather updates to help plan outdoor activities.\n"
                            "- calendar_tool: Assist in scheduling and planning daily routines.\n\n"
                            "Instructions:\n"
                            "1. Listen to the user's personal queries and lifestyle challenges.\n"
                            "2. Offer practical advice and motivational support.\n"
                            "3. Use the available tools to supply additional context when necessary.\n"
                            "4. Maintain an empathetic and encouraging tone throughout the conversation."
                        ),
                    }
                ]
            )
        ]
    },
)
life_coach_workflow.add_node("tools", tool_node)

# Define control flow edges
life_coach_workflow.add_edge(START, "life_coach")

# Add conditional edge from life_coach to either tools or END
life_coach_workflow.add_conditional_edges(
    "life_coach",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to life_coach
life_coach_workflow.add_edge("tools", "life_coach")

================
File: langstuff_multi_agent/agents/professional_coach.py
================
# agents/professional_coach.py
"""
Professional Coach Agent module for career guidance.

This module provides a workflow for offering career advice and
job search strategies using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, job_search_tool
from langchain_anthropic import ChatAnthropic

professional_coach_workflow = StateGraph(MessagesState)

# Define the tools for professional coaching
tools = [search_web, job_search_tool]
tool_node = ToolNode(tools)

# Bind the LLM with the available tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with a system prompt for career guidance
professional_coach_workflow.add_node(
    "coach",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Professional Coach Agent. Your task is to provide career advice and job search strategies.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search for career advice and job market trends.\n"
                            "- job_search_tool: Retrieve job listings and career opportunities.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's career-related queries.\n"
                            "2. Offer actionable advice and strategies for job searching.\n"
                            "3. Use the available tools to provide up-to-date information and resources.\n"
                            "4. Communicate in a supportive and motivational tone."
                        ),
                    }
                ]
            )
        ]
    },
)
professional_coach_workflow.add_node("tools", tool_node)

# Define control flow edges
professional_coach_workflow.add_edge(START, "coach")

# Add conditional edge from coach to either tools or END
professional_coach_workflow.add_conditional_edges(
    "coach",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to coach
professional_coach_workflow.add_edge("tools", "coach")

================
File: langstuff_multi_agent/agents/project_manager.py
================
# agents/project_manager.py
"""
Project Manager Agent module for task and timeline management.

This module provides a workflow for overseeing project schedules
and coordinating tasks using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    calendar_tool,
    task_tracker_tool
)
from langchain_anthropic import ChatAnthropic

project_manager_workflow = StateGraph(MessagesState)

# Define project management tools
tools = [search_web, calendar_tool, task_tracker_tool]
tool_node = ToolNode(tools)

# Bind the LLM with tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node for managing projects with detailed instructions
project_manager_workflow.add_node(
    "manage_project",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Project Manager Agent. Your task is to oversee project timelines, tasks, and scheduling.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for project management best practices.\n"
                            "- calendar_tool: Access and update project calendars.\n"
                            "- task_tracker_tool: Manage and update project task lists.\n\n"
                            "Instructions:\n"
                            "1. Review project details and timelines.\n"
                            "2. Update project schedules and task lists as needed.\n"
                            "3. Use search_web for additional project management information.\n"
                            "4. Provide clear instructions and updates regarding project progress."
                        ),
                    }
                ]
            )
        ]
    },
)
project_manager_workflow.add_node("tools", tool_node)

# Define control flow edges
project_manager_workflow.add_edge(START, "manage_project")

# Add conditional edge from manage_project to either tools or END
project_manager_workflow.add_conditional_edges(
    "manage_project",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to manage_project
project_manager_workflow.add_edge("tools", "manage_project")

================
File: langstuff_multi_agent/agents/researcher.py
================
# agents/researcher.py
# This file defines the Researcher Agent workflow.
# The Researcher Agent gathers and summarizes news and research information.
# It uses tools such as search_web and news_tool.
# The agent is powered by ChatAnthropic (Claude‑2) and uses a ToolNode for research tasks.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, news_tool
from langchain_anthropic import ChatAnthropic

researcher_workflow = StateGraph(MessagesState)

# Define research tools
tools = [search_web, news_tool]
tool_node = ToolNode(tools)

# Bind the LLM with research tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with instructions for conducting research
researcher_workflow.add_node(
    "research",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Researcher Agent. Your task is to gather and summarize news and research information.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up recent information and background data.\n"
                            "- news_tool: Retrieve the latest news headlines and articles.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's research query.\n"
                            "2. Use the available tools to gather accurate and relevant information.\n"
                            "3. Provide a clear summary of your findings."
                        ),
                    }
                ]
            )
        ]
    },
)
researcher_workflow.add_node("tools", tool_node)

# Define control flow edges
researcher_workflow.add_edge(START, "research")

# Add conditional edge from research to either tools or END
researcher_workflow.add_conditional_edges(
    "research",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to research
researcher_workflow.add_edge("tools", "research")

================
File: langstuff_multi_agent/agents/supervisor.py
================
# agents/supervisor.py
"""
Supervisor Agent module for integrating and routing individual LangGraph 
agent workflows.

This module uses an LLM—instantiated via the get_llm() factory function 
from langstuff_multi_agent/config.py—to classify incoming user requests 
and dynamically route the request to the appropriate specialized agent 
workflow. The available agents include:
  DEBUGGER, CONTEXT_MANAGER, PROJECT_MANAGER, PROFESSIONAL_COACH, 
  LIFE_COACH, CODER, ANALYST, RESEARCHER, and GENERAL_ASSISTANT.

Each agent workflow is compiled with persistent checkpointing enabled 
by explicitly passing the shared checkpointer 
(Config.PERSISTENT_CHECKPOINTER) during compilation.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langstuff_multi_agent.config import Config, get_llm

# Import individual workflows.
from langstuff_multi_agent.agents.debugger import (
    debugger_workflow
)
from langstuff_multi_agent.agents.context_manager import (
    context_manager_workflow
)
from langstuff_multi_agent.agents.project_manager import (
    project_manager_workflow
)
from langstuff_multi_agent.agents.professional_coach import (
    professional_coach_workflow
)
from langstuff_multi_agent.agents.life_coach import (
    life_coach_workflow
)
from langstuff_multi_agent.agents.coder import (
    coder_workflow
)
from langstuff_multi_agent.agents.analyst import (
    analyst_workflow
)
from langstuff_multi_agent.agents.researcher import (
    researcher_workflow
)
from langstuff_multi_agent.agents.general_assistant import (
    general_assistant_workflow
)

# Create supervisor workflow
supervisor_workflow = StateGraph(MessagesState)

# Define the available agent options with descriptions
AGENT_OPTIONS = {
    "DEBUGGER": "Code debugging and error analysis",
    "CONTEXT_MANAGER": "Conversation context tracking and management",
    "PROJECT_MANAGER": "Project timeline and task management",
    "PROFESSIONAL_COACH": "Career advice and job search strategies",
    "LIFE_COACH": "Personal development and lifestyle guidance",
    "CODER": "Code writing and improvement",
    "ANALYST": "Data analysis and interpretation",
    "RESEARCHER": "Information gathering and research",
    "GENERAL_ASSISTANT": "General purpose assistance"
}

# Map agent names to their workflows
workflow_map = {
    "DEBUGGER": debugger_workflow,
    "CONTEXT_MANAGER": context_manager_workflow,
    "PROJECT_MANAGER": project_manager_workflow,
    "PROFESSIONAL_COACH": professional_coach_workflow,
    "LIFE_COACH": life_coach_workflow,
    "CODER": coder_workflow,
    "ANALYST": analyst_workflow,
    "RESEARCHER": researcher_workflow,
    "GENERAL_ASSISTANT": general_assistant_workflow
}

# Get supervisor LLM
supervisor_llm = get_llm()

def format_agent_list():
    """Format the agent list with descriptions for the prompt."""
    return "\n".join(f"- {name}: {desc}" for name, desc in AGENT_OPTIONS.items())

def route_request(state):
    """Route the user request to appropriate agent workflow with enhanced feedback."""
    messages = state.get("messages", [])
    if not messages:
        return {
            "messages": [
                AIMessage(content="Welcome! I'm your AI assistant. How can I help you today?")
            ]
        }
    
    request = messages[-1].content
    
    # Create a detailed prompt for agent selection
    prompt = (
        "You are a Supervisor Agent tasked with routing user requests to the most appropriate specialized agent.\n\n"
        "Available agents and their specialties:\n"
        f"{format_agent_list()}\n\n"
        f"User Request: '{request}'\n\n"
        "Instructions:\n"
        "1. Analyze the user's request carefully\n"
        "2. Select the most appropriate agent based on their specialties\n"
        "3. Respond with exactly one agent name from the list (case-insensitive)\n"
        "4. If unsure, use GENERAL_ASSISTANT\n\n"
        "Selected agent:"
    )
    
    try:
        # Get agent selection
        response = supervisor_llm.invoke([SystemMessage(content=prompt)])
        agent_key = response.content.strip().upper()
        
        if agent_key not in AGENT_OPTIONS:
            agent_key = "GENERAL_ASSISTANT"
        
        # Compile and invoke the selected workflow
        workflow = workflow_map[agent_key].compile()
        result = workflow.invoke({"messages": [
            SystemMessage(content=f"You are the {agent_key} agent, specialized in {AGENT_OPTIONS[agent_key]}."),
            HumanMessage(content=request)
        ]})
        
        # Add routing information to the response
        return {
            "messages": [
                AIMessage(content=f"[Routing to {agent_key} - {AGENT_OPTIONS[agent_key]}]\n\n"),
                *result["messages"]
            ]
        }
        
    except Exception as e:
        return {
            "messages": [
                AIMessage(content=f"I apologize, but I encountered an error while processing your request: {str(e)}\n\n"
                                "Please try rephrasing your request or contact support if the issue persists.")
            ]
        }

# Add the routing node
supervisor_workflow.add_node("route", route_request)

# Define edges
supervisor_workflow.add_edge(START, "route")
supervisor_workflow.add_edge("route", END)

# Export the workflow
__all__ = ["supervisor_workflow"]

================
File: langstuff_multi_agent/config.py
================
# config.py
"""
Configuration settings for the LangGraph multi-agent AI project.

This file reads critical configuration values from environment variables,
provides default settings, initializes logging, sets up a persistent checkpoint
instance using MemorySaver, and exposes configuration and factory functions for LangGraph.

Supported providers:
  - "anthropic": Uses ChatAnthropic with the key from ANTHROPIC_API_KEY.
  - "openai": Uses ChatOpenAI with the key from OPENAI_API_KEY.
  - "grok" (or "xai"): Uses ChatOpenAI as an interface to Grok with the key from XAI_API_KEY.
"""

import os
import logging
from langgraph.checkpoint.memory import MemorySaver
from typing import Optional, Dict, Any
from typing_extensions import TypedDict
from langchain_core.messages import SystemMessage
from langchain_core.runnables.config import RunnableConfig

# Import provider libraries.
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel


class ConfigSchema(TypedDict):
    """Schema for LangGraph runtime configuration."""
    model: Optional[str]
    system_message: Optional[str]
    temperature: Optional[float]
    top_p: Optional[float]
    max_tokens: Optional[int]


class Config:
    # API Keys
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    XAI_API_KEY = os.environ.get("XAI_API_KEY")

    # Default model settings
    DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "gpt-4-turbo-preview")
    DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", 0))
    DEFAULT_PROVIDER = os.environ.get("AI_PROVIDER", "openai").lower()

    # Model configurations
    MODEL_CONFIGS = {
        "anthropic": {
            "model_name": "claude-3-opus-20240229",
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        },
        "openai": {
            "model_name": "gpt-4-turbo-preview",
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        },
        "grok": {
            "model_name": "grok-1",
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        }
    }

    # Logging configuration
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Persistent checkpointer instance
    PERSISTENT_CHECKPOINTER = MemorySaver()

    @classmethod
    def init_logging(cls):
        """Initialize logging with configured settings."""
        logging.basicConfig(level=cls.LOG_LEVEL, format=cls.LOG_FORMAT)
        logging.info("Logging initialized at level: %s", cls.LOG_LEVEL)

    @classmethod
    def get_api_key(cls, provider: str) -> str:
        """Get the API key for the specified provider."""
        key_map = {
            "openai": ("OPENAI_API_KEY", cls.OPENAI_API_KEY),
            "anthropic": ("ANTHROPIC_API_KEY", cls.ANTHROPIC_API_KEY),
            "grok": ("XAI_API_KEY", cls.XAI_API_KEY),
        }
        
        env_var, key = key_map.get(provider, (None, None))
        if not key:
            raise ValueError(f"{env_var} environment variable not set")
        return key


# Initialize logging immediately
Config.init_logging()


def get_model_instance(provider: str, **kwargs) -> BaseChatModel:
    """Create a model instance for the specified provider with optional overrides."""
    config = Config.MODEL_CONFIGS[provider].copy()
    config.update(kwargs)
    
    if provider == "anthropic":
        return ChatAnthropic(
            api_key=Config.get_api_key("anthropic"),
            **config
        )
    elif provider in ["openai", "grok"]:
        return ChatOpenAI(
            api_key=Config.get_api_key("openai"),
            **config
        )
    else:
        raise ValueError(f"Unsupported provider: {provider}")


def get_llm(
    config: Optional[Dict[str, Any]] = None,
) -> BaseChatModel:
    """
    Factory function to create a language model instance based on configuration.
    
    Args:
        config: Optional configuration dictionary that can include:
               - model: Provider name ("anthropic", "openai", "grok")
               - system_message: Optional system message to prepend
               - temperature: Temperature parameter for generation
               - top_p: Top-p parameter for generation
               - max_tokens: Maximum tokens to generate

    Returns:
        An instance of BaseChatModel configured according to the specified parameters
    """
    config = config or {}
    provider = config.get("model", Config.DEFAULT_PROVIDER)
    
    model_kwargs = {
        k: v for k, v in config.items() 
        if k in ["temperature", "top_p", "max_tokens", "model_name"]
    }
    
    return get_model_instance(provider, **model_kwargs)


def create_model_config(
    model: Optional[str] = None,
    system_message: Optional[str] = None,
    **kwargs
) -> RunnableConfig:
    """
    Create a RunnableConfig for LangGraph workflow configuration.
    
    Args:
        model: Optional model provider to use
        system_message: Optional system message to prepend
        **kwargs: Additional configuration parameters
        
    Returns:
        RunnableConfig with the specified configuration
    """
    config = {"model": model} if model else {}
    if system_message:
        config["system_message"] = system_message
    config.update(kwargs)
    
    return {"configurable": config}

================
File: langstuff_multi_agent/utils/tools.py
================
# langstuff_multi_agent/utils/tools.py
"""
This module defines various utility tools for the LangGraph multi-agent AI.
Each tool is decorated with @tool from langchain_core.tools for compatibility
with LangGraph. The tools include:

  - search_web: Simulates a web search.
  - python_repl: Simulates executing Python code.
  - read_file: Reads file contents.
  - write_file: Writes content to a file.
  - calendar_tool: Simulates calendar updates.
  - task_tracker_tool: Simulates task tracking updates.
  - job_search_tool: Simulates job search results.
  - get_current_weather: Simulates current weather information.
  - calc_tool: Evaluates a mathematical expression.
  - news_tool: Simulates retrieving news headlines.
"""

from langchain_core.tools import tool


@tool
def search_web(query: str) -> str:
    """
    Simulates a web search for the given query.

    :param query: The search query.
    :return: A simulated search result.
    """
    return f"Simulated search result for query: '{query}'."


@tool
def python_repl(code: str) -> str:
    """
    Simulates executing Python code.

    :param code: Python code as a string.
    :return: Simulated output from executing the code.
    """
    try:
        # WARNING: In production, executing code via eval/exec can be dangerous.
        # Here, we simulate code execution without running untrusted code.
        return f"Simulated execution output for code: '{code}'."
    except Exception as e:
        return f"Error during simulated code execution: {str(e)}"


@tool
def read_file(filepath: str) -> str:
    """
    Reads the content of a file.

    :param filepath: The path to the file.
    :return: The file's content or an error message.
    """
    try:
        with open(filepath, 'r') as file:
            return file.read()
    except Exception as e:
        return f"Error reading file '{filepath}': {str(e)}"


@tool
def write_file(params: dict) -> str:
    """
    Writes content to a file.

    Expects a dictionary with the keys:
      - 'filepath': The path to the file.
      - 'content': The content to write.

    :param params: Dictionary containing file path and content.
    :return: Success message or an error message.
    """
    try:
        filepath = params.get("filepath")
        content = params.get("content", "")
        with open(filepath, 'w') as file:
            file.write(content)
        return f"Successfully wrote to '{filepath}'."
    except Exception as e:
        return f"Error writing to file '{filepath}': {str(e)}"


@tool
def calendar_tool(event_details: str) -> str:
    """
    Simulates updating a calendar with event details.

    :param event_details: Details of the event.
    :return: Confirmation message.
    """
    return f"Calendar updated with event: {event_details}"


@tool
def task_tracker_tool(task_details: str) -> str:
    """
    Simulates updating a task tracker with task details.

    :param task_details: Details of the task.
    :return: Confirmation message.
    """
    return f"Task tracker updated with task: {task_details}"


@tool
def job_search_tool(query: str) -> str:
    """
    Simulates a job search based on the given query.

    :param query: The job search query.
    :return: Simulated job listings.
    """
    return f"Simulated job listings for query: '{query}'."


@tool
def get_current_weather(location: str) -> str:
    """
    Simulates retrieving current weather information for a given location.

    :param location: The location for which to retrieve weather.
    :return: Simulated weather details.
    """
    return f"Simulated weather for {location}: 75°F, Sunny."


@tool
def calc_tool(expression: str) -> str:
    """
    Evaluates a simple mathematical expression.

    :param expression: A string containing the mathematical expression.
    :return: The result as a string or an error message.
    """
    try:
        # Evaluate the expression safely using a restricted namespace.
        result = eval(expression, {"__builtins__": {}})
        return str(result)
    except Exception as e:
        return f"Error evaluating expression: {str(e)}"


@tool
def news_tool(topic: str) -> str:
    """
    Simulates retrieving news headlines for a given topic.

    :param topic: The news topic.
    :return: Simulated news headlines.
    """
    return f"Simulated news headlines for topic: '{topic}'."

================
File: requirements.txt
================
langgraph>=0.0.20
langchain-anthropic>=0.0.10
langchain-core>=0.1.20
langchain-openai>=0.0.5
python-dotenv>=1.0.0
tavily-python>=0.5.1
langchain_community>=0.3.17
