This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-19T14:41:41.103Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
=0.1
langgraph.json
langstuff_multi_agent/__init__.py
langstuff_multi_agent/agent.py
langstuff_multi_agent/agents/analyst.py
langstuff_multi_agent/agents/coder.py
langstuff_multi_agent/agents/context_manager.py
langstuff_multi_agent/agents/creative_content.py
langstuff_multi_agent/agents/customer_support.py
langstuff_multi_agent/agents/debugger.py
langstuff_multi_agent/agents/financial_analyst.py
langstuff_multi_agent/agents/general_assistant.py
langstuff_multi_agent/agents/life_coach.py
langstuff_multi_agent/agents/marketing_strategist.py
langstuff_multi_agent/agents/news_reporter.py
langstuff_multi_agent/agents/professional_coach.py
langstuff_multi_agent/agents/project_manager.py
langstuff_multi_agent/agents/researcher.py
langstuff_multi_agent/agents/supervisor.py
langstuff_multi_agent/config.py
langstuff_multi_agent/utils/memory.py
langstuff_multi_agent/utils/tools.py
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
.env
.venv
venv

================
File: =0.1
================
Collecting langgraph
  Downloading langgraph-0.2.72-py3-none-any.whl.metadata (17 kB)
Collecting langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 (from langgraph)
  Downloading langchain_core-0.3.35-py3-none-any.whl.metadata (5.9 kB)
Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)
  Downloading langgraph_checkpoint-2.0.13-py3-none-any.whl.metadata (4.6 kB)
Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)
  Using cached langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)
Collecting langsmith<0.4,>=0.1.125 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Downloading langsmith-0.3.8-py3-none-any.whl.metadata (14 kB)
Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)
Collecting jsonpatch<2.0,>=1.33 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)
Collecting PyYAML>=5.3 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)
Collecting packaging<25,>=23.2 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)
Collecting typing-extensions>=4.7 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting pydantic<3.0.0,>=2.7.4 (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)
Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)
  Using cached msgpack-1.1.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)
Collecting httpx>=0.25.2 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)
Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached orjson-3.10.15-cp312-cp312-win_amd64.whl.metadata (42 kB)
Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)
Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)
Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)
Collecting idna (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)
Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)
Collecting requests<3,>=2 (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)
Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached zstandard-0.23.0-cp312-cp312-win_amd64.whl.metadata (3.0 kB)
Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)
Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)
Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph)
  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)
Collecting sniffio>=1.1 (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)
  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Downloading langgraph-0.2.72-py3-none-any.whl (150 kB)
   ---------------------------------------- 150.2/150.2 kB 2.3 MB/s eta 0:00:00
Downloading langchain_core-0.3.35-py3-none-any.whl (413 kB)
   ---------------------------------------- 413.2/413.2 kB 6.4 MB/s eta 0:00:00
Downloading langgraph_checkpoint-2.0.13-py3-none-any.whl (38 kB)
Using cached langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)
Using cached httpx-0.28.1-py3-none-any.whl (73 kB)
Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)
Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)
Downloading langsmith-0.3.8-py3-none-any.whl (332 kB)
   --------------------------------------- 332.8/332.8 kB 20.2 MB/s eta 0:00:00
Using cached msgpack-1.1.0-cp312-cp312-win_amd64.whl (75 kB)
Using cached orjson-3.10.15-cp312-cp312-win_amd64.whl (133 kB)
Using cached packaging-24.2-py3-none-any.whl (65 kB)
Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)
Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)
Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)
Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)
Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)
Using cached zstandard-0.23.0-cp312-cp312-win_amd64.whl (495 kB)
Using cached anyio-4.8.0-py3-none-any.whl (96 kB)
Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)
Using cached h11-0.14.0-py3-none-any.whl (58 kB)
Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)
   ---------------------------------------- 128.4/128.4 kB 7.4 MB/s eta 0:00:00
Installing collected packages: zstandard, urllib3, typing-extensions, tenacity, sniffio, PyYAML, packaging, orjson, msgpack, jsonpointer, idna, h11, charset-normalizer, certifi, annotated-types, requests, pydantic-core, jsonpatch, httpcore, anyio, requests-toolbelt, pydantic, httpx, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph
Successfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.8.0 certifi-2025.1.31 charset-normalizer-3.4.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.35 langgraph-0.2.72 langgraph-checkpoint-2.0.13 langgraph-sdk-0.1.51 langsmith-0.3.8 msgpack-1.1.0 orjson-3.10.15 packaging-24.2 pydantic-2.10.6 pydantic-core-2.27.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 typing-extensions-4.12.2 urllib3-2.3.0 zstandard-0.23.0

================
File: langgraph.json
================
{
    "version": "1.0",
    "project": "LangGraph Multi-Agent AI",
    "description": "Configuration for deploying the LangGraph multi-agent AI project via LangGraph Studio.",
    "entry_point": "langstuff_multi_agent/agent.py:graph",
    "graphs": {
        "main": "langstuff_multi_agent/agent.py:graph"
    },
    "agents": [
        {
            "name": "SUPERVISOR",
            "file": "langstuff_multi_agent/agent.py",
            "graph": "main",
            "description": "Main supervisor agent that routes requests to specialized agents."
        },
        {
            "name": "DEBUGGER",
            "file": "langstuff_multi_agent/agents/debugger.py",
            "graph": "main",
            "description": "Agent responsible for debugging code, managed by the supervisor."
        },
        {
            "name": "CONTEXT_MANAGER",
            "file": "langstuff_multi_agent/agents/context_manager.py",
            "graph": "main",
            "description": "Agent responsible for managing conversation context, managed by the supervisor."
        },
        {
            "name": "PROJECT_MANAGER",
            "file": "langstuff_multi_agent/agents/project_manager.py",
            "graph": "main",
            "description": "Agent responsible for managing project timelines and tasks, managed by the supervisor."
        },
        {
            "name": "PROFESSIONAL_COACH",
            "file": "langstuff_multi_agent/agents/professional_coach.py",
            "graph": "main",
            "description": "Agent providing professional and career guidance, managed by the supervisor."
        },
        {
            "name": "LIFE_COACH",
            "file": "langstuff_multi_agent/agents/life_coach.py",
            "graph": "main",
            "description": "Agent providing lifestyle and personal advice, managed by the supervisor."
        },
        {
            "name": "CODER",
            "file": "langstuff_multi_agent/agents/coder.py",
            "graph": "main",
            "description": "Agent that assists with coding tasks, managed by the supervisor."
        },
        {
            "name": "ANALYST",
            "file": "langstuff_multi_agent/agents/analyst.py",
            "graph": "main",
            "description": "Agent specializing in data analysis, managed by the supervisor."
        },
        {
            "name": "RESEARCHER",
            "file": "langstuff_multi_agent/agents/researcher.py",
            "graph": "main",
            "description": "Agent that gathers and summarizes research and news, managed by the supervisor."
        },
        {
            "name": "GENERAL_ASSISTANT",
            "file": "langstuff_multi_agent/agents/general_assistant.py",
            "graph": "main",
            "description": "Agent for general queries and assistance, managed by the supervisor."
        },
        {
            "name": "NEWS_REPORTER",
            "file": "langstuff_multi_agent/agents/news_reporter.py",
            "graph": "main",
            "description": "Agent that searches, researches, explains and summarizes news reports, managed by the supervisor."
        },
        {
            "name": "CUSTOMER_SUPPORT",
            "file": "langstuff_multi_agent/agents/customer_support.py",
            "graph": "main",
            "description": "Agent that provides customer support and assistance, managed by the supervisor."
        },
        {
            "name": "MARKETING_STRATEGIST",
            "file": "langstuff_multi_agent/agents/marketing_strategist.py",
            "graph": "main",
            "description": "Agent that provides marketing strategy, insights, trends, and planning, managed by the supervisor."
        },
        {
            "name": "CREATIVE_CONTENT",
            "file": "langstuff_multi_agent/agents/creative_content.py",
            "graph": "main",
            "description": "Agent that provides creative content, marketing copy, social media posts, or brainstorming ideas, managed by the supervisor."
        },
        {
            "name": "FINANCIAL_ANALYST",
            "file": "langstuff_multi_agent/agents/financial_analyst.py",
            "graph": "main",
            "description": "Agent that provides financial analysis, market data, forecasting, and investment insights, managed by the supervisor."
        }
    ],
    "dependencies": [
        "langgraph>=0.0.20",
        "langchain-anthropic>=0.0.10",
        "langchain-core>=0.1.20",
        "langchain-openai>=0.0.5",
        "python-dotenv>=1.0.0",
        "tavily-python>=0.5.1",
        "langchain_community>=0.3.17",
        "./langstuff_multi_agent"
    ],
    "configuration": {
        "xai_api_key": "${XAI_API_KEY}",
        "anthropic_api_key": "${ANTHROPIC_API_KEY}",
        "openai_api_key": "${OPENAI_API_KEY}",
        "tavily_api_key": "${TAVILY_API_KEY}",
        "serpapi_api_key": "${SERPAPI_API_KEY}",
        "news_api_key": "${NEWS_API_KEY}",
        "model_settings": {
            "default_provider": "openai",
            "default_model": "gpt-4o-mini",
            "default_temperature": 0.4,
            "default_top_p": 0.9,
            "default_max_tokens": 4000
        },
        "checkpointer": "memory",
        "logging": {
            "level": "INFO",
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        }
    }
}

================
File: langstuff_multi_agent/__init__.py
================
"""
LangStuff Multi-Agent package.
A multi-agent system built with LangGraph for handling various 
specialized tasks.
"""

__version__ = "0.1.0"

================
File: langstuff_multi_agent/agent.py
================
"""
Entry point for the LangGraph multi-agent system.
"""

import logging
from langstuff_multi_agent.agents.supervisor import create_supervisor
from langstuff_multi_agent.config import get_llm
from langgraph.checkpoint.memory import MemorySaver

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Initializing supervisor workflow with preprocessing...")

# Create and compile the supervisor graph
try:
    supervisor_graph = create_supervisor(llm=get_llm()).compile(checkpointer=MemorySaver())
    logger.info("Graph compiled successfully")
except Exception as e:
    logger.error(f"Graph compilation failed: {str(e)}", exc_info=True)
    raise

# Alias for entry point
graph = supervisor_graph

__all__ = ["graph", "supervisor_graph"]

logger.info("Supervisor workflow initialized.")

if __name__ == "__main__":
    from langchain_core.messages import HumanMessage
    test_input = {"messages": [HumanMessage(content="Test query")]}
    result = graph.invoke(test_input)
    print(result)

================
File: langstuff_multi_agent/agents/analyst.py
================
"""
Analyst Agent module for data analysis and interpretation.

This module provides a workflow for analyzing data and performing calculations using various tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, python_repl, calc_tool, news_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import AIMessage, SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def analyze_data(state: MessagesState, config: dict) -> dict:
    """Analyze data and perform calculations."""
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, python_repl, calc_tool, news_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are an Analyst Agent. Your task is to analyze data and perform calculations.\n\n"
            "You have access to the following tools:\n"
            "- search_web: Gather data from the web.\n"
            "- python_repl: Execute Python code for analysis.\n"
            "- calc_tool: Perform mathematical calculations.\n"
            "- news_tool: Retrieve relevant news data.\n\n"
            "Instructions:\n"
            "1. Analyze the user's data request.\n"
            "2. Use tools to gather or compute data.\n"
            "3. Provide a clear, concise analysis."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final analysis."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, python_repl, calc_tool, news_tool] if t.name == tc["name"])
        try:
            output = tool.invoke(tc["args"])
            tool_messages.append({
                "role": "tool",
                "content": output,
                "tool_call_id": tc["id"]
            })
        except Exception as e:
            logger.error(f"Tool execution failed: {str(e)}")
            tool_messages.append({
                "role": "tool",
                "content": f"Error: {str(e)}",
                "tool_call_id": tc["id"]
            })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages + [
        SystemMessage(content="Analyze and interpret these results:")
    ])
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
analyst_graph = StateGraph(MessagesState)
analyst_graph.add_node("analyze_data", analyze_data)
analyst_graph.add_node("tools", tools_node)  # Use wrapped tools_node
analyst_graph.add_node("process_results", process_tool_results)
analyst_graph.set_entry_point("analyze_data")
analyst_graph.add_conditional_edges(
    "analyze_data",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
analyst_graph.add_edge("tools", "process_results")
analyst_graph.add_edge("process_results", "analyze_data")
analyst_graph = analyst_graph.compile()

__all__ = ["analyst_graph"]

================
File: langstuff_multi_agent/agents/coder.py
================
"""
Coder Agent module for writing and improving code.

This module provides a workflow for code generation, debugging, and optimization using various development tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, python_repl, read_file, write_file, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def code(state: MessagesState, config: dict) -> dict:
    """Write and improve code with configuration support."""
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    tools = [search_web, python_repl, read_file, write_file, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Coder Agent. Your task is to write, debug, and improve code.\n\n"
            "You have access to the following tools:\n"
            "- search_web: Find coding examples and docs.\n"
            "- python_repl: Execute and test Python code.\n"
            "- read_file: Retrieve code from files.\n"
            "- write_file: Save code modifications to files.\n"
            "- calc_tool: Perform calculations if needed.\n\n"
            "Instructions:\n"
            "1. Analyze the user's code or coding request.\n"
            "2. Provide solutions, test code, and explain your reasoning.\n"
            "3. Use the available tools to execute code and verify fixes as necessary."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, python_repl, read_file, write_file, calc_tool] if t.name == tc["name"])
        output = tool.invoke(tc["args"])
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
coder_graph = StateGraph(MessagesState)
coder_graph.add_node("code", code)
coder_graph.add_node("tools", tools_node)  # Use wrapped tools_node
coder_graph.add_node("process_results", process_tool_results)
coder_graph.set_entry_point("code")
coder_graph.add_conditional_edges(
    "code",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
coder_graph.add_edge("tools", "process_results")
coder_graph.add_edge("process_results", "code")
coder_graph = coder_graph.compile()

__all__ = ["coder_graph"]

================
File: langstuff_multi_agent/agents/context_manager.py
================
"""
Context Manager Agent module for tracking conversation context.

This module provides a workflow for managing conversation history and maintaining context across interactions.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, read_file, write_file, save_memory, search_memories
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def manage_context(state: MessagesState, config: dict) -> dict:
    """Manage conversation context using memory tools."""
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, read_file, write_file, save_memory, search_memories]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Context Manager Agent. Your task is to track and maintain conversation context.\n"
            "You have access to the following tools:\n"
            "- search_web: Look up additional context.\n"
            "- read_file: Retrieve stored context from files.\n"
            "- write_file: Save context to files.\n"
            "- save_memory: Save conversation history to memory.\n"
            "- search_memories: Retrieve relevant past context.\n\n"
            "Instructions:\n"
            "1. Analyze the current conversation.\n"
            "2. Use tools to save or retrieve context as needed.\n"
            "3. Provide a response that reflects the full context."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Process tool outputs and format final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, read_file, write_file, save_memory, search_memories] if t.name == tc["name"])
        # Pass config only to memory tools that need it
        output = tool.invoke(tc["args"], config=config if tc["name"] in ["save_memory", "search_memories"] else None)
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
context_manager_graph = StateGraph(MessagesState)
context_manager_graph.add_node("manage_context", manage_context)
context_manager_graph.add_node("tools", tools_node)  # Use wrapped tools_node
context_manager_graph.add_node("process_results", process_tool_results)
context_manager_graph.set_entry_point("manage_context")
context_manager_graph.add_conditional_edges(
    "manage_context",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
context_manager_graph.add_edge("tools", "process_results")
context_manager_graph.add_edge("process_results", "manage_context")
context_manager_graph = context_manager_graph.compile()

__all__ = ["context_manager_graph"]

================
File: langstuff_multi_agent/agents/creative_content.py
================
"""
Creative Content Agent module for generating creative writing, marketing copy, social media posts, or brainstorming ideas.

This module provides a workflow for generating creative content using various tools and a creative prompt.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import AIMessage, SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def creative_content(state: MessagesState, config: dict) -> dict:
    """Generate creative content based on the user's query with configuration support."""
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    tools = [search_web, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Creative Content Agent. Your task is to generate creative writing, marketing copy, "
            "social media posts, or brainstorming ideas. Use vivid, imaginative, and engaging language.\n\n"
            "You have access to the following tools:\n"
            "- search_web: Look up trends or inspiration from online sources.\n"
            "- calc_tool: Perform calculations if needed (secondary in this role).\n\n"
            "Instructions:\n"
            "1. Analyze the user's creative query.\n"
            "2. Draw upon your creative instincts (and tool data if helpful) to generate an inspiring draft.\n"
            "3. Produce a final piece of creative content that addresses the query."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Process tool outputs and integrate them into a final creative content draft."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, calc_tool] if t.name == tc["name"])
        output = tool.invoke(tc["args"])
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages + [
        SystemMessage(content="Synthesize the following inspirations into a creative draft:")
    ])
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
creative_content_graph = StateGraph(MessagesState)
creative_content_graph.add_node("creative_content", creative_content)
creative_content_graph.add_node("tools", tools_node)  # Use wrapped tools_node
creative_content_graph.add_node("process_results", process_tool_results)
creative_content_graph.set_entry_point("creative_content")
creative_content_graph.add_conditional_edges(
    "creative_content",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
creative_content_graph.add_edge("tools", "process_results")
creative_content_graph.add_edge("process_results", "creative_content")
creative_content_graph = creative_content_graph.compile()

__all__ = ["creative_content_graph"]

================
File: langstuff_multi_agent/agents/customer_support.py
================
"""
Customer Support Agent module for handling customer inquiries, troubleshooting, and FAQs.

This module provides a workflow for addressing common customer support issues using tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def support(state: MessagesState, config: dict) -> dict:
    """Conduct customer support interaction with configuration support."""
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    tools = [search_web, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Customer Support Agent. Your task is to address customer inquiries, provide troubleshooting steps, and answer FAQs.\n\n"
            "You have access to the following tools:\n"
            "- search_web: Look up support documentation and FAQs.\n"
            "- calc_tool: Perform calculations if needed.\n\n"
            "Instructions:\n"
            "1. Analyze the customer's query.\n"
            "2. Use tools to retrieve accurate support information.\n"
            "3. Provide a clear, concise response to help the customer."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats the final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, calc_tool] if t.name == tc["name"])
        output = tool.invoke(tc["args"])
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
customer_support_graph = StateGraph(MessagesState)
customer_support_graph.add_node("support", support)
customer_support_graph.add_node("tools", tools_node)  # Use wrapped tools_node
customer_support_graph.add_node("process_results", process_tool_results)
customer_support_graph.set_entry_point("support")
customer_support_graph.add_conditional_edges(
    "support",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
customer_support_graph.add_edge("tools", "process_results")
customer_support_graph.add_edge("process_results", "support")
customer_support_graph = customer_support_graph.compile()

__all__ = ["customer_support_graph"]

================
File: langstuff_multi_agent/agents/debugger.py
================
"""
Debugger Agent module for analyzing code and identifying errors.

This module provides a workflow for debugging code using various tools and LLM-based analysis.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, python_repl, read_file, write_file, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def analyze_code(state: MessagesState, config: dict) -> dict:
    """Analyze code and identify errors."""
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, python_repl, read_file, write_file, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Debugger Agent. Your task is to analyze code and identify errors.\n"
            "You have access to the following tools:\n"
            "- search_web: Look up debugging resources.\n"
            "- python_repl: Test code snippets.\n"
            "- read_file: Retrieve code from files.\n"
            "- write_file: Save corrected code.\n"
            "- calc_tool: Perform calculations if needed.\n\n"
            "Instructions:\n"
            "1. Analyze the user's code or error description.\n"
            "2. Use tools to test or research solutions.\n"
            "3. Provide a clear explanation and fix."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, python_repl, read_file, write_file, calc_tool] if t.name == tc["name"])
        output = tool.invoke(tc["args"])
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
debugger_graph = StateGraph(MessagesState)
debugger_graph.add_node("analyze_code", analyze_code)
debugger_graph.add_node("tools", tools_node)  # Use wrapped tools_node
debugger_graph.add_node("process_results", process_tool_results)
debugger_graph.set_entry_point("analyze_code")
debugger_graph.add_conditional_edges(
    "analyze_code",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
debugger_graph.add_edge("tools", "process_results")
debugger_graph.add_edge("process_results", "analyze_code")
debugger_graph = debugger_graph.compile()

__all__ = ["debugger_graph"]

================
File: langstuff_multi_agent/agents/financial_analyst.py
================
"""
Financial Analyst Agent module for analyzing market data, forecasting trends, and providing investment insights.

This module provides a workflow for gathering financial news and data, analyzing stock performance or economic indicators, and synthesizing a concise summary with actionable investment insights.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, news_tool, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import AIMessage, SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def financial_analysis(state: MessagesState, config: dict) -> dict:
    """Conduct financial analysis with configuration support."""
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    tools = [search_web, news_tool, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Financial Analyst Agent. Your task is to analyze current market data, stock performance, economic indicators, and forecast trends.\n"
            "You have access to the following tools:\n"
            "- search_web: Look up up-to-date financial news and data.\n"
            "- news_tool: Retrieve the latest financial headlines and market insights.\n"
            "- calc_tool: Perform any necessary calculations.\n\n"
            "Instructions:\n"
            "1. Analyze the user's query about market conditions or investment opportunities.\n"
            "2. Use tools to gather accurate and relevant financial information.\n"
            "3. Synthesize a clear, concise summary with actionable insights."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Process tool outputs and format the final financial analysis report."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, news_tool, calc_tool] if t.name == tc["name"])
        try:
            output = tool.invoke(tc["args"])
            tool_messages.append({
                "role": "tool",
                "content": output,
                "tool_call_id": tc["id"]
            })
        except Exception as e:
            logger.error(f"Tool execution failed: {str(e)}")
            tool_messages.append({
                "role": "tool",
                "content": f"Error: {str(e)}",
                "tool_call_id": tc["id"]
            })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages + [
        SystemMessage(content="Synthesize the financial data into a concise analysis with key insights:")
    ])
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
financial_analyst_graph = StateGraph(MessagesState)
financial_analyst_graph.add_node("financial_analysis", financial_analysis)
financial_analyst_graph.add_node("tools", tools_node)  # Use wrapped tools_node
financial_analyst_graph.add_node("process_results", process_tool_results)
financial_analyst_graph.set_entry_point("financial_analysis")
financial_analyst_graph.add_conditional_edges(
    "financial_analysis",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
financial_analyst_graph.add_edge("tools", "process_results")
financial_analyst_graph.add_edge("process_results", "financial_analysis")
financial_analyst_graph = financial_analyst_graph.compile()

__all__ = ["financial_analyst_graph"]

================
File: langstuff_multi_agent/agents/general_assistant.py
================
"""
General Assistant Agent module for handling diverse queries.

This module provides a workflow for addressing general user requests using a variety of tools.
"""

from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, get_current_weather, news_tool
from langstuff_multi_agent.config import get_llm
from langchain_core.messages import AIMessage, SystemMessage, BaseMessage, ToolMessage, ToolCall
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def assist(state: MessagesState, config: dict) -> dict:
    """Provide general assistance with configuration support."""
    logger.info(f"Assist config received: type={type(config)}, value={config}")
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, get_current_weather, news_tool]
    llm_with_tools = llm.bind_tools(tools)
    
    # Log incoming state
    logger.info(f"Assist input state: {state}")
    
    messages = state["messages"]
    system_prompt = SystemMessage(content=(
        "You are a General Assistant Agent. Your task is to assist with a variety of general queries and tasks.\n\n"
        "You have access to the following tools:\n"
        "- search_web: Provide general information and answer questions.\n"
        "- get_current_weather: Retrieve current weather updates.\n"
        "- news_tool: Retrieve news headlines and articles.\n\n"
        "Instructions:\n"
        "1. Understand the user's request.\n"
        "2. Use the available tools to gather relevant information when needed.\n"
        "3. Provide clear, concise, and helpful responses to assist the user."
    ))
    
    # Invoke LLM and ensure response is an AIMessage
    response = llm_with_tools.invoke([system_prompt] + messages)
    logger.info(f"LLM response: type={type(response)}, value={response}")
    
    # Convert raw tool calls to proper objects (from LangChain v0.1 docs)
    if isinstance(response, dict):
        raw_tool_calls = response.get("tool_calls", [])
        tool_calls = [
            ToolCall(**tc) if isinstance(tc, dict) else tc
            for tc in raw_tool_calls
        ]
        response = AIMessage(
            content=response.get("content", ""),
            tool_calls=tool_calls
        )
    else:
        # Ensure existing tool calls are proper objects
        response.tool_calls = [
            ToolCall(**tc) if isinstance(tc, dict) else tc
            for tc in getattr(response, 'tool_calls', [])
        ]

    logger.info(f"Returning response: {response}")
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final response."""
    logger.info(f"Process tool results config: type={type(config)}, value={config}")
    last_message = state["messages"][-1]
    
    # Convert to ToolCall objects if needed (from LangChain docs)
    tool_calls = getattr(last_message, 'tool_calls', [])
    if not tool_calls:
        return state
    
    logger.info(f"Processing tool calls: {tool_calls}")
    tool_messages = []
    for tc in tool_calls:
        # Safely handle both dict and ToolCall types
        tool_name = tc.name if isinstance(tc, ToolCall) else tc.get("name")
        tool_args = tc.args if isinstance(tc, ToolCall) else tc.get("args")
        tool_id = tc.id if isinstance(tc, ToolCall) else tc.get("id")
        
        # Find matching tool (from search results example)
        tool = next(
            t for t in [search_web, get_current_weather, news_tool]
            if t.name == tool_name
        )
        output = tool.invoke(tool_args)
        tool_messages.append(ToolMessage(
            content=str(output),
            tool_call_id=tool_id,
            name=tool_name
        ))

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)

    if not isinstance(final_response, AIMessage):
        if isinstance(final_response, dict):
            content = final_response.get("content", "Task completed")
            raw_tool_calls = final_response.get("tool_calls", [])
            tool_calls = [ToolCall(**tc) for tc in raw_tool_calls]
            final_response = AIMessage(content=content, tool_calls=tool_calls)
        else:
            final_response = AIMessage(content=str(final_response))

    final_response.additional_kwargs["final_answer"] = True

    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState, config: dict) -> dict:
    return tool_node(state)

# Define and compile the graph
general_assistant_graph = StateGraph(MessagesState)
general_assistant_graph.add_node("assist", assist)
general_assistant_graph.add_node("tools", lambda state, config: tools_node(state, config))
general_assistant_graph.add_node("process_results", process_tool_results)
general_assistant_graph.set_entry_point("assist")
general_assistant_graph.add_conditional_edges(
    "assist",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
general_assistant_graph.add_edge("tools", "process_results")
general_assistant_graph.add_edge("process_results", "assist")
general_assistant_graph = general_assistant_graph.compile()

__all__ = ["general_assistant_graph"]

================
File: langstuff_multi_agent/agents/life_coach.py
================
"""
Life Coach Agent module for personal advice and guidance.

This module provides a workflow for offering lifestyle tips and personal development advice using various tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, get_current_weather, calendar_tool, save_memory, search_memories
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def life_coach(state: MessagesState, config: dict) -> dict:
    """Provide life coaching and personal advice."""
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, get_current_weather, calendar_tool, save_memory, search_memories]
    llm = llm.bind_tools(tools)
    messages = state["messages"]
    if "user_id" in config.get("configurable", {}):
        preferences = search_memories.invoke("lifestyle preferences", config)
        if preferences:
            messages.append(SystemMessage(content=f"User preferences: {preferences}"))
    response = llm.invoke(messages + [
        SystemMessage(content=(
            "You are a Life Coach Agent. Your task is to offer lifestyle tips and personal development advice.\n"
            "You have access to the following tools:\n"
            "- search_web: Find advice and resources.\n"
            "- get_current_weather: Provide weather-based suggestions.\n"
            "- calendar_tool: Schedule activities.\n"
            "- save_memory: Save user preferences.\n"
            "- search_memories: Retrieve past preferences.\n\n"
            "Instructions:\n"
            "1. Analyze the user's request.\n"
            "2. Use tools to provide tailored advice.\n"
            "3. Offer actionable suggestions."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, get_current_weather, calendar_tool, save_memory, search_memories] if t.name == tc["name"])
        output = tool.invoke(tc["args"], config=config if tc["name"] in ["save_memory", "search_memories"] else None)
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
life_coach_graph = StateGraph(MessagesState)
life_coach_graph.add_node("life_coach", life_coach)
life_coach_graph.add_node("tools", tools_node)  # Use wrapped tools_node
life_coach_graph.add_node("process_results", process_tool_results)
life_coach_graph.set_entry_point("life_coach")
life_coach_graph.add_conditional_edges(
    "life_coach",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
life_coach_graph.add_edge("tools", "process_results")
life_coach_graph.add_edge("process_results", "life_coach")
life_coach_graph = life_coach_graph.compile()

__all__ = ["life_coach_graph"]

================
File: langstuff_multi_agent/agents/marketing_strategist.py
================
"""
Marketing Strategist Agent module for analyzing trends, planning campaigns, and providing social media strategy insights.

This module provides a workflow for gathering market data, identifying trends, and delivering actionable marketing strategies.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, news_tool, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def marketing(state: MessagesState, config: dict) -> dict:
    """Conduct marketing strategy analysis with configuration support."""
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    tools = [search_web, news_tool, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Marketing Strategist Agent. Your task is to analyze current trends, plan marketing campaigns, and provide social media strategy insights.\n\n"
            "You have access to the following tools:\n"
            "- search_web: Gather market and trend information.\n"
            "- news_tool: Retrieve the latest news and social media trends.\n"
            "- calc_tool: Perform quantitative analysis if needed.\n\n"
            "Instructions:\n"
            "1. Analyze the customer's marketing query.\n"
            "2. Use tools to gather accurate market data and trend information.\n"
            "3. Provide detailed, actionable marketing strategies and social media insights."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats the final marketing strategy response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, news_tool, calc_tool] if t.name == tc["name"])
        output = tool.invoke(tc["args"])
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
marketing_strategist_graph = StateGraph(MessagesState)
marketing_strategist_graph.add_node("marketing", marketing)
marketing_strategist_graph.add_node("tools", tools_node)  # Use wrapped tools_node
marketing_strategist_graph.add_node("process_results", process_tool_results)
marketing_strategist_graph.set_entry_point("marketing")
marketing_strategist_graph.add_conditional_edges(
    "marketing",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
marketing_strategist_graph.add_edge("tools", "process_results")
marketing_strategist_graph.add_edge("process_results", "marketing")
marketing_strategist_graph = marketing_strategist_graph.compile()

__all__ = ["marketing_strategist_graph"]

================
File: langstuff_multi_agent/agents/news_reporter.py
================
"""
Enhanced News Reporter Agent for LangGraph.

This module provides a workflow for fetching and summarizing news based on user queries.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, news_tool, calc_tool
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def news_report(state: MessagesState, config: dict) -> dict:
    """Main node: fetch news based on user query and update the state."""
    messages = state.get("messages", [])
    user_query = next((msg.content for msg in messages if isinstance(msg, HumanMessage)), "")

    if not user_query:
        response = AIMessage(content="No query provided to search for news.", additional_kwargs={"final_answer": True})
        return {"messages": messages + [response]}

    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, news_tool, calc_tool]
    llm = llm.bind_tools(tools)
    response = llm.invoke(messages + [
        SystemMessage(content=(
            "You are a News Reporter Agent. Use the available tools to gather and summarize news.\n"
            "Always use news_tool first, then search_web if needed for additional context."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": messages + [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Process tool outputs and format final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, news_tool, calc_tool] if t.name == tc["name"])
        try:
            output = tool.invoke(tc["args"])
            tool_messages.append({
                "role": "tool",
                "content": output,
                "tool_call_id": tc["id"]
            })
        except Exception as e:
            logger.error(f"Tool execution failed: {str(e)}")
            tool_messages.append({
                "role": "tool",
                "content": f"Error: {str(e)}",
                "tool_call_id": tc["id"]
            })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages + [
        SystemMessage(content="Create a clear and concise summary of the news articles.")
    ])
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
news_reporter_graph = StateGraph(MessagesState)
news_reporter_graph.add_node("news_report", news_report)
news_reporter_graph.add_node("tools", tools_node)  # Use wrapped tools_node
news_reporter_graph.add_node("process_results", process_tool_results)
news_reporter_graph.set_entry_point("news_report")
news_reporter_graph.add_conditional_edges(
    "news_report",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
news_reporter_graph.add_edge("tools", "process_results")
news_reporter_graph.add_edge("process_results", "news_report")
news_reporter_graph = news_reporter_graph.compile()

__all__ = ["news_reporter_graph"]

================
File: langstuff_multi_agent/agents/professional_coach.py
================
"""
Professional Coach Agent module for career guidance.

This module provides a workflow for offering career advice and job search strategies using various tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, job_search_tool, get_current_weather, calendar_tool, save_memory, search_memories
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def coach(state: MessagesState, config: dict) -> dict:
    """Provide professional coaching and career advice."""
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, job_search_tool, get_current_weather, calendar_tool, save_memory, search_memories]
    llm = llm.bind_tools(tools)
    messages = state["messages"]
    if "user_id" in config.get("configurable", {}):
        career_goals = search_memories.invoke("career goals", config)
        if career_goals:
            messages.append(SystemMessage(content=f"User career history: {career_goals}"))
    response = llm.invoke(messages + [
        SystemMessage(content=(
            "You are a Professional Coach Agent. Your task is to offer career advice and job search strategies.\n"
            "Use tools to provide relevant information and schedule events as needed."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, job_search_tool, get_current_weather, calendar_tool, save_memory, search_memories] if t.name == tc["name"])
        output = tool.invoke(tc["args"], config=config if tc["name"] in ["save_memory", "search_memories"] else None)
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
professional_coach_graph = StateGraph(MessagesState)
professional_coach_graph.add_node("coach", coach)
professional_coach_graph.add_node("tools", tools_node)  # Use wrapped tools_node
professional_coach_graph.add_node("process_results", process_tool_results)
professional_coach_graph.set_entry_point("coach")
professional_coach_graph.add_conditional_edges(
    "coach",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
professional_coach_graph.add_edge("tools", "process_results")
professional_coach_graph.add_edge("process_results", "coach")
professional_coach_graph = professional_coach_graph.compile()

__all__ = ["professional_coach_graph"]

================
File: langstuff_multi_agent/agents/project_manager.py
================
"""
Project Manager Agent module for task and timeline management.

This module provides a workflow for overseeing project schedules and coordinating tasks using various tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, python_repl
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import SystemMessage

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def manage(state: MessagesState, config: dict) -> dict:
    """Project management agent that coordinates tasks and timelines."""
    llm = get_llm(config.get("configurable", {}))
    tools = [search_web, python_repl]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Project Manager Agent. Your task is to oversee project schedules and coordinate tasks.\n"
            "Use tools like search_web and python_repl to gather info or perform calculations.\n"
            "Provide actionable plans or updates."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs and formats final response."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    tool_messages = []
    for tc in last_message.tool_calls:
        tool = next(t for t in [search_web, python_repl] if t.name == tc["name"])
        output = tool.invoke(tc["args"])
        tool_messages.append({
            "role": "tool",
            "content": output,
            "tool_call_id": tc["id"]
        })

    llm = get_llm(config.get("configurable", {}))
    final_response = llm.invoke(state["messages"] + tool_messages)
    final_response.additional_kwargs["final_answer"] = True
    return {"messages": state["messages"] + tool_messages + [final_response]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
project_manager_graph = StateGraph(MessagesState)
project_manager_graph.add_node("manage", manage)
project_manager_graph.add_node("tools", tools_node)  # Use wrapped tools_node
project_manager_graph.add_node("process_results", process_tool_results)
project_manager_graph.set_entry_point("manage")
project_manager_graph.add_conditional_edges(
    "manage",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
project_manager_graph.add_edge("tools", "process_results")
project_manager_graph.add_edge("process_results", "manage")
project_manager_graph = project_manager_graph.compile()

__all__ = ["project_manager_graph"]

================
File: langstuff_multi_agent/agents/researcher.py
================
"""
Researcher Agent module for gathering and summarizing information.

This module provides a workflow for gathering and summarizing news and research 
information using various tools.
"""

import logging
from langgraph.graph import StateGraph, MessagesState, END
from langstuff_multi_agent.utils.tools import tool_node, has_tool_calls, search_web, news_tool, calc_tool, save_memory, search_memories
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage, SystemMessage, HumanMessage, AIMessage
import json

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def research(state: MessagesState, config: dict) -> dict:
    """Conduct research with configuration support."""
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    tools = [search_web, news_tool, calc_tool, save_memory, search_memories]
    llm = llm.bind_tools(tools)
    response = llm.invoke(state["messages"] + [
        SystemMessage(content=(
            "You are a Researcher Agent. Your task is to gather and summarize news and research information.\n\n"
            "You have access to the following tools:\n"
            "- search_web: Look up recent info and data.\n"
            "- news_tool: Get latest news and articles.\n"
            "- calc_tool: Perform calculations.\n"
            "- save_memory: Save information to memory.\n"
            "- search_memories: Search for information in memory.\n\n"
            "Instructions:\n"
            "1. Analyze the user's research query.\n"
            "2. Use tools to gather accurate and relevant info.\n"
            "3. Provide a clear summary of your findings."
        ))
    ])
    if not response.tool_calls:
        response.additional_kwargs["final_answer"] = True  # Signal completion
    return {"messages": [response]}

def process_tool_results(state: MessagesState, config: dict) -> dict:
    """Processes tool outputs with enhanced error handling."""
    last_message = state["messages"][-1]
    if not last_message.tool_calls:
        return state

    try:
        tool_outputs = []
        for tc in last_message.tool_calls:
            tool = next(t for t in [search_web, news_tool, calc_tool, save_memory, search_memories] if t.name == tc["name"])
            result = tool.invoke(tc["args"], config=config if tc["name"] in ["save_memory", "search_memories"] else None)
            tool_outputs.append({"tool_call_id": tc["id"], "output": result})

        llm = get_llm(config.get("configurable", {}))
        summary = llm.invoke([
            SystemMessage(content="Synthesize these research findings:"),
            HumanMessage(content="\n".join([to["output"] if isinstance(to["output"], str) else json.dumps(to["output"]) for to in tool_outputs]))
        ])
        summary.additional_kwargs["final_answer"] = True
        return {"messages": state["messages"] + [ToolMessage(
            content=summary.content,
            tool_call_id=last_message.tool_calls[0]["id"]  # Use first tool call ID for simplicity
        )]}

    except Exception as e:
        return {"messages": state["messages"] + [AIMessage(content=f"Error processing research: {str(e)}", additional_kwargs={"final_answer": True})]}

# Define a wrapper for the tools node to avoid passing config
def tools_node(state: MessagesState) -> dict:
    return tool_node(state)

# Define and compile the graph
researcher_graph = StateGraph(MessagesState)
researcher_graph.add_node("research", research)
researcher_graph.add_node("tools", tools_node)  # Use wrapped tools_node
researcher_graph.add_node("process_results", process_tool_results)
researcher_graph.set_entry_point("research")
researcher_graph.add_conditional_edges(
    "research",
    lambda state: "tools" if has_tool_calls(state["messages"]) else END,
    {"tools": "tools", END: END}
)
researcher_graph.add_edge("tools", "process_results")
researcher_graph.add_edge("process_results", "research")
researcher_graph = researcher_graph.compile()

__all__ = ["researcher_graph"]

================
File: langstuff_multi_agent/agents/supervisor.py
================
"""
Supervisor module for managing a hierarchical multi-agent system.
"""

import logging
from typing import List, Literal, Dict, Any, TypedDict, Annotated
from pydantic.v1 import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph.message import add_messages
import operator
from langstuff_multi_agent.config import get_llm

# Import all agent graphs
from langstuff_multi_agent.agents.debugger import debugger_graph
from langstuff_multi_agent.agents.context_manager import context_manager_graph
from langstuff_multi_agent.agents.project_manager import project_manager_graph
from langstuff_multi_agent.agents.professional_coach import professional_coach_graph
from langstuff_multi_agent.agents.life_coach import life_coach_graph
from langstuff_multi_agent.agents.coder import coder_graph
from langstuff_multi_agent.agents.analyst import analyst_graph
from langstuff_multi_agent.agents.researcher import researcher_graph
from langstuff_multi_agent.agents.general_assistant import general_assistant_graph
from langstuff_multi_agent.agents.news_reporter import news_reporter_graph
from langstuff_multi_agent.agents.customer_support import customer_support_graph
from langstuff_multi_agent.agents.marketing_strategist import marketing_strategist_graph
from langstuff_multi_agent.agents.creative_content import creative_content_graph
from langstuff_multi_agent.agents.financial_analyst import financial_analyst_graph

# Set up logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Define all available agents
AVAILABLE_AGENTS = [
    'debugger', 'context_manager', 'project_manager', 'professional_coach',
    'life_coach', 'coder', 'analyst', 'researcher', 'general_assistant',
    'news_reporter', 'customer_support', 'marketing_strategist',
    'creative_content', 'financial_analyst'
]

# Map agent names to their respective graphs
member_graphs = {
    "debugger": debugger_graph,
    "context_manager": context_manager_graph,
    "project_manager": project_manager_graph,
    "professional_coach": professional_coach_graph,
    "life_coach": life_coach_graph,
    "coder": coder_graph,
    "analyst": analyst_graph,
    "researcher": researcher_graph,
    "general_assistant": general_assistant_graph,
    "news_reporter": news_reporter_graph,
    "customer_support": customer_support_graph,
    "marketing_strategist": marketing_strategist_graph,
    "creative_content": creative_content_graph,
    "financial_analyst": financial_analyst_graph
}

# Define the state structure for the supervisor
class SupervisorState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add, add_messages]
    next: str
    error_count: Annotated[int, operator.add]
    reasoning: str | None

# Define the routing decision model
class RouteDecision(BaseModel):
    reasoning: str = Field(..., description="Step-by-step routing logic")
    destination: Literal[
        'debugger', 'context_manager', 'project_manager', 'professional_coach',
        'life_coach', 'coder', 'analyst', 'researcher', 'general_assistant',
        'news_reporter', 'customer_support', 'marketing_strategist',
        'creative_content', 'financial_analyst', 'FINISH'
    ] = Field(..., description="Target agent or FINISH")

# Preprocess input messages
def preprocess_input(state: SupervisorState, config: RunnableConfig) -> Dict[str, Any]:
    """
    Converts raw input into a list of BaseMessage objects.
    """
    messages = state.get("messages", [])
    if not messages:
        raw_input = state.get("messages", []) or [{"type": "human", "content": "Hello"}]
        messages = []
        for msg in raw_input:
            if isinstance(msg, dict):
                role = msg.get("type", "human")
                content = msg.get("content", "")
                if role == "human":
                    messages.append(HumanMessage(content=content))
                else:
                    messages.append(BaseMessage(content=content, type=role))
            elif isinstance(msg, BaseMessage):
                messages.append(msg)
    logger.info(f"Preprocess output: {messages}")
    return {"messages": messages, "error_count": 0}

# Supervisor routing logic
def supervisor_logic(state: SupervisorState, config: RunnableConfig) -> Dict[str, Any]:
    """
    Determines the next agent to route to based on the current state.
    """
    messages = state["messages"]
    if not messages:
        return {
            "next": "general_assistant",
            "error_count": 0,
            "messages": messages,
            "reasoning": "No messages provided, defaulting to general_assistant"
        }
    last_message = messages[-1]
    if isinstance(last_message, AIMessage) and last_message.additional_kwargs.get("final_answer", False):
        return {
            "next": "FINISH",
            "error_count": state.get("error_count", 0),
            "messages": messages,
            "reasoning": "Agent marked response as final"
        }

    system_prompt = (
        f"You manage these workers: {', '.join(AVAILABLE_AGENTS)}. "
        "Analyze the query and route to ONE specialized agent or FINISH if the task is fully resolved.\n"
        "Rules:\n"
        "1. Route complex queries through multiple agents sequentially if needed.\n"
        "2. Use FINISH only when an agent has provided a complete response (marked as final_answer).\n"
        "3. For greetings, identity questions (e.g., 'who are you'), or vague/general queries, route to general_assistant.\n"
        "4. On errors or uncertainty, route to general_assistant.\n"
        "Provide step-by-step reasoning and your decision."
    )
    structured_llm = get_llm().with_structured_output(RouteDecision)
    try:
        decision = structured_llm.invoke([SystemMessage(content=system_prompt), *messages])
        next_destination = decision.destination if decision.destination in AVAILABLE_AGENTS + ["FINISH"] else "general_assistant"
        return {
            "next": next_destination,
            "reasoning": decision.reasoning,
            "error_count": state.get("error_count", 0),
            "messages": messages
        }
    except Exception as e:
        logger.error(f"Routing failed: {str(e)}")
        return {
            "next": "general_assistant",
            "error_count": state.get("error_count", 0) + 1,
            "messages": messages + [SystemMessage(content=f"Routing error: {str(e)}")],
            "reasoning": "Fallback to general_assistant due to routing failure"
        }

# Create the supervisor workflow
def create_supervisor(llm) -> StateGraph:
    """
    Sets up the StateGraph with all nodes and edges.
    """
    workflow = StateGraph(SupervisorState)
    workflow.add_node("preprocess", preprocess_input)
    workflow.add_node("supervisor", supervisor_logic)

    # Track successfully added agents
    added_agents = []

    for name in AVAILABLE_AGENTS:
        try:
            subgraph = member_graphs[name]

            # Factory function to create a unique wrapper for each subgraph
            def make_subgraph_node(subgraph):
                def subgraph_node(state: SupervisorState, config: RunnableConfig) -> SupervisorState:
                    # Prepare the subgraph state with messages
                    subgraph_state = {"messages": state["messages"]}
                    try:
                        # Attempt to call the subgraph with config as a keyword argument
                        result = subgraph(subgraph_state, config=config.dict())
                    except TypeError:
                        # Fallback to calling without config if not supported
                        result = subgraph(subgraph_state)
                    # Update the supervisor state with the subgraph's output
                    state["messages"] = result["messages"]
                    return state
                return subgraph_node

            # Create and add the specific wrapper function as a node
            specific_subgraph_node = make_subgraph_node(subgraph)
            workflow.add_node(name, specific_subgraph_node)
            workflow.add_edge(name, "supervisor")
            added_agents.append(name)
            logger.info(f"Successfully added node: {name}")
        except Exception as e:
            logger.error(f"Failed to add node {name}: {str(e)}", exc_info=True)

    # Define conditional edges based on successfully added agents
    workflow.add_conditional_edges(
        "supervisor",
        lambda state: state["next"],
        {name: name for name in added_agents} | {"FINISH": END}
    )
    workflow.add_edge("preprocess", "supervisor")
    workflow.set_entry_point("preprocess")
    return workflow

# Instantiate the supervisor workflow
supervisor_workflow = create_supervisor(llm=get_llm())

# Export public symbols
__all__ = ["create_supervisor", "supervisor_workflow", "SupervisorState", "member_graphs"]

================
File: langstuff_multi_agent/config.py
================
"""
Configuration for the LangGraph multi-agent AI project.
Handles LLM instantiation, checkpointer setup, and logging.
"""

import os
import logging
import json
from functools import lru_cache, wraps
from typing import Optional, Dict, Literal, Callable, Any, TypedDict
from pydantic import BaseModel, ValidationError
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.runnables.config import RunnableConfig
from langstuff_multi_agent.utils.memory import MemoryManager, LangGraphMemoryCheckpointer


class ConfigSchema(TypedDict):
    model: Optional[str]
    system_message: Optional[str]
    temperature: Optional[float]
    top_p: Optional[float]
    max_tokens: Optional[int]
    provider: Literal["openai", "anthropic", "grok"]


class Config:
    # API Keys
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    XAI_API_KEY = os.environ.get("XAI_API_KEY")

    # Defaults
    DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "gpt-4o-mini")
    DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", 0.4))
    DEFAULT_PROVIDER = os.environ.get("AI_PROVIDER", "openai").lower()
    LLM_CACHE_SIZE = int(os.environ.get("LLM_CACHE_SIZE", "8"))

    # Model configs
    MODEL_CONFIGS = {
        "anthropic": {"model_name": "claude-3-5-sonnet-20240620", "temperature": 0.0, "top_p": 0.9, "max_tokens": 4000},
        "openai": {"model_name": "gpt-4o-mini", "temperature": 0.4, "top_p": 0.9, "max_tokens": 4000},
        "grok": {"model_name": "grok-2-1212", "temperature": 0.4, "top_p": 0.9, "max_tokens": 4000}
    }

    # Logging
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    # Checkpointer initialization
    MEMORY_MANAGER = MemoryManager()
    checkpointer = LangGraphMemoryCheckpointer(MEMORY_MANAGER)  # Changed to checkpointer for consistency

    @classmethod
    def get_api_key(cls, provider: str) -> str:
        key_map = {"openai": cls.OPENAI_API_KEY, "anthropic": cls.ANTHROPIC_API_KEY, "grok": cls.XAI_API_KEY}
        key = key_map.get(provider)
        if not key:
            raise ValueError(f"{provider.upper()}_API_KEY not set")
        return key


class ModelConfig(BaseModel):
    provider: Literal["openai", "anthropic", "grok"]
    model_name: str
    temperature: float = 0.7
    max_tokens: int = 2048


def get_model_instance(provider: str, **kwargs) -> BaseChatModel:
    config_data = {**Config.MODEL_CONFIGS[provider], **kwargs}
    config_obj = ModelConfig(provider=provider, **config_data)
    params = config_obj.dict(exclude={"provider"})
    if provider == "anthropic":
        return ChatAnthropic(api_key=Config.get_api_key("anthropic"), **params)
    elif provider in ["openai", "grok"]:
        return ChatOpenAI(api_key=Config.get_api_key(provider), **params)
    raise ValueError(f"Unsupported provider: {provider}")


@lru_cache(maxsize=Config.LLM_CACHE_SIZE)
def get_llm(configurable: Dict[str, Any] = {}) -> BaseChatModel:
    provider = configurable.get("provider", Config.DEFAULT_PROVIDER)
    model_kwargs = configurable.get("model_kwargs", {})
    llm = get_model_instance(provider, **model_kwargs)
    return llm


def create_model_config(model: Optional[str] = None, system_message: Optional[str] = None, **kwargs) -> RunnableConfig:
    validated = ConfigSchema(
        model=model or Config.DEFAULT_MODEL,
        system_message=system_message,
        temperature=kwargs.get("temperature", Config.DEFAULT_TEMPERATURE),
        provider=kwargs.get("provider", Config.DEFAULT_PROVIDER),
        top_p=kwargs.get("top_p", 0.9),
        max_tokens=kwargs.get("max_tokens", 4000)
    )
    return {"configurable": validated}


__all__ = ["Config", "get_llm", "create_model_config"]

================
File: langstuff_multi_agent/utils/memory.py
================
"""
Memory management for the LangGraph multi-agent system.
Stores and retrieves conversation memories and full SupervisorState with persistence.
"""

import json
import os
from typing import List, TypedDict, Optional, Annotated, Dict
from datetime import datetime, timedelta
from langchain_core.runnables.config import RunnableConfig
from langgraph.checkpoint.base import BaseCheckpointSaver, Checkpoint
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langgraph.graph.message import add_messages
import operator


class MemoryTriple(TypedDict):
    subject: str
    predicate: str
    object_: str
    timestamp: str


class SupervisorState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add, add_messages]
    next: str
    error_count: Annotated[int, operator.add]  # Updated to match supervisor.py
    reasoning: Optional[str]
    memory_triples: List[MemoryTriple]


class MemoryManager:
    def __init__(self, memory_path: str = "memory_store.json", state_dir: str = "state_store"):
        self.memories: Dict[str, List[MemoryTriple]] = {}
        self.memory_path = memory_path
        self.state_dir = state_dir
        os.makedirs(state_dir, exist_ok=True)
        self.load_memories_from_disk()

    def save_memory(self, user_id: str, memories: List[Dict[str, str]]) -> None:
        """Save memories for a user with timestamps."""
        if user_id not in self.memories:
            self.memories[user_id] = []
        for memory in memories:
            memory["timestamp"] = datetime.now().isoformat()
            self.memories[user_id].append(memory)
        self.save_memories_to_disk()

    def search_memories(self, user_id: str, query: str, k: int = 3) -> List[MemoryTriple]:
        """Return k most recent memories matching the query."""
        if user_id not in self.memories:
            return []
        relevant = [m for m in self.memories[user_id] if query.lower() in f"{m['subject']} {m['predicate']} {m['object_']}".lower()]
        return sorted(relevant, key=lambda x: x["timestamp"], reverse=True)[:k]

    def delete_old_memories(self, user_id: str, days: int = 30) -> None:
        """Delete memories older than specified days."""
        if user_id not in self.memories:
            return
        cutoff = datetime.now() - timedelta(days=days)
        self.memories[user_id] = [
            m for m in self.memories[user_id]
            if datetime.fromisoformat(m["timestamp"]) > cutoff
        ]
        self.save_memories_to_disk()

    def save_state(self, user_id: str, state: SupervisorState) -> None:
        """Save full SupervisorState to disk."""
        state_path = os.path.join(self.state_dir, f"{user_id}.json")
        serializable_state = {
            "messages": [msg.to_json() for msg in state["messages"]],
            "next": state["next"],
            "error_count": state["error_count"],
            "reasoning": state["reasoning"],
            "memory_triples": state["memory_triples"]
        }
        with open(state_path, "w", encoding="utf-8") as f:
            json.dump(serializable_state, f)

    def load_state(self, user_id: str) -> Optional[SupervisorState]:
        """Load full SupervisorState from disk."""
        state_path = os.path.join(self.state_dir, f"{user_id}.json")
        if os.path.exists(state_path):
            with open(state_path, "r", encoding="utf-8") as f:
                serializable_state = json.load(f)
                messages = []
                for msg_dict in serializable_state["messages"]:
                    role = msg_dict.get("kwargs", {}).get("role") or msg_dict.get("type")
                    content = msg_dict.get("kwargs", {}).get("content", "")
                    if role == "human":
                        messages.append(HumanMessage(content=content))
                    elif role == "ai":
                        messages.append(AIMessage(content=content))
                    elif role == "system":
                        messages.append(SystemMessage(content=content))
                    elif role == "tool":
                        messages.append(ToolMessage(
                            content=content,
                            tool_call_id=msg_dict.get("kwargs", {}).get("tool_call_id", ""),
                            name=msg_dict.get("kwargs", {}).get("name", "")
                        ))
                    else:
                        messages.append(BaseMessage(type=role, content=content))
                state = {
                    "messages": messages,
                    "next": serializable_state["next"],
                    "error_count": serializable_state["error_count"],
                    "reasoning": serializable_state["reasoning"],
                    "memory_triples": serializable_state["memory_triples"]
                }
                return state
        return None

    def save_memories_to_disk(self) -> None:
        """Persist memories to disk."""
        with open(self.memory_path, "w", encoding="utf-8") as f:
            json.dump(self.memories, f)

    def load_memories_from_disk(self) -> None:
        """Load memories from disk if available."""
        if os.path.exists(self.memory_path):
            with open(self.memory_path, "r", encoding="utf-8") as f:
                self.memories = json.load(f)


class LangGraphMemoryCheckpointer(BaseCheckpointSaver):
    def __init__(self, memory_manager: MemoryManager, max_history: int = 5):
        self.mm = memory_manager
        self.max_history = max_history

    def get(self, config: RunnableConfig) -> Checkpoint:
        """Retrieve checkpoint with full SupervisorState."""
        user_id = config["configurable"].get("user_id", "global")
        state = self.mm.load_state(user_id)
        if state is None:
            state = {
                "messages": [],
                "next": "supervisor",
                "error_count": 0,
                "reasoning": None,
                "memory_triples": self.mm.search_memories(user_id, "recent", self.max_history)
            }
        return Checkpoint(v=state)

    def put(self, config: RunnableConfig, checkpoint: Checkpoint) -> None:
        """Store checkpoint with full SupervisorState."""
        user_id = config["configurable"].get("user_id", "global")
        state = checkpoint["v"]
        if "memory_triples" in state:
            self.mm.save_memory(user_id, state["memory_triples"])
        state["memory_triples"] = self.mm.search_memories(user_id, "recent", self.max_history)
        self.mm.save_state(user_id, state)


memory_manager = MemoryManager()
checkpointer = LangGraphMemoryCheckpointer(memory_manager)

__all__ = ["MemoryManager", "LangGraphMemoryCheckpointer", "memory_manager", "checkpointer"]

================
File: langstuff_multi_agent/utils/tools.py
================
"""
This module defines utility tools for the LangGraph multi-agent AI project.
Each tool is decorated with @tool for LangGraph compatibility and is fully functional.

Tools include:
  - search_web: Web search via SerpAPI.
  - python_repl: Execute Python code safely.
  - read_file: Read file contents.
  - write_file: Write to a file.
  - calendar_tool: Append to a local calendar file.
  - task_tracker_tool: Manage tasks in SQLite.
  - job_search_tool: Job search via SerpAPI.
  - get_current_weather: Weather data via OpenWeatherMap.
  - calc_tool: Safe mathematical evaluation.
  - news_tool: News headlines via NewsAPI.
  - save_memory: Save conversation memories.
  - search_memories: Search stored memories.
"""

import os
import requests
import sqlite3
import io
import contextlib
from langchain_core.tools import tool
from typing import List, Dict, Any
from langstuff_multi_agent.config import get_llm
from langgraph.prebuilt import ToolNode
from langchain_core.runnables.config import RunnableConfig
from langchain_core.messages import AIMessage

# Initialize SQLite for task tracking
def init_task_db():
    conn = sqlite3.connect("tasks.db")
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS tasks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_details TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

init_task_db()

def has_tool_calls(messages: List[Any]) -> bool:
    """
    Check if the last message in a list contains tool calls.
    """
    if not messages or not isinstance(messages, list):
        return False
    last_message = messages[-1]
    if isinstance(last_message, dict):
        return bool(last_message.get("tool_calls", []) or last_message.get("function_call"))
    elif hasattr(last_message, "tool_calls"):
        return bool(last_message.tool_calls)
    return False

# --- Tools ---
@tool(return_direct=True)
def search_web(query: str) -> str:
    """Perform a web search using SerpAPI."""
    api_key = os.environ.get("SERPAPI_API_KEY")
    if not api_key:
        return "Error: SERPAPI_API_KEY not set"
    params = {"engine": "google", "q": query, "api_key": api_key, "num": 5}
    try:
        response = requests.get("https://serpapi.com/search", params=params)
        response.raise_for_status()
        data = response.json()
        results = [f"{r.get('title', 'No title')}: {r.get('snippet', '')} ({r.get('link', '')})"
                  for r in data.get("organic_results", [])]
        return "\n".join(results) or "No results found"
    except requests.RequestException as e:
        return f"Error performing web search: {str(e)}"

@tool
def python_repl(code: str) -> str:
    """Execute Python code in a restricted environment."""
    safe_builtins = {"print": print, "range": range, "len": len, "str": str, "int": int,
                     "float": float, "bool": bool, "list": list, "dict": dict, "set": set,
                     "min": min, "max": max, "sum": sum}
    restricted_globals = {"__builtins__": safe_builtins}
    output = io.StringIO()
    try:
        with contextlib.redirect_stdout(output):
            exec(code, restricted_globals, {})
        return output.getvalue() or "Code executed successfully"
    except Exception as e:
        return f"Error: {str(e)}"

@tool
def read_file(filepath: str) -> str:
    """Read content from a file."""
    try:
        with open(filepath, 'r', encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        return f"Error reading file '{filepath}': {str(e)}"

@tool
def write_file(filepath: str, content: str) -> str:
    """Write content to a file."""
    try:
        with open(filepath, 'w', encoding="utf-8") as file:
            file.write(content)
        return f"Successfully wrote to '{filepath}'"
    except Exception as e:
        return f"Error writing to file '{filepath}': {str(e)}"

@tool
def calendar_tool(event_details: str) -> str:
    """Add an event to a local calendar file."""
    try:
        with open("calendar.txt", "a", encoding="utf-8") as f:
            f.write(event_details + "\n")
        return f"Event added: {event_details}"
    except Exception as e:
        return f"Error updating calendar: {str(e)}"

@tool
def task_tracker_tool(task_details: str) -> str:
    """Add a task to the SQLite task tracker."""
    try:
        conn = sqlite3.connect("tasks.db")
        c = conn.cursor()
        c.execute("INSERT INTO tasks (task_details) VALUES (?)", (task_details,))
        conn.commit()
        task_id = c.lastrowid
        conn.close()
        return f"Task {task_id} added: {task_details}"
    except Exception as e:
        return f"Error adding task: {str(e)}"

@tool(return_direct=True)
def job_search_tool(query: str) -> str:
    """Perform a job search using SerpAPI."""
    api_key = os.environ.get("SERPAPI_API_KEY")
    if not api_key:
        return "Error: SERPAPI_API_KEY not set"
    params = {"engine": "google_jobs", "q": query, "api_key": api_key}
    try:
        response = requests.get("https://serpapi.com/search", params=params)
        response.raise_for_status()
        data = response.json()
        results = [f"{j.get('title', 'No title')} at {j.get('company', 'Unknown')} in {j.get('location', 'Unknown')}: {j.get('snippet', '')}"
                  for j in data.get("job_results", [])]
        return "\n".join(results) or "No job listings found"
    except requests.RequestException as e:
        return f"Error performing job search: {str(e)}"

@tool(return_direct=True)
def get_current_weather(location: str) -> str:
    """Retrieve current weather using OpenWeatherMap."""
    api_key = os.environ.get("OPENWEATHER_API_KEY")
    if not api_key:
        return "Error: OPENWEATHER_API_KEY not set"
    params = {"q": location, "appid": api_key, "units": "imperial"}
    try:
        response = requests.get("http://api.openweathermap.org/data/2.5/weather", params=params)
        response.raise_for_status()
        data = response.json()
        temp = data["main"]["temp"]
        wind_speed = data["wind"]["speed"]
        wind_direction = data["wind"].get("deg", "N/A")
        return f"Current weather in {location}: {temp}F, wind {wind_speed} mph at {wind_direction}"
    except requests.RequestException as e:
        return f"Error fetching weather: {str(e)}"

@tool
def calc_tool(expression: str) -> str:
    """Evaluate a mathematical expression safely."""
    safe_builtins = {"abs": abs, "round": round, "min": min, "max": max, "sum": sum}
    try:
        result = eval(expression, {"__builtins__": safe_builtins}, {})
        return str(result)
    except Exception as e:
        return f"Error evaluating expression: {str(e)}"

@tool(return_direct=True)
def news_tool(topic: str) -> str:
    """Retrieve news headlines using NewsAPI."""
    api_key = os.environ.get("NEWSAPI_API_KEY")
    if not api_key:
        return "Error: NEWSAPI_API_KEY not set"
    params = {"q": topic, "apiKey": api_key, "pageSize": 5, "sortBy": "relevancy"}
    try:
        response = requests.get("https://newsapi.org/v2/everything", params=params)
        response.raise_for_status()
        data = response.json()
        results = [f"{a.get('title', 'No title')} ({a.get('source', {}).get('name', 'Unknown')})"
                  for a in data.get("articles", [])]
        return "\n".join(results) or "No news found"
    except requests.RequestException as e:
        return f"Error fetching news: {str(e)}"

# Memory tools
memory_manager = None  # Initialized lazily

@tool
def save_memory(memories: List[Dict[str, str]], config: RunnableConfig) -> str:
    """Save important facts about users or conversations."""
    global memory_manager
    if memory_manager is None:
        from langstuff_multi_agent.utils.memory import MemoryManager
        memory_manager = MemoryManager()
    user_id = config.get("configurable", {}).get("user_id", "global")
    memory_manager.save_memory(user_id, memories)
    return "Memories saved successfully"

@tool
def search_memories(query: str, config: RunnableConfig) -> List[str]:
    """Search long-term conversation memories."""
    global memory_manager
    if memory_manager is None:
        from langstuff_multi_agent.utils.memory import MemoryManager
        memory_manager = MemoryManager()
    user_id = config.get("configurable", {}).get("user_id", "global")
    results = memory_manager.search_memories(user_id, query)
    return [f"{r['subject']} {r['predicate']} {r['object_']}" for r in results]

# Tool collection and node
def get_tools():
    """Return list of all tools."""
    return [
        search_web, python_repl, read_file, write_file, calendar_tool,
        task_tracker_tool, job_search_tool, get_current_weather, calc_tool,
        news_tool, save_memory, search_memories
    ]

tool_node = ToolNode(get_tools())

# Explicitly define tool names to avoid StructuredTool attribute issues
tool_names = [
    "search_web", "python_repl", "read_file", "write_file", "calendar_tool",
    "task_tracker_tool", "job_search_tool", "get_current_weather", "calc_tool",
    "news_tool", "save_memory", "search_memories"
]

__all__ = ["tool_node", "has_tool_calls", "get_tools"] + tool_names

================
File: requirements.txt
================
langgraph>=0.0.20
langchain-anthropic>=0.0.10
langchain-core
langchain-openai
python-dotenv
tavily-python
langchain_community>=0.3.17
pypika
embedchain
