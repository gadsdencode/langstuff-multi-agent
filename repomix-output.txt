This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-10T11:25:41.847Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
agents/analyst.py
agents/coder.py
agents/context_manager.py
agents/debugger.py
agents/general_assistant.py
agents/life_coach.py
agents/professional_coach.py
agents/project_manager.py
agents/researcher.py
agents/supervisor.py
config.py
langgraph.json
main.py
requirements.txt
utils/tools.py

================================================================
Files
================================================================

================
File: agents/analyst.py
================
# agents/analyst.py
# This file defines the Analyst Agent workflow.
# The Analyst Agent specializes in data analysis, performing calculations, and interpreting results.
# It leverages tools such as search_web, python_repl, and calc_tool.
# Powered by ChatAnthropic (Claude‑2) with a ToolNode for analytical tasks.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, calc_tool
from langchain_anthropic import ChatAnthropic


analyst_workflow = StateGraph(MessagesState)

# Define tools for analysis tasks
tools = [search_web, python_repl, calc_tool]
tool_node = ToolNode(tools)

# Bind the LLM with analytical tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node for data analysis with a detailed system prompt
analyst_workflow.add_node(
    "analyze_data",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are an Analyst Agent. Your task is to analyze data, perform calculations, and interpret results.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Retrieve background information and data.\n"
                            "- python_repl: Run Python code to perform calculations and tests.\n"
                            "- calc_tool: Execute specific calculations and numerical analysis.\n\n"
                            "Instructions:\n"
                            "1. Review the data or query provided by the user.\n"
                            "2. Perform necessary calculations and analyze the results.\n"
                            "3. Summarize your findings in clear, concise language."
                        ),
                    }
                ]
            )
        ]
    },
)
analyst_workflow.add_node("tools", tool_node)

# Define control flow edges
analyst_workflow.add_edge(START, "analyze_data")
analyst_workflow.add_edge(
    "analyze_data",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
analyst_workflow.add_edge("tools", "analyze_data")
analyst_workflow.add_edge(
    "analyze_data",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/coder.py
================
# agents/coder.py
# This file defines the Coder Agent workflow.
# The Coder Agent assists with writing, debugging, and improving code.
# It utilizes tools like search_web, python_repl, read_file, and write_file.
# The agent is powered by ChatAnthropic (Claude‑2) and uses a ToolNode for code-related tasks.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, read_file, write_file
from langchain_anthropic import ChatAnthropic

coder_workflow = StateGraph(MessagesState)

# Define tools for coding tasks
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)

# Bind the LLM with the coding tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node for coding with a system prompt that guides code writing and debugging
coder_workflow.add_node(
    "code",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Coder Agent. Your task is to write, debug, and improve code.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Find coding examples and documentation.\n"
                            "- python_repl: Execute and test Python code snippets.\n"
                            "- read_file: Retrieve code from files.\n"
                            "- write_file: Save code modifications to files.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code or coding request.\n"
                            "2. Provide solutions, test code, and explain your reasoning.\n"
                            "3. Use the available tools to execute code and verify fixes as necessary."
                        ),
                    }
                ]
            )
        ]
    },
)
coder_workflow.add_node("tools", tool_node)

# Define control flow edges
coder_workflow.add_edge(START, "code")
coder_workflow.add_edge(
    "code",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
coder_workflow.add_edge("tools", "code")
coder_workflow.add_edge(
    "code",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/context_manager.py
================
# agents/context_manager.py
# This file defines the Context Manager Agent workflow.
# The Context Manager Agent tracks conversation context, summarizes important details,
# and can use tools such as search_web, read_file, and write_file.
# It uses ChatAnthropic (Claude‑2) and a ToolNode to perform these tasks.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, read_file, write_file
from langchain_anthropic import ChatAnthropic

context_manager_workflow = StateGraph(MessagesState)

# Define tools for context management
tools = [search_web, read_file, write_file]
tool_node = ToolNode(tools)

# Bind the LLM with tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node that manages context with a system prompt
context_manager_workflow.add_node(
    "manage_context",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Context Manager Agent. Your task is to track and manage conversation context.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Keep track of key information and topics discussed in the conversation.\n"
                            "2. Summarize important points and decisions made.\n"
                            "3. Use read_file and write_file to store and retrieve context information.\n"
                            "4. If necessary, use search_web to gather additional context.\n"
                            "5. Ensure that the conversation stays focused and relevant."
                        ),
                    }
                ]
            )
        ]
    },
)
context_manager_workflow.add_node("tools", tool_node)

# Define the control flow edges (mirroring the debugger workflow structure)
context_manager_workflow.add_edge(START, "manage_context")
context_manager_workflow.add_edge(
    "manage_context",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
context_manager_workflow.add_edge("tools", "manage_context")
context_manager_workflow.add_edge(
    "manage_context",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/debugger.py
================
# debugger.py
# This file defines the Debugger Agent workflow for LangGraph.
# The Debugger Agent is responsible for analyzing code, identifying errors,
# and optionally using tools such as search_web, python_repl, read_file, and write_file.
# It uses ChatAnthropic (Claude‑2) as the underlying LLM and leverages a ToolNode
# to invoke tools when necessary.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, read_file, write_file
from langchain_anthropic import ChatAnthropic

debugger_workflow = StateGraph(MessagesState)

# Define the tools available to the Debugger Agent
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)

# Define the LLM and bind the tools to it
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main agent node with a system prompt detailing the agent's role and instructions
debugger_workflow.add_node(
    "analyze_code",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Debugger Agent. Your task is to identify and analyze code errors.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- python_repl: Execute Python code.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code and identify potential errors.\n"
                            "2. Use the search_web tool to find relevant information about the error or related debugging techniques.\n"
                            "3. Use the python_repl tool to execute code snippets and test potential fixes.\n"
                            "4. If necessary, use read_file and write_file to modify the code.\n"
                            "5. Provide clear and concise explanations of the error and the debugging process."
                        ),
                    }
                ]
            )
        ]
    },
)
debugger_workflow.add_node("tools", tool_node)

# Define the control flow edges:
# 1. Start at 'analyze_code'.
# 2. If any message contains tool_calls, transition from 'analyze_code' to 'tools'.
# 3. After running tools, loop back to 'analyze_code'.
# 4. If no tool_calls remain, finish.
debugger_workflow.add_edge(START, "analyze_code")
debugger_workflow.add_edge(
    "analyze_code",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
debugger_workflow.add_edge("tools", "analyze_code")
debugger_workflow.add_edge(
    "analyze_code",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

# The debugger_workflow is now complete.
# :contentReference[oaicite:0]{index=0}

================
File: agents/general_assistant.py
================
# agents/general_assistant.py
# This file defines the General Assistant Agent workflow.
# The General Assistant Agent handles a wide range of general queries and tasks.
# It uses tools such as search_web and get_current_weather.
# The agent is powered by ChatAnthropic (Claude‑2) and employs a ToolNode to fulfill requests.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather
from langchain_anthropic import ChatAnthropic

general_assistant_workflow = StateGraph(MessagesState)

# Define general assistant tools
tools = [search_web, get_current_weather]
tool_node = ToolNode(tools)

# Bind the LLM with the general assistant tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with a system prompt for general assistance
general_assistant_workflow.add_node(
    "assist",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a General Assistant Agent. Your task is to assist with a variety of general queries and tasks.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Provide general information and answer questions.\n"
                            "- get_current_weather: Retrieve current weather updates.\n\n"
                            "Instructions:\n"
                            "1. Understand the user's request.\n"
                            "2. Use the available tools to gather relevant information when needed.\n"
                            "3. Provide clear, concise, and helpful responses to assist the user."
                        ),
                    }
                ]
            )
        ]
    },
)
general_assistant_workflow.add_node("tools", tool_node)

# Define control flow edges
general_assistant_workflow.add_edge(START, "assist")
general_assistant_workflow.add_edge(
    "assist",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
general_assistant_workflow.add_edge("tools", "assist")
general_assistant_workflow.add_edge(
    "assist",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/life_coach.py
================
# agents/life_coach.py
# This file defines the Life Coach Agent workflow.
# The Life Coach Agent offers personal advice and lifestyle tips.
# It can use tools such as search_web, get_current_weather, and calendar_tool.
# It uses ChatAnthropic (Claude‑2) and a ToolNode for interactive coaching.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather, calendar_tool
from langchain_anthropic import ChatAnthropic

life_coach_workflow = StateGraph(MessagesState)

# Define tools for life coaching
tools = [search_web, get_current_weather, calendar_tool]
tool_node = ToolNode(tools)

# Bind the LLM with tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with instructions for personal and lifestyle advice
life_coach_workflow.add_node(
    "life_coach",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Life Coach Agent. Your task is to provide personal advice and lifestyle tips.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up general lifestyle tips and motivational content.\n"
                            "- get_current_weather: Provide weather updates to help plan outdoor activities.\n"
                            "- calendar_tool: Assist in scheduling and planning daily routines.\n\n"
                            "Instructions:\n"
                            "1. Listen to the user's personal queries and lifestyle challenges.\n"
                            "2. Offer practical advice and motivational support.\n"
                            "3. Use the available tools to supply additional context when necessary.\n"
                            "4. Maintain an empathetic and encouraging tone throughout the conversation."
                        ),
                    }
                ]
            )
        ]
    },
)
life_coach_workflow.add_node("tools", tool_node)

# Define control flow edges
life_coach_workflow.add_edge(START, "life_coach")
life_coach_workflow.add_edge(
    "life_coach",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
life_coach_workflow.add_edge("tools", "life_coach")
life_coach_workflow.add_edge(
    "life_coach",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/professional_coach.py
================
# agents/professional_coach.py
# This file defines the Professional Coach Agent workflow.
# The Professional Coach Agent provides career advice and job search strategies.
# It can use tools such as search_web and job_search_tool.
# It uses ChatAnthropic (Claude‑2) and a ToolNode to guide users in their career development.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, job_search_tool
from langchain_anthropic import ChatAnthropic

professional_coach_workflow = StateGraph(MessagesState)

# Define the tools for professional coaching
tools = [search_web, job_search_tool]
tool_node = ToolNode(tools)

# Bind the LLM with the available tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with a system prompt for career guidance
professional_coach_workflow.add_node(
    "coach",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Professional Coach Agent. Your task is to provide career advice and job search strategies.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search for career advice and job market trends.\n"
                            "- job_search_tool: Retrieve job listings and career opportunities.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's career-related queries.\n"
                            "2. Offer actionable advice and strategies for job searching.\n"
                            "3. Use the available tools to provide up-to-date information and resources.\n"
                            "4. Communicate in a supportive and motivational tone."
                        ),
                    }
                ]
            )
        ]
    },
)
professional_coach_workflow.add_node("tools", tool_node)

# Define control flow edges
professional_coach_workflow.add_edge(START, "coach")
professional_coach_workflow.add_edge(
    "coach",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
professional_coach_workflow.add_edge("tools", "coach")
professional_coach_workflow.add_edge(
    "coach",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/project_manager.py
================
# agents/project_manager.py
# This file defines the Project Manager Agent workflow.
# The Project Manager Agent oversees project timelines, tasks, and scheduling.
# It has access to tools like search_web, calendar_tool, and task_tracker_tool.
# It uses ChatAnthropic (Claude‑2) and a ToolNode to manage project-related activities.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, calendar_tool, task_tracker_tool
from langchain_anthropic import ChatAnthropic

project_manager_workflow = StateGraph(MessagesState)

# Define project management tools
tools = [search_web, calendar_tool, task_tracker_tool]
tool_node = ToolNode(tools)

# Bind the LLM with tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node for managing projects with detailed instructions
project_manager_workflow.add_node(
    "manage_project",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Project Manager Agent. Your task is to oversee project timelines, tasks, and scheduling.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for project management best practices.\n"
                            "- calendar_tool: Access and update project calendars.\n"
                            "- task_tracker_tool: Manage and update project task lists.\n\n"
                            "Instructions:\n"
                            "1. Review project details and timelines.\n"
                            "2. Update project schedules and task lists as needed.\n"
                            "3. Use search_web for additional project management information.\n"
                            "4. Provide clear instructions and updates regarding project progress."
                        ),
                    }
                ]
            )
        ]
    },
)
project_manager_workflow.add_node("tools", tool_node)

# Define control flow edges
project_manager_workflow.add_edge(START, "manage_project")
project_manager_workflow.add_edge(
    "manage_project",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
project_manager_workflow.add_edge("tools", "manage_project")
project_manager_workflow.add_edge(
    "manage_project",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/researcher.py
================
# agents/researcher.py
# This file defines the Researcher Agent workflow.
# The Researcher Agent gathers and summarizes news and research information.
# It uses tools such as search_web and news_tool.
# The agent is powered by ChatAnthropic (Claude‑2) and uses a ToolNode for research tasks.

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, news_tool
from langchain_anthropic import ChatAnthropic

researcher_workflow = StateGraph(MessagesState)

# Define research tools
tools = [search_web, news_tool]
tool_node = ToolNode(tools)

# Bind the LLM with research tools
llm = ChatAnthropic(model="claude-2", temperature=0).bind_tools(tools)

# Define the main node with instructions for conducting research
researcher_workflow.add_node(
    "research",
    lambda state: {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Researcher Agent. Your task is to gather and summarize news and research information.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up recent information and background data.\n"
                            "- news_tool: Retrieve the latest news headlines and articles.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's research query.\n"
                            "2. Use the available tools to gather accurate and relevant information.\n"
                            "3. Provide a clear summary of your findings."
                        ),
                    }
                ]
            )
        ]
    },
)
researcher_workflow.add_node("tools", tool_node)

# Define control flow edges
researcher_workflow.add_edge(START, "research")
researcher_workflow.add_edge(
    "research",
    "tools",
    condition=lambda state: any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)
researcher_workflow.add_edge("tools", "research")
researcher_workflow.add_edge(
    "research",
    END,
    condition=lambda state: not any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]),
)

================
File: agents/supervisor.py
================
# agents/supervisor.py
"""
Supervisor Agent module for integrating and routing individual LangGraph agent workflows.

This module uses an LLM—instantiated via the get_llm() factory function from config.py—to
classify incoming user requests and dynamically route the request to the appropriate
specialized agent workflow. The available agents include:
  DEBUGGER, CONTEXT_MANAGER, PROJECT_MANAGER, PROFESSIONAL_COACH, LIFE_COACH, CODER,
  ANALYST, RESEARCHER, and GENERAL_ASSISTANT.

Each agent workflow is compiled with persistent checkpointing enabled by passing the shared
checkpointer (Config.PERSISTENT_CHECKPOINTER) during compilation.
"""

from langgraph.graph import START, END
from langchain_core.messages import HumanMessage
from langstuff_multi_agent.config import Config, get_llm

# Import individual workflows.
from debugger import debugger_workflow
from agents.context_manager import context_manager_workflow
from agents.project_manager import project_manager_workflow
from agents.professional_coach import professional_coach_workflow


from agents.life_coach import life_coach_workflow
from agents.coder import coder_workflow
from agents.analyst import analyst_workflow
from agents.researcher import researcher_workflow
from agents.general_assistant import general_assistant_workflow

# Compile each workflow with the persistent checkpointer.
compiled_workflows = {
    "DEBUGGER": debugger_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "CONTEXT_MANAGER": context_manager_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "PROJECT_MANAGER": project_manager_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "PROFESSIONAL_COACH": professional_coach_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "LIFE_COACH": life_coach_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "CODER": coder_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "ANALYST": analyst_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "RESEARCHER": researcher_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
    "GENERAL_ASSISTANT": general_assistant_workflow.compile(checkpointer=Config.PERSISTENT_CHECKPOINTER),
}

# Define the available agent options.
AGENT_OPTIONS = [
    "DEBUGGER",
    "CONTEXT_MANAGER",
    "PROJECT_MANAGER",
    "PROFESSIONAL_COACH",
    "LIFE_COACH",
    "CODER",
    "ANALYST",
    "RESEARCHER",
    "GENERAL_ASSISTANT",
]

# Instantiate a classification LLM using the get_llm() factory.
supervisor_llm = get_llm()


class SupervisorAgent:
    """
    Routes user requests to the most appropriate specialized agent workflow.

    It uses a classification LLM to analyze the user request and returns one of:
    DEBUGGER, CONTEXT_MANAGER, PROJECT_MANAGER, PROFESSIONAL_COACH, LIFE_COACH,
    CODER, ANALYST, RESEARCHER, or GENERAL_ASSISTANT.
    """
    def __init__(self, workflows: dict, classification_llm):
        """
        :param workflows: Dictionary mapping agent keys to compiled workflow instances.
        :param classification_llm: LLM instance used to classify user requests.
        """
        self.workflows = workflows
        self.classification_llm = classification_llm

    def classify_request(self, user_request: str) -> str:
        """
        Classify the user request to select the best-suited agent.

        Constructs a prompt listing the available agents and asks the LLM to respond
        with exactly one agent key (case-insensitive).

        :param user_request: The user's input request.
        :return: An agent key from AGENT_OPTIONS.
        """
        prompt = (
            "You are a Supervisor Agent tasked with routing user requests to the most appropriate specialized agent. "
            "The available agents are: " + ", ".join(AGENT_OPTIONS) + ".\n\n"
            "Given the user request below, select the best-suited agent to handle it. "
            "Respond with exactly one of the following options (case-insensitive): DEBUGGER, CONTEXT_MANAGER, "
            "PROJECT_MANAGER, PROFESSIONAL_COACH, LIFE_COACH, CODER, ANALYST, RESEARCHER, GENERAL_ASSISTANT.\n\n"
            f"User Request: \"{user_request}\"\n\n"
            "Your answer:"
        )
        response = self.classification_llm.invoke([HumanMessage(content=prompt)])
        agent_key = response.content.strip().upper()
        if agent_key not in AGENT_OPTIONS:
            agent_key = "GENERAL_ASSISTANT"
        return agent_key

    def handle_request(self, user_request: str) -> str:
        """
        Processes the user request by routing it to the selected agent workflow.

        :param user_request: The user's query.
        :return: The final response from the chosen agent.
        """
        state = {"messages": [HumanMessage(content=user_request)]}
        agent_key = self.classify_request(user_request)
        print(f"Supervisor routed the request to: {agent_key}")
        workflow = self.workflows.get(agent_key)
        if not workflow:
            return "Error: No workflow available for the selected agent."
        result_state = workflow.invoke(state)
        if result_state.get("messages"):
            final_message = result_state["messages"][-1]
            return final_message.content
        else:
            return "Error: The selected agent did not produce any response."


if __name__ == "__main__":
    supervisor_agent = SupervisorAgent(compiled_workflows, supervisor_llm)
    user_request = input("User Request: ")
    agent_response = supervisor_agent.handle_request(user_request)
    print("\nAgent Response:")
    print(agent_response)

================
File: config.py
================
# config.py
"""
Configuration settings for the LangGraph multi-agent AI project.

This file reads critical configuration values from environment variables,
provides default settings, initializes logging, sets up a persistent checkpoint
instance using MemorySaver, and exposes a factory function (get_llm) that returns

an LLM instance based on the chosen AI provider (anthropic, openai, or grok/xai).

Supported providers:
  - "anthropic": Uses ChatAnthropic with the key from ANTHROPIC_API_KEY.
  - "openai": Uses ChatOpenAI with the key from OPENAI_API_KEY.
  - "grok" (or "xai"): Uses ChatOpenAI as an interface to Grok with the key from XAI_API_KEY and a custom base URL.
"""

import os
import logging
from langgraph.checkpoint.memory import MemorySaver

# Import provider libraries.
from langchain_anthropic import ChatAnthropic
from langchain.chat_models import ChatOpenAI


class Config:
    # REQUIRED: Anthropic API Key (for ChatAnthropic).
    # Also, ensure OPENAI_API_KEY and XAI_API_KEY are set for OpenAI and Grok respectively.
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    if not ANTHROPIC_API_KEY:
        raise ValueError("Environment variable 'ANTHROPIC_API_KEY' is required but not set.")

    # Default model settings.
    DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "grok-2-1212")
    DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", 0))

    # AI Provider: options are "anthropic", "openai", or "grok" (or "xai"). Default is "anthropic".
    AI_PROVIDER = os.environ.get("AI_PROVIDER", "xai").lower()

    # Logging configuration.
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    LOG_FORMAT = os.environ.get("LOG_FORMAT", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    # Persistent checkpointer instance for use across workflows.
    PERSISTENT_CHECKPOINTER = MemorySaver()

    @classmethod
    def init_logging(cls):
        logging.basicConfig(level=cls.LOG_LEVEL, format=cls.LOG_FORMAT)
        logging.info("Logging initialized at level: %s", cls.LOG_LEVEL)


# Initialize logging immediately.
Config.init_logging()


def get_llm(model_name: str = None, temperature: float = None):
    """
    Returns an LLM instance based on the AI_PROVIDER configuration.

    This function supports three providers:
      - "anthropic": Uses ChatAnthropic.
      - "openai": Uses ChatOpenAI with GPT-4o-mini.
      - "grok" (or "xai"): Uses ChatOpenAI as an interface to Grok with a custom base URL.

    :param model_name: Optional model name override.
    :param temperature: Optional temperature override.
    :return: An LLM instance.
    :raises ValueError: if a required API key is missing or provider is unsupported.
    """
    # Use provided parameters or fallback to defaults.
    model_name = model_name or Config.DEFAULT_MODEL
    temperature = temperature if temperature is not None else Config.DEFAULT_TEMPERATURE
    provider = Config.AI_PROVIDER

    if provider == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        model = ChatOpenAI(
            temperature=0.7,
            max_tokens=2000,
            top_p=0.95,
            model_name="gpt-4o-mini",
            api_key=api_key
        )
    elif provider == "anthropic":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment")
        model = ChatAnthropic(
            temperature=0,
            max_tokens=500,
            top_p=0.95,
            model_name=model_name,
            anthropic_api_key=api_key
        )
    elif provider in ("grok", "xai"):
        api_key = os.getenv("XAI_API_KEY")
        if not api_key:
            raise ValueError("XAI_API_KEY not found in environment")
        # Using ChatOpenAI as an interface to Grok with a custom base URL.
        model = ChatOpenAI(
            temperature=0.7,
            max_tokens=2000,
            top_p=0.95,
            model_name="grok-2-1212",
            api_key=api_key,
            base_url="https://api.x.ai/v1"
        )
    else:
        raise ValueError(f"Unsupported AI provider: {provider}")

    return model

================
File: langgraph.json
================
{
    "version": "1.0",
    "project": "LangGraph Multi-Agent AI",
    "description": "Configuration for deploying the LangGraph multi-agent AI project via LangGraph Studio.",
    "entry_point": "main.py",
    "agents": [
      {
        "name": "DEBUGGER",
        "file": "debugger.py",
        "description": "Agent responsible for debugging code."
      },
      {
        "name": "CONTEXT_MANAGER",
        "file": "agents/context_manager.py",
        "description": "Agent responsible for managing conversation context."
      },
      {
        "name": "PROJECT_MANAGER",
        "file": "agents/project_manager.py",
        "description": "Agent responsible for managing project timelines and tasks."
      },
      {
        "name": "PROFESSIONAL_COACH",
        "file": "agents/professional_coach.py",
        "description": "Agent providing professional and career guidance."
      },
      {
        "name": "LIFE_COACH",
        "file": "agents/life_coach.py",
        "description": "Agent providing lifestyle and personal advice."
      },
      {
        "name": "CODER",
        "file": "agents/coder.py",
        "description": "Agent that assists with coding tasks."
      },
      {
        "name": "ANALYST",
        "file": "agents/analyst.py",
        "description": "Agent specializing in data analysis."
      },
      {
        "name": "RESEARCHER",
        "file": "agents/researcher.py",
        "description": "Agent that gathers and summarizes research and news."
      },
      {
        "name": "GENERAL_ASSISTANT",
        "file": "agents/general_assistant.py",
        "description": "Agent for general queries and assistance."
      }
    ],
    "dependencies": [
      "langgraph",
      "langchain-anthropic",
      "langchain-core",
      "langchain-openai",
      "langstuff_multi_agent"
    ],
    "configuration": {
      "anthropic_api_key": "${ANTHROPIC_API_KEY}",

      "default_model": "claude-2",
      "default_temperature": 0,
      "checkpointer": "MemorySaver",
      "logging": {
        "level": "INFO",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
      }
    }
  }

================
File: main.py
================
# main.py
"""
Main entry point for the LangGraph Multi-Agent AI system.

This script initializes the system by loading configuration from config.py
(which sets up logging and other environment settings) and invoking the
SupervisorAgent to process incoming user requests.
"""

from config import Config  # This import initializes configuration and logging.
from agents.supervisor import SupervisorAgent, compiled_workflows, supervisor_llm


def main():
    print("Welcome to the LangGraph Multi-Agent AI System!")
    # Access a config value to confirm the configuration is loaded.
    print(f"Configuration loaded: Using model {Config.DEFAULT_MODEL} at temperature {Config.DEFAULT_TEMPERATURE}")

    user_request = input("Please enter your request: ")

    # Instantiate the SupervisorAgent with compiled workflows and the classification LLM.
    supervisor_agent = SupervisorAgent(compiled_workflows, supervisor_llm)

    # Route the user request to the appropriate agent workflow.
    response = supervisor_agent.handle_request(user_request)

    print("\nResponse from Agent:")
    print(response)


if __name__ == "__main__":
    main()

================
File: requirements.txt
================
langgraph
langchain-anthropic
langchain-core
langchain-openai

================
File: utils/tools.py
================
# langstuff_multi_agent/utils/tools.py
"""
This module defines various utility tools for the LangGraph multi-agent AI.
Each tool is decorated with @tool from langchain_core.tools for compatibility
with LangGraph. The tools include:

  - search_web: Simulates a web search.
  - python_repl: Simulates executing Python code.
  - read_file: Reads file contents.
  - write_file: Writes content to a file.
  - calendar_tool: Simulates calendar updates.
  - task_tracker_tool: Simulates task tracking updates.
  - job_search_tool: Simulates job search results.
  - get_current_weather: Simulates current weather information.
  - calc_tool: Evaluates a mathematical expression.
  - news_tool: Simulates retrieving news headlines.
"""

from langchain_core.tools import tool


@tool
def search_web(query: str) -> str:
    """
    Simulates a web search for the given query.

    :param query: The search query.
    :return: A simulated search result.
    """
    return f"Simulated search result for query: '{query}'."


@tool
def python_repl(code: str) -> str:
    """
    Simulates executing Python code.

    :param code: Python code as a string.
    :return: Simulated output from executing the code.
    """
    try:
        # WARNING: In production, executing code via eval/exec can be dangerous.
        # Here, we simulate code execution without running untrusted code.
        return f"Simulated execution output for code: '{code}'."
    except Exception as e:
        return f"Error during simulated code execution: {str(e)}"


@tool
def read_file(filepath: str) -> str:
    """
    Reads the content of a file.

    :param filepath: The path to the file.
    :return: The file's content or an error message.
    """
    try:
        with open(filepath, 'r') as file:
            return file.read()
    except Exception as e:
        return f"Error reading file '{filepath}': {str(e)}"


@tool
def write_file(params: dict) -> str:
    """
    Writes content to a file.

    Expects a dictionary with the keys:
      - 'filepath': The path to the file.
      - 'content': The content to write.

    :param params: Dictionary containing file path and content.
    :return: Success message or an error message.
    """
    try:
        filepath = params.get("filepath")
        content = params.get("content", "")
        with open(filepath, 'w') as file:
            file.write(content)
        return f"Successfully wrote to '{filepath}'."
    except Exception as e:
        return f"Error writing to file '{filepath}': {str(e)}"


@tool
def calendar_tool(event_details: str) -> str:
    """
    Simulates updating a calendar with event details.

    :param event_details: Details of the event.
    :return: Confirmation message.
    """
    return f"Calendar updated with event: {event_details}"


@tool
def task_tracker_tool(task_details: str) -> str:
    """
    Simulates updating a task tracker with task details.

    :param task_details: Details of the task.
    :return: Confirmation message.
    """
    return f"Task tracker updated with task: {task_details}"


@tool
def job_search_tool(query: str) -> str:
    """
    Simulates a job search based on the given query.

    :param query: The job search query.
    :return: Simulated job listings.
    """
    return f"Simulated job listings for query: '{query}'."


@tool
def get_current_weather(location: str) -> str:
    """
    Simulates retrieving current weather information for a given location.

    :param location: The location for which to retrieve weather.
    :return: Simulated weather details.
    """
    return f"Simulated weather for {location}: 75°F, Sunny."


@tool
def calc_tool(expression: str) -> str:
    """
    Evaluates a simple mathematical expression.

    :param expression: A string containing the mathematical expression.
    :return: The result as a string or an error message.
    """
    try:
        # Evaluate the expression safely using a restricted namespace.
        result = eval(expression, {"__builtins__": {}})
        return str(result)
    except Exception as e:
        return f"Error evaluating expression: {str(e)}"


@tool
def news_tool(topic: str) -> str:
    """
    Simulates retrieving news headlines for a given topic.

    :param topic: The news topic.
    :return: Simulated news headlines.
    """
    return f"Simulated news headlines for topic: '{topic}'."
