This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-11T09:27:10.640Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
langgraph.json
langstuff_multi_agent/__init__.py
langstuff_multi_agent/agent.py
langstuff_multi_agent/agents/analyst.py
langstuff_multi_agent/agents/coder.py
langstuff_multi_agent/agents/context_manager.py
langstuff_multi_agent/agents/debugger.py
langstuff_multi_agent/agents/general_assistant.py
langstuff_multi_agent/agents/life_coach.py
langstuff_multi_agent/agents/professional_coach.py
langstuff_multi_agent/agents/project_manager.py
langstuff_multi_agent/agents/researcher.py
langstuff_multi_agent/agents/supervisor.py
langstuff_multi_agent/config.py
langstuff_multi_agent/utils/tools.py
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
.env

================
File: langgraph.json
================
{
    "version": "1.0",
    "project": "LangGraph Multi-Agent AI",
    "description": "Configuration for deploying the LangGraph multi-agent AI project via LangGraph Studio.",
    "entry_point": "./langstuff_multi_agent/agent.py:graph",
    "graphs": {
        "main": "./langstuff_multi_agent/agent.py:graph",
        "debugger": "./langstuff_multi_agent/agent.py:debugger_graph",
        "context_manager": "./langstuff_multi_agent/agent.py:context_manager_graph",
        "project_manager": "./langstuff_multi_agent/agent.py:project_manager_graph",
        "professional_coach": "./langstuff_multi_agent/agent.py:professional_coach_graph",
        "life_coach": "./langstuff_multi_agent/agent.py:life_coach_graph",
        "coder": "./langstuff_multi_agent/agent.py:coder_graph",
        "analyst": "./langstuff_multi_agent/agent.py:analyst_graph",
        "researcher": "./langstuff_multi_agent/agent.py:researcher_graph",
        "general_assistant": "./langstuff_multi_agent/agent.py:general_assistant_graph"
    },
    "agents": [
        {
            "name": "SUPERVISOR",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "main",
            "description": "Main supervisor agent that routes requests to specialized agents."
        },
        {
            "name": "DEBUGGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "debugger",
            "description": "Agent responsible for debugging code."
        },
        {
            "name": "CONTEXT_MANAGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "context_manager",
            "description": "Agent responsible for managing conversation context."
        },
        {
            "name": "PROJECT_MANAGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "project_manager",
            "description": "Agent responsible for managing project timelines and tasks."
        },
        {
            "name": "PROFESSIONAL_COACH",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "professional_coach",
            "description": "Agent providing professional and career guidance."
        },
        {
            "name": "LIFE_COACH",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "life_coach",
            "description": "Agent providing lifestyle and personal advice."
        },
        {
            "name": "CODER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "coder",
            "description": "Agent that assists with coding tasks."
        },
        {
            "name": "ANALYST",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "analyst",
            "description": "Agent specializing in data analysis."
        },
        {
            "name": "RESEARCHER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "researcher",
            "description": "Agent that gathers and summarizes research and news."
        },
        {
            "name": "GENERAL_ASSISTANT",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "general_assistant",
            "description": "Agent for general queries and assistance."
        }
    ],
    "dependencies": [
        "langgraph>=0.0.20",
        "langchain-anthropic>=0.0.10",
        "langchain-core>=0.1.20",
        "langchain-openai>=0.0.5",
        "python-dotenv>=1.0.0",
        "tavily-python>=0.5.1",
        "langchain_community>=0.3.17",
        "./langstuff_multi_agent"
    ],
    "configuration": {
        "xai_api_key": "${XAI_API_KEY}",
        "anthropic_api_key": "${ANTHROPIC_API_KEY}",
        "openai_api_key": "${OPENAI_API_KEY}",
        "tavily_api_key": "${TAVILY_API_KEY}",
        "model_settings": {
            "default_provider": "openai",
            "default_model": "gpt-4-turbo-2024-04-09",
            "default_temperature": 0.0,
            "default_top_p": 0.1,
            "default_max_tokens": 4000
        },
        "checkpointer": "MemorySaver",
        "logging": {
            "level": "INFO",
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        }
    }
}

================
File: langstuff_multi_agent/__init__.py
================
"""
LangStuff Multi-Agent package.
A multi-agent system built with LangGraph for handling various 
specialized tasks.
"""

__version__ = "0.1.0"

================
File: langstuff_multi_agent/agent.py
================
# agent.py
"""
Main agent module that exports the graph for LangGraph Studio.

This module serves as the entry point for LangGraph Studio, exporting both
the main supervisor workflow and all individual agent workflows.
"""

import logging
from typing import Dict, Any
from langgraph.graph import Graph

from langstuff_multi_agent.agents.supervisor import (
    supervisor_workflow,
    AGENT_OPTIONS,
    workflow_map
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Initializing agent workflows...")

# Compile all workflows with their configurations
compiled_workflows: Dict[str, Graph] = {
    "supervisor": supervisor_workflow.compile(),
    **{
        name.lower(): workflow.compile()
        for name, workflow in workflow_map.items()
    }
}

# Export the main graph for LangGraph Studio
graph = compiled_workflows["supervisor"]

# Explicitly register individual agent graphs
for name, workflow in compiled_workflows.items():
    globals()[f"{name}_graph"] = workflow
    logger.info(f"Registered workflow: {name}_graph")

# Ensure metadata for all agents is accessible
agent_metadata: Dict[str, Dict[str, Any]] = {
    name.lower(): {
        "name": name,
        "description": desc,
        "graph": compiled_workflows.get(name.lower(), None)
    }
    for name, desc in AGENT_OPTIONS.items()
}

# Ensure all graphs are explicitly listed in __all__
__all__ = [
    "graph",  # Main supervisor graph
    *[f"{name.lower()}_graph" for name in compiled_workflows.keys()],  # Individual agent graphs
    "agent_metadata",  # Agent metadata
    "compiled_workflows"  # All compiled workflows
]

logger.info("Agent workflows successfully initialized.")

================
File: langstuff_multi_agent/agents/analyst.py
================
# agents/analyst.py
"""
Analyst Agent module for data analysis and interpretation.

This module provides a workflow for analyzing data and performing
calculations using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    calc_tool
)
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

analyst_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for analysis tasks
tools = [search_web, python_repl, calc_tool]
tool_node = ToolNode(tools)

def analyze_data(state, config):
    """Analyze data with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are an Analyst Agent. Your task is to analyze data, perform calculations, and interpret results.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Retrieve background information and data.\n"
                            "- python_repl: Run Python code to perform calculations and tests.\n"
                            "- calc_tool: Execute specific calculations and numerical analysis.\n\n"
                            "Instructions:\n"
                            "1. Review the data or query provided by the user.\n"
                            "2. Perform necessary calculations and analyze the results.\n"
                            "3. Summarize your findings in clear, concise language."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
analyst_workflow.add_node("analyze_data", analyze_data)
analyst_workflow.add_node("tools", tool_node)

# Define control flow edges
analyst_workflow.add_edge(START, "analyze_data")

# Add conditional edge from analyze_data to either tools or END
analyst_workflow.add_conditional_edges(
    "analyze_data",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to analyze_data
analyst_workflow.add_edge("tools", "analyze_data")

# Export the workflow
__all__ = ["analyst_workflow"]

================
File: langstuff_multi_agent/agents/coder.py
================
# agents/coder.py
"""
Coder Agent module for writing and improving code.

This module provides a workflow for code generation, debugging,
and optimization using various development tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, read_file, write_file
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

coder_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for coding tasks
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)

def code(state, config):
    """Write and improve code with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Coder Agent. Your task is to write, debug, and improve code.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Find coding examples and documentation.\n"
                            "- python_repl: Execute and test Python code snippets.\n"
                            "- read_file: Retrieve code from files.\n"
                            "- write_file: Save code modifications to files.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code or coding request.\n"
                            "2. Provide solutions, test code, and explain your reasoning.\n"
                            "3. Use the available tools to execute code and verify fixes as necessary."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
coder_workflow.add_node("code", code)
coder_workflow.add_node("tools", tool_node)

# Define control flow edges
coder_workflow.add_edge(START, "code")

# Add conditional edge from code to either tools or END
coder_workflow.add_conditional_edges(
    "code",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to code
coder_workflow.add_edge("tools", "code")

# Export the workflow
__all__ = ["coder_workflow"]

================
File: langstuff_multi_agent/agents/context_manager.py
================
# agents/context_manager.py
"""
Context Manager Agent module for tracking conversation context.

This module provides a workflow for managing conversation history
and maintaining context across interactions.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, read_file, write_file
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

context_manager_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for context management
tools = [search_web, read_file, write_file]
tool_node = ToolNode(tools)

def manage_context(state, config):
    """Manage conversation context with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Context Manager Agent. Your task is to track and manage conversation context.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Keep track of key information and topics discussed in the conversation.\n"
                            "2. Summarize important points and decisions made.\n"
                            "3. Use read_file and write_file to store and retrieve context information.\n"
                            "4. If necessary, use search_web to gather additional context.\n"
                            "5. Ensure that the conversation stays focused and relevant."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
context_manager_workflow.add_node("manage_context", manage_context)
context_manager_workflow.add_node("tools", tool_node)

# Define the control flow edges
context_manager_workflow.add_edge(START, "manage_context")

# Add conditional edge from manage_context to either tools or END
context_manager_workflow.add_conditional_edges(
    "manage_context",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to manage_context
context_manager_workflow.add_edge("tools", "manage_context")

# Export the workflow
__all__ = ["context_manager_workflow"]

================
File: langstuff_multi_agent/agents/debugger.py
================
# agents/debugger.py
"""
Debugger Agent module for analyzing code and identifying errors.

This module provides a workflow for debugging code using various tools
and LLM-based analysis.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    read_file,
    write_file
)
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

debugger_workflow = StateGraph(MessagesState, ConfigSchema)

# Define the tools available to the Debugger Agent
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)

def analyze_code(state, config):
    """Analyze code and identify errors with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Debugger Agent. Your task is to identify and analyze code errors.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- python_repl: Execute Python code.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code and identify potential errors.\n"
                            "2. Use the search_web tool to find relevant information about the error or related debugging techniques.\n"
                            "3. Use the python_repl tool to execute code snippets and test potential fixes.\n"
                            "4. If necessary, use read_file and write_file to modify the code.\n"
                            "5. Provide clear and concise explanations of the error and the debugging process."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main agent node with configuration support
debugger_workflow.add_node("analyze_code", analyze_code)
debugger_workflow.add_node("tools", tool_node)

# Define the control flow edges:
# 1. Start at 'analyze_code'.
# 2. If any message contains tool_calls, transition from 'analyze_code' to 'tools'.
# 3. After running tools, loop back to 'analyze_code'.
# 4. If no tool_calls remain, finish.
debugger_workflow.add_edge(START, "analyze_code")

# Add conditional edge from analyze_code to either tools or END
debugger_workflow.add_conditional_edges(
    "analyze_code",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to analyze_code
debugger_workflow.add_edge("tools", "analyze_code")

# Export the workflow
__all__ = ["debugger_workflow"]

================
File: langstuff_multi_agent/agents/general_assistant.py
================
# agents/general_assistant.py
"""
General Assistant Agent module for handling diverse queries.

This module provides a workflow for addressing general user requests
using a variety of tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

general_assistant_workflow = StateGraph(MessagesState, ConfigSchema)

# Define general assistant tools
tools = [search_web, get_current_weather]
tool_node = ToolNode(tools)

def assist(state, config):
    """Provide general assistance with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a General Assistant Agent. Your task is to assist with a variety of general queries and tasks.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Provide general information and answer questions.\n"
                            "- get_current_weather: Retrieve current weather updates.\n\n"
                            "Instructions:\n"
                            "1. Understand the user's request.\n"
                            "2. Use the available tools to gather relevant information when needed.\n"
                            "3. Provide clear, concise, and helpful responses to assist the user."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
general_assistant_workflow.add_node("assist", assist)
general_assistant_workflow.add_node("tools", tool_node)

# Define control flow edges
general_assistant_workflow.add_edge(START, "assist")

# Add conditional edge from assist to either tools or END
general_assistant_workflow.add_conditional_edges(
    "assist",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to assist
general_assistant_workflow.add_edge("tools", "assist")

# Export the workflow
__all__ = ["general_assistant_workflow"]

================
File: langstuff_multi_agent/agents/life_coach.py
================
# agents/life_coach.py
"""
Life Coach Agent module for personal advice and guidance.

This module provides a workflow for offering lifestyle tips and
personal development advice using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather, calendar_tool
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

life_coach_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for life coaching
tools = [search_web, get_current_weather, calendar_tool]
tool_node = ToolNode(tools)

def life_coach(state, config):
    """Provide life coaching with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Life Coach Agent. Your task is to provide personal advice and lifestyle tips.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up general lifestyle tips and motivational content.\n"
                            "- get_current_weather: Provide weather updates to help plan outdoor activities.\n"
                            "- calendar_tool: Assist in scheduling and planning daily routines.\n\n"
                            "Instructions:\n"
                            "1. Listen to the user's personal queries and lifestyle challenges.\n"
                            "2. Offer practical advice and motivational support.\n"
                            "3. Use the available tools to supply additional context when necessary.\n"
                            "4. Maintain an empathetic and encouraging tone throughout the conversation."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
life_coach_workflow.add_node("life_coach", life_coach)
life_coach_workflow.add_node("tools", tool_node)

# Define control flow edges
life_coach_workflow.add_edge(START, "life_coach")

# Add conditional edge from life_coach to either tools or END
life_coach_workflow.add_conditional_edges(
    "life_coach",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to life_coach
life_coach_workflow.add_edge("tools", "life_coach")

# Export the workflow
__all__ = ["life_coach_workflow"]

================
File: langstuff_multi_agent/agents/professional_coach.py
================
# agents/professional_coach.py
"""
Professional Coach Agent module for career guidance.

This module provides a workflow for offering career advice and
job search strategies using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, job_search_tool
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

professional_coach_workflow = StateGraph(MessagesState, ConfigSchema)

# Define the tools for professional coaching
tools = [search_web, job_search_tool]
tool_node = ToolNode(tools)

def coach(state, config):
    """Provide professional coaching with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Professional Coach Agent. Your task is to provide career advice and job search strategies.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search for career advice and job market trends.\n"
                            "- job_search_tool: Retrieve job listings and career opportunities.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's career-related queries.\n"
                            "2. Offer actionable advice and strategies for job searching.\n"
                            "3. Use the available tools to provide up-to-date information and resources.\n"
                            "4. Communicate in a supportive and motivational tone."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
professional_coach_workflow.add_node("coach", coach)
professional_coach_workflow.add_node("tools", tool_node)

# Define control flow edges
professional_coach_workflow.add_edge(START, "coach")

# Add conditional edge from coach to either tools or END
professional_coach_workflow.add_conditional_edges(
    "coach",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to coach
professional_coach_workflow.add_edge("tools", "coach")

# Export the workflow
__all__ = ["professional_coach_workflow"]

================
File: langstuff_multi_agent/agents/project_manager.py
================
# agents/project_manager.py
"""
Project Manager Agent module for task and timeline management.

This module provides a workflow for overseeing project schedules
and coordinating tasks using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    calendar_tool,
    task_tracker_tool
)
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

project_manager_workflow = StateGraph(MessagesState, ConfigSchema)

# Define project management tools
tools = [search_web, calendar_tool, task_tracker_tool]
tool_node = ToolNode(tools)

def manage_project(state, config):
    """Manage project with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Project Manager Agent. Your task is to oversee project timelines, tasks, and scheduling.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for project management best practices.\n"
                            "- calendar_tool: Access and update project calendars.\n"
                            "- task_tracker_tool: Manage and update project task lists.\n\n"
                            "Instructions:\n"
                            "1. Review project details and timelines.\n"
                            "2. Update project schedules and task lists as needed.\n"
                            "3. Use search_web for additional project management information.\n"
                            "4. Provide clear instructions and updates regarding project progress."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
project_manager_workflow.add_node("manage_project", manage_project)
project_manager_workflow.add_node("tools", tool_node)

# Define control flow edges
project_manager_workflow.add_edge(START, "manage_project")

# Add conditional edge from manage_project to either tools or END
project_manager_workflow.add_conditional_edges(
    "manage_project",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to manage_project
project_manager_workflow.add_edge("tools", "manage_project")

# Export the workflow
__all__ = ["project_manager_workflow"]

================
File: langstuff_multi_agent/agents/researcher.py
================
# agents/researcher.py
"""
Researcher Agent module for gathering and summarizing information.

This module provides a workflow for gathering and summarizing news and research 
information using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, news_tool
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

researcher_workflow = StateGraph(MessagesState, ConfigSchema)

# Define research tools
tools = [search_web, news_tool]
tool_node = ToolNode(tools)

def research(state, config):
    """Conduct research with configuration support."""
    # Get LLM with configuration
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Researcher Agent. Your task is to gather and summarize news and research information.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up recent information and background data.\n"
                            "- news_tool: Retrieve the latest news headlines and articles.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's research query.\n"
                            "2. Use the available tools to gather accurate and relevant information.\n"
                            "3. Provide a clear summary of your findings."
                        ),
                    }
                ]
            )
        ]
    }

# Define the main node with configuration support
researcher_workflow.add_node("research", research)
researcher_workflow.add_node("tools", tool_node)

# Define control flow edges
researcher_workflow.add_edge(START, "research")

# Add conditional edge from research to either tools or END
researcher_workflow.add_conditional_edges(
    "research",
    lambda state: "tools" if any(hasattr(msg, "tool_calls") and msg.tool_calls for msg in state["messages"]) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

# Add edge from tools back to research
researcher_workflow.add_edge("tools", "research")

# Export the workflow
__all__ = ["researcher_workflow"]

================
File: langstuff_multi_agent/agents/supervisor.py
================
# agents/supervisor.py
"""
Supervisor Agent module for integrating and routing individual LangGraph 
agent workflows.

This module uses an LLM—instantiated via the get_llm() factory function 
from langstuff_multi_agent/config.py—to classify incoming user requests 
and dynamically route the request to the appropriate specialized agent 
workflow. The available agents include:
  DEBUGGER, CONTEXT_MANAGER, PROJECT_MANAGER, PROFESSIONAL_COACH, 
  LIFE_COACH, CODER, ANALYST, RESEARCHER, and GENERAL_ASSISTANT.

Each agent workflow is compiled with persistent checkpointing enabled 
by explicitly passing the shared checkpointer 
(Config.PERSISTENT_CHECKPOINTER) during compilation.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langstuff_multi_agent.config import Config, get_llm, ConfigSchema, create_model_config

# Import individual workflows.
from langstuff_multi_agent.agents.debugger import (
    debugger_workflow
)
from langstuff_multi_agent.agents.context_manager import (
    context_manager_workflow
)
from langstuff_multi_agent.agents.project_manager import (
    project_manager_workflow
)
from langstuff_multi_agent.agents.professional_coach import (
    professional_coach_workflow
)
from langstuff_multi_agent.agents.life_coach import (
    life_coach_workflow
)
from langstuff_multi_agent.agents.coder import (
    coder_workflow
)
from langstuff_multi_agent.agents.analyst import (
    analyst_workflow
)
from langstuff_multi_agent.agents.researcher import (
    researcher_workflow
)
from langstuff_multi_agent.agents.general_assistant import (
    general_assistant_workflow
)

# Create supervisor workflow with configuration schema
supervisor_workflow = StateGraph(MessagesState, ConfigSchema)

# Define the available agent options with descriptions
AGENT_OPTIONS = {
    "DEBUGGER": "Code debugging and error analysis",
    "CONTEXT_MANAGER": "Conversation context tracking and management",
    "PROJECT_MANAGER": "Project timeline and task management",
    "PROFESSIONAL_COACH": "Career advice and job search strategies",
    "LIFE_COACH": "Personal development and lifestyle guidance",
    "CODER": "Code writing and improvement",
    "ANALYST": "Data analysis and interpretation",
    "RESEARCHER": "Information gathering and research",
    "GENERAL_ASSISTANT": "General purpose assistance"
}

# Map agent names to their workflows
workflow_map = {
    "DEBUGGER": debugger_workflow,
    "CONTEXT_MANAGER": context_manager_workflow,
    "PROJECT_MANAGER": project_manager_workflow,
    "PROFESSIONAL_COACH": professional_coach_workflow,
    "LIFE_COACH": life_coach_workflow,
    "CODER": coder_workflow,
    "ANALYST": analyst_workflow,
    "RESEARCHER": researcher_workflow,
    "GENERAL_ASSISTANT": general_assistant_workflow
}

def format_agent_list():
    """Format the agent list with descriptions for the prompt."""
    return "\n".join(f"- {name}: {desc}" for name, desc in AGENT_OPTIONS.items())

def route_request(state, config):
    """Route the user request to appropriate agent workflow with enhanced feedback."""
    messages = state.get("messages", [])
    if not messages:
        return {
            "messages": [
                AIMessage(content="Welcome! I'm your AI assistant. How can I help you today?")
            ]
        }
    
    request = messages[-1].content
    
    # Create a detailed prompt for agent selection
    prompt = (
        "You are a Supervisor Agent tasked with routing user requests to the most appropriate specialized agent.\n\n"
        "Available agents and their specialties:\n"
        f"{format_agent_list()}\n\n"
        f"User Request: '{request}'\n\n"
        "Instructions:\n"
        "1. Analyze the user's request carefully\n"
        "2. Select the most appropriate agent based on their specialties\n"
        "3. Respond with exactly one agent name from the list (case-insensitive)\n"
        "4. If unsure, use GENERAL_ASSISTANT\n\n"
        "Selected agent:"
    )
    
    try:
        # Get supervisor LLM with configuration
        supervisor_llm = get_llm(config.get("configurable", {}))
        
        # Get agent selection
        response = supervisor_llm.invoke([SystemMessage(content=prompt)])
        agent_key = response.content.strip().upper()
        
        if agent_key not in AGENT_OPTIONS:
            agent_key = "GENERAL_ASSISTANT"
        
        # Compile and invoke the selected workflow with configuration
        workflow = workflow_map[agent_key].compile()
        result = workflow.invoke(
            {"messages": [
                SystemMessage(content=f"You are the {agent_key} agent, specialized in {AGENT_OPTIONS[agent_key]}."),
                HumanMessage(content=request)
            ]},
            config=config
        )
        
        # Add routing information to the response
        return {
            "messages": [
                AIMessage(content=f"[Routing to {agent_key} - {AGENT_OPTIONS[agent_key]}]\n\n"),
                *result["messages"]
            ]
        }
        
    except Exception as e:
        return {
            "messages": [
                AIMessage(content=f"I apologize, but I encountered an error while processing your request: {str(e)}\n\n"
                                "Please try rephrasing your request or contact support if the issue persists.")
            ]
        }

# Add the routing node with configuration support
supervisor_workflow.add_node("route", route_request)

# Add nodes for each agent with proper configuration
for agent_name, workflow in workflow_map.items():
    # Create a wrapper function to ensure configuration is passed correctly
    def create_agent_node(agent_workflow, agent_name):
        def agent_node(state, config):
            # Compile the workflow with configuration
            compiled_workflow = agent_workflow.compile()
            # Execute the workflow with the current state and config
            result = compiled_workflow.invoke(state, config=config)
            return result
        return agent_node
    
    # Add the node with the wrapped function
    supervisor_workflow.add_node(
        agent_name.lower(),
        create_agent_node(workflow, agent_name)
    )

# Define edges
supervisor_workflow.add_edge(START, "route")
for agent_name in workflow_map.keys():
    supervisor_workflow.add_edge("route", agent_name.lower())
    supervisor_workflow.add_edge(agent_name.lower(), END)

# Export the workflow
__all__ = ["supervisor_workflow"]

================
File: langstuff_multi_agent/config.py
================
# config.py
"""
Configuration settings for the LangGraph multi-agent AI project.

This file reads critical configuration values from environment variables,
provides default settings, initializes logging, sets up a persistent checkpoint
instance using MemorySaver, and exposes configuration and factory functions for LangGraph.

Supported providers:
  - "anthropic": Uses ChatAnthropic with the key from ANTHROPIC_API_KEY.
  - "openai": Uses ChatOpenAI with the key from OPENAI_API_KEY.
  - "grok" (or "xai"): Uses ChatOpenAI as an interface to Grok with the key from XAI_API_KEY.
"""

import os
import logging
from langgraph.checkpoint.memory import MemorySaver
from typing import Optional, Dict, Any, Literal
from typing_extensions import TypedDict
from langchain_core.messages import SystemMessage
from langchain_core.runnables.config import RunnableConfig
from pydantic import BaseModel, ValidationError

# Import provider libraries.
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel


class ConfigSchema(TypedDict):
    """Schema for LangGraph runtime configuration."""
    model: Optional[str]
    system_message: Optional[str]
    temperature: Optional[float]
    top_p: Optional[float]
    max_tokens: Optional[int]


class Config:
    # API Keys
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    XAI_API_KEY = os.environ.get("XAI_API_KEY")

    # Default model settings
    DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "gpt-4o")
    DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", 0))
    DEFAULT_PROVIDER = os.environ.get("AI_PROVIDER", "openai").lower()

    # Model configurations
    MODEL_CONFIGS = {
        "anthropic": {
            "model_name": "claude-3-opus-20240229",
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        },
        "openai": {
            "model_name": "gpt-4o",  # Latest GA model
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        },
        "grok": {
            "model_name": "gpt-4o",  # Fallback to latest OpenAI model
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        }
    }

    # Logging configuration
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Persistent checkpointer instance
    PERSISTENT_CHECKPOINTER = MemorySaver()

    @classmethod
    def init_logging(cls):
        """Initialize logging with configured settings."""
        logging.basicConfig(level=cls.LOG_LEVEL, format=cls.LOG_FORMAT)
        logging.info("Logging initialized at level: %s", cls.LOG_LEVEL)

    @classmethod
    def get_api_key(cls, provider: str) -> str:
        """Get the API key for the specified provider."""
        key_map = {
            "openai": ("OPENAI_API_KEY", cls.OPENAI_API_KEY),
            "anthropic": ("ANTHROPIC_API_KEY", cls.ANTHROPIC_API_KEY),
            "grok": ("XAI_API_KEY", cls.XAI_API_KEY),
        }

        env_var, key = key_map.get(provider, (None, None))
        if not key:
            raise ValueError(f"{env_var} environment variable not set")
        return key


# Initialize logging immediately
Config.init_logging()


class ModelConfig(BaseModel):
    """Validation schema for LLM configurations"""
    provider: Literal['openai', 'anthropic', 'grok', 'azure_openai']
    model_name: str
    api_key: str
    temperature: float = 0.7
    max_tokens: int = 2048
    streaming: bool = True


def get_model_instance(provider: str, **kwargs):
    # Validate provider first
    if not provider or provider not in Config.MODEL_CONFIGS:
        available = list(Config.MODEL_CONFIGS.keys())
        raise ValueError(f"Invalid LLM provider: {provider}. Available: {available}")

    try:
        # Validate configuration against schema
        config = ModelConfig(
            provider=provider,
            **{**Config.MODEL_CONFIGS[provider], **kwargs}
        )
    except ValidationError as e:
        error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
        raise ValueError(
            f"Invalid model configuration:\n{'\n'.join(error_messages)}"
        )

    if provider == "anthropic":
        return ChatAnthropic(
            api_key=Config.get_api_key("anthropic"),
            **config.model_dump()
        )
    elif provider in ["openai"]:
        return ChatOpenAI(
            api_key=Config.get_api_key("openai"),
            **config.model_dump()
        )
    elif provider in ["grok"]:
        return ChatOpenAI(
            api_key=Config.get_api_key("grok"),
            **config.model_dump()
        )
    else:
        raise ValueError(f"Unsupported provider: {provider}")


def get_llm(configurable: dict = {}):
    """
    Factory function to create a language model instance based on configuration.

    Args:
        configurable: Optional configuration dictionary that can include:
               - model: Provider name ("anthropic", "openai", "grok")
               - system_message: Optional system message to prepend
               - temperature: Temperature parameter for generation
               - top_p: Top-p parameter for generation
               - max_tokens: Maximum tokens to generate

    Returns:
        An instance of BaseChatModel configured according to the specified parameters
    """
    provider = configurable.get('provider', 'openai')  # Set default provider
    model_kwargs = configurable.get('model_kwargs', {})

    return get_model_instance(provider, **model_kwargs)


def create_model_config(
    model: Optional[str] = None,
    system_message: Optional[str] = None,
    **kwargs
) -> RunnableConfig:
    """
    Create a RunnableConfig for LangGraph workflow configuration.

    Args:
        model: Optional model provider to use
        system_message: Optional system message to prepend
        **kwargs: Additional configuration parameters

    Returns:
        RunnableConfig with the specified configuration
    """
    config = {"model": model} if model else {}
    if system_message:
        config["system_message"] = system_message
    config.update(kwargs)

    return {"configurable": config}

================
File: langstuff_multi_agent/utils/tools.py
================
# langstuff_multi_agent/utils/tools.py
"""
This module defines various utility tools for the LangGraph multi-agent AI.
Each tool is decorated with @tool from langchain_core.tools for compatibility
with LangGraph. The tools include:

  - search_web: Simulates a web search.
  - python_repl: Simulates executing Python code.
  - read_file: Reads file contents.
  - write_file: Writes content to a file.
  - calendar_tool: Simulates calendar updates.
  - task_tracker_tool: Simulates task tracking updates.
  - job_search_tool: Simulates job search results.
  - get_current_weather: Simulates current weather information.
  - calc_tool: Evaluates a mathematical expression.
  - news_tool: Simulates retrieving news headlines.
"""

from langchain_core.tools import tool


@tool
def search_web(query: str) -> str:
    """
    Simulates a web search for the given query.

    :param query: The search query.
    :return: A simulated search result.
    """
    return f"Simulated search result for query: '{query}'."


@tool
def python_repl(code: str) -> str:
    """
    Simulates executing Python code.

    :param code: Python code as a string.
    :return: Simulated output from executing the code.
    """
    try:
        # WARNING: In production, executing code via eval/exec can be dangerous.
        # Here, we simulate code execution without running untrusted code.
        return f"Simulated execution output for code: '{code}'."
    except Exception as e:
        return f"Error during simulated code execution: {str(e)}"


@tool
def read_file(filepath: str) -> str:
    """
    Reads the content of a file.

    :param filepath: The path to the file.
    :return: The file's content or an error message.
    """
    try:
        with open(filepath, 'r') as file:
            return file.read()
    except Exception as e:
        return f"Error reading file '{filepath}': {str(e)}"


@tool
def write_file(params: dict) -> str:
    """
    Writes content to a file.

    Expects a dictionary with the keys:
      - 'filepath': The path to the file.
      - 'content': The content to write.

    :param params: Dictionary containing file path and content.
    :return: Success message or an error message.
    """
    try:
        filepath = params.get("filepath")
        content = params.get("content", "")
        with open(filepath, 'w') as file:
            file.write(content)
        return f"Successfully wrote to '{filepath}'."
    except Exception as e:
        return f"Error writing to file '{filepath}': {str(e)}"


@tool
def calendar_tool(event_details: str) -> str:
    """
    Simulates updating a calendar with event details.

    :param event_details: Details of the event.
    :return: Confirmation message.
    """
    return f"Calendar updated with event: {event_details}"


@tool
def task_tracker_tool(task_details: str) -> str:
    """
    Simulates updating a task tracker with task details.

    :param task_details: Details of the task.
    :return: Confirmation message.
    """
    return f"Task tracker updated with task: {task_details}"


@tool
def job_search_tool(query: str) -> str:
    """
    Simulates a job search based on the given query.

    :param query: The job search query.
    :return: Simulated job listings.
    """
    return f"Simulated job listings for query: '{query}'."


@tool
def get_current_weather(location: str) -> str:
    """
    Simulates retrieving current weather information for a given location.

    :param location: The location for which to retrieve weather.
    :return: Simulated weather details.
    """
    return f"Simulated weather for {location}: 75°F, Sunny."


@tool
def calc_tool(expression: str) -> str:
    """
    Evaluates a simple mathematical expression.

    :param expression: A string containing the mathematical expression.
    :return: The result as a string or an error message.
    """
    try:
        # Evaluate the expression safely using a restricted namespace.
        result = eval(expression, {"__builtins__": {}})
        return str(result)
    except Exception as e:
        return f"Error evaluating expression: {str(e)}"


@tool
def news_tool(topic: str) -> str:
    """
    Simulates retrieving news headlines for a given topic.

    :param topic: The news topic.
    :return: Simulated news headlines.
    """
    return f"Simulated news headlines for topic: '{topic}'."

================
File: requirements.txt
================
langgraph>=0.0.20
langchain-anthropic>=0.0.10
langchain-core>=0.1.20
langchain-openai>=0.0.5
python-dotenv>=1.0.0
tavily-python>=0.5.1
langchain_community>=0.3.17
