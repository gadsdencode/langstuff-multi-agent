This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-13T12:27:03.888Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
langgraph.json
langstuff_multi_agent/__init__.py
langstuff_multi_agent/agent.py
langstuff_multi_agent/agents/analyst.py
langstuff_multi_agent/agents/coder.py
langstuff_multi_agent/agents/context_manager.py
langstuff_multi_agent/agents/creative_content.py
langstuff_multi_agent/agents/customer_support.py
langstuff_multi_agent/agents/debugger.py
langstuff_multi_agent/agents/financial_analyst.py
langstuff_multi_agent/agents/general_assistant.py
langstuff_multi_agent/agents/life_coach.py
langstuff_multi_agent/agents/marketing_strategist.py
langstuff_multi_agent/agents/news_reporter.py
langstuff_multi_agent/agents/professional_coach.py
langstuff_multi_agent/agents/project_manager.py
langstuff_multi_agent/agents/researcher.py
langstuff_multi_agent/agents/supervisor.py
langstuff_multi_agent/config.py
langstuff_multi_agent/utils/memory.py
langstuff_multi_agent/utils/tools.py
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
.env

================
File: langgraph.json
================
{
    "version": "1.0",
    "project": "LangGraph Multi-Agent AI",
    "description": "Configuration for deploying the LangGraph multi-agent AI project via LangGraph Studio.",
    "entry_point": "./langstuff_multi_agent/agent.py:graph",
    "graphs": {
        "main": "./langstuff_multi_agent/agent.py:graph",
        "debugger": "./langstuff_multi_agent/agent.py:debugger_graph",
        "context_manager": "./langstuff_multi_agent/agent.py:context_manager_graph",
        "project_manager": "./langstuff_multi_agent/agent.py:project_manager_graph",
        "professional_coach": "./langstuff_multi_agent/agent.py:professional_coach_graph",
        "life_coach": "./langstuff_multi_agent/agent.py:life_coach_graph",
        "coder": "./langstuff_multi_agent/agent.py:coder_graph",
        "analyst": "./langstuff_multi_agent/agent.py:analyst_graph",
        "researcher": "./langstuff_multi_agent/agent.py:researcher_graph",
        "general_assistant": "./langstuff_multi_agent/agent.py:general_assistant_graph",
        "news_reporter": "./langstuff_multi_agent/agent.py:news_reporter_graph",
        "customer_support": "./langstuff_multi_agent/agent.py:customer_support_graph",
        "marketing_strategist": "./langstuff_multi_agent/agent.py:marketing_strategist_graph",
        "creative_content": "./langstuff_multi_agent/agent.py:creative_content_graph",
        "financial_analyst": "./langstuff_multi_agent/agent.py:financial_analyst_graph"
    },
    "agents": [
        {
            "name": "SUPERVISOR",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "main",
            "description": "Main supervisor agent that routes requests to specialized agents."
        },
        {
            "name": "DEBUGGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "debugger",
            "description": "Agent responsible for debugging code."
        },
        {
            "name": "CONTEXT_MANAGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "context_manager",
            "description": "Agent responsible for managing conversation context."
        },
        {
            "name": "PROJECT_MANAGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "project_manager",
            "description": "Agent responsible for managing project timelines and tasks."
        },
        {
            "name": "PROFESSIONAL_COACH",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "professional_coach",
            "description": "Agent providing professional and career guidance."
        },
        {
            "name": "LIFE_COACH",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "life_coach",
            "description": "Agent providing lifestyle and personal advice."
        },
        {
            "name": "CODER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "coder",
            "description": "Agent that assists with coding tasks."
        },
        {
            "name": "ANALYST",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "analyst",
            "description": "Agent specializing in data analysis."
        },
        {
            "name": "RESEARCHER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "researcher",
            "description": "Agent that gathers and summarizes research and news."
        },
        {
            "name": "GENERAL_ASSISTANT",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "general_assistant",
            "description": "Agent for general queries and assistance."
        },
        {
            "name": "NEWS_REPORTER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "news_reporter",
            "description": "Agent that searches, researches, explains and summarizes news reports."
        },
        {
            "name": "CUSTOMER_SUPPORT",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "customer_support",
            "description": "Agent that provides customer support and assistance."
        },
        {
            "name": "MARKETING_STRATEGIST",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "marketing_strategist",
            "description": "Agent that provides marketing strategy, insights, trends, and planning."
        },
        {
            "name": "CREATIVE_CONTENT",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "creative_content",
            "description": "Agent that provides creative content, marketing copy, social media posts, or brainstorming ideas."
        },
        {
            "name": "FINANCIAL_ANALYST",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "financial_analyst",
            "description": "Agent that provides financial analysis, market data, forecasting, and investment insights."
        }
    ],
    "dependencies": [
        "langgraph>=0.0.20",
        "langchain-anthropic>=0.0.10",
        "langchain-core>=0.1.20",
        "langchain-openai>=0.0.5",
        "python-dotenv>=1.0.0",
        "tavily-python>=0.5.1",
        "langchain_community>=0.3.17",
        "./langstuff_multi_agent"
    ],
    "configuration": {
        "xai_api_key": "${XAI_API_KEY}",
        "anthropic_api_key": "${ANTHROPIC_API_KEY}",
        "openai_api_key": "${OPENAI_API_KEY}",
        "tavily_api_key": "${TAVILY_API_KEY}",
        "model_settings": {
            "default_provider": "openai",
            "default_model": "gpt-4o-mini",
            "default_temperature": 0.4,
            "default_top_p": 0.9,
            "default_max_tokens": 4000
        },
        "checkpointer": "MemorySaver",
        "logging": {
            "level": "INFO",
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        }
    }
}

================
File: langstuff_multi_agent/__init__.py
================
"""
LangStuff Multi-Agent package.
A multi-agent system built with LangGraph for handling various 
specialized tasks.
"""

__version__ = "0.1.0"

================
File: langstuff_multi_agent/agent.py
================
# langstuff_multi_agent/agent.py
"""
Main agent module that exports the graph for LangGraph Studio.

This module serves as the entry point for LangGraph Studio, exporting only
the primary supervisor workflow. This avoids potential MultipleSubgraphsError
by isolating internal subgraphs.
"""

import logging
from langstuff_multi_agent.agents.supervisor import supervisor_workflow
from langstuff_multi_agent.agents.debugger import debugger_graph
from langstuff_multi_agent.agents.context_manager import context_manager_graph
from langstuff_multi_agent.agents.project_manager import project_manager_graph
from langstuff_multi_agent.agents.professional_coach import professional_coach_graph
from langstuff_multi_agent.agents.life_coach import life_coach_graph
from langstuff_multi_agent.agents.coder import coder_graph
from langstuff_multi_agent.agents.analyst import analyst_graph
from langstuff_multi_agent.agents.researcher import researcher_graph
from langstuff_multi_agent.agents.general_assistant import general_assistant_graph
from langstuff_multi_agent.agents.news_reporter import news_reporter_graph
from langstuff_multi_agent.agents.customer_support import customer_support_graph
from langstuff_multi_agent.agents.marketing_strategist import marketing_strategist_graph
from langstuff_multi_agent.agents.creative_content import creative_content_graph
from langstuff_multi_agent.agents.financial_analyst import financial_analyst_graph
import threading
from langstuff_multi_agent.agents.supervisor import create_supervisor
from langstuff_multi_agent.config import get_llm
from langstuff_multi_agent.config import Config

config = Config()
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Initializing primary supervisor workflow...")


def create_agent_graphs():
    return {
        "debugger": debugger_graph,
        "context_manager": context_manager_graph,
        "project_manager": project_manager_graph,
        "professional_coach": professional_coach_graph,
        "life_coach": life_coach_graph,
        "coder": coder_graph,
        "analyst": analyst_graph,
        "researcher": researcher_graph,
        "general_assistant": general_assistant_graph,
        "news_reporter": news_reporter_graph,
        "customer_support": customer_support_graph,
        "marketing_strategist": marketing_strategist_graph,
        "creative_content": creative_content_graph,
        "financial_analyst": financial_analyst_graph
    }


# Replace manual supervisor setup with official pattern
supervisor_graph = create_supervisor(
    create_agent_graphs(),
    getattr(config, 'configurable', {}),
    supervisor_name="main_supervisor"
)

# Export all graphs required by langgraph.json
__all__ = [
    "supervisor_graph",  # Renamed from supervisor_workflow
    "debugger_graph",
    "context_manager_graph",
    "project_manager_graph",
    "professional_coach_graph",
    "life_coach_graph",
    "coder_graph",
    "analyst_graph",
    "researcher_graph",
    "general_assistant_graph",
    "news_reporter_graph",
    "customer_support_graph",
    "marketing_strategist_graph",
    "creative_content_graph",
    "financial_analyst_graph"
]

# Add explicit graph alias for entry point
graph = supervisor_graph
__all__.insert(0, "graph")  # Add to beginning of exports list

# Add monitoring after graph initialization
available_agents = [  # Define available agents list
    "debugger", "context_manager", "project_manager",
    "professional_coach", "life_coach", "coder",
    "analyst", "researcher", "general_assistant",
    "news_reporter",
    "customer_support",
    "marketing_strategist",
    "creative_content",
    "financial_analyst"
]


def monitor_agents():
    """Prints agent statuses every 10 seconds"""
    import time
    while True:
        print("Active agents:", ", ".join(available_agents))
        time.sleep(10)


# Start monitoring thread
threading.Thread(target=monitor_agents, daemon=True).start()

logger.info("Primary supervisor workflow successfully initialized.")

================
File: langstuff_multi_agent/agents/analyst.py
================
# langstuff_multi_agent/agents/analyst.py
"""
Analyst Agent module for data analysis and interpretation.

This module provides a workflow for analyzing data and performing
calculations using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    calc_tool,
    has_tool_calls,
    news_tool
)
from langstuff_multi_agent.config import get_llm
from langchain_core.messages import ToolMessage
import json
from langchain.schema import SystemMessage, HumanMessage, AIMessage
import logging

analyst_graph = StateGraph(MessagesState)

# Define tools for analysis tasks
tools = [search_web, python_repl, calc_tool, news_tool]
tool_node = ToolNode(tools)

logger = logging.getLogger(__name__)


def analyze_data(state):
    """Analyze data and perform calculations."""
    messages = state.get("messages", [])
    config = state.get("config", {})

    llm = get_llm(config.get("configurable", {}))
    response = llm.invoke(messages)

    return {"messages": messages + [response]}


def process_tool_results(state, config):
    """Processes tool outputs with robust data validation"""
    # Clean previous error messages
    state["messages"] = [msg for msg in state["messages"]
                        if not (isinstance(msg, ToolMessage) and "⚠️" in msg.content)]

    try:
        # Get last tool message with content validation
        last_tool_msg = next(msg for msg in reversed(state["messages"])
                            if isinstance(msg, ToolMessage))

        # Clean and validate raw content
        raw_content = last_tool_msg.content
        clean_content = raw_content.replace('\0', '').replace('\ufeff', '').strip()
        if not clean_content:
            raise ValueError("Empty tool response after cleaning")

        # Hybrid data parsing
        if clean_content[0] in ('{', '['):
            results = json.loads(clean_content, strict=False)
        else:
            results = [{"content": line} for line in clean_content.split("\n") if line.strip()]

        # Validate and process results
        if not isinstance(results, list):
            results = [results]

        valid_results = [
            res for res in results[:5]
            if validate_analysis_result(res)
        ]

        if not valid_results:
            raise ValueError("No valid analysis results")

        # Generate analytical summary
        tool_outputs = []
        for res in valid_results:
            output = f"{res.get('metric', 'Result')}: {res['value']}" if 'value' in res else res['content']
            tool_outputs.append(output[:200])

        llm = get_llm(config.get("configurable", {}))
        summary = llm.invoke([
            SystemMessage(content="Analyze and interpret these results:"),
            HumanMessage(content="\n".join(tool_outputs))
        ])

        return {"messages": [summary]}

    except (json.JSONDecodeError, ValueError) as e:
        logger.error(f"Analysis Error: {str(e)}")
        return {"messages": [AIMessage(
            content=f"Analysis summary:\n{clean_content[:500]}",
            additional_kwargs={"raw_data": True}
        )]}


def validate_analysis_result(result: dict) -> bool:
    """Validate analysis result structure"""
    return isinstance(result, dict) and any(key in result for key in ['content', 'value'])


# Initialize and configure the analyst graph
analyst_graph.add_node("analyze_data", analyze_data)
analyst_graph.add_node("tools", tool_node)
analyst_graph.add_node("process_results", process_tool_results)
analyst_graph.set_entry_point("analyze_data")
analyst_graph.add_edge(START, "analyze_data")

analyst_graph.add_conditional_edges(
    "analyze_data",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else "END"
    ),
    {"tools": "tools", "END": END}
)

analyst_graph.add_edge("tools", "process_results")
analyst_graph.add_edge("process_results", "analyze_data")

analyst_graph = analyst_graph.compile()

__all__ = ["analyst_graph"]

================
File: langstuff_multi_agent/agents/coder.py
================
# langstuff_multi_agent/agents/coder.py
"""
Coder Agent module for writing and improving code.

This module provides a workflow for code generation, debugging,
and optimization using various development tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    read_file,
    write_file,
    calc_tool,
    has_tool_calls
)
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage

coder_graph = StateGraph(MessagesState, ConfigSchema)

# Define tools for coding tasks.
tools = [search_web, python_repl, read_file, write_file, calc_tool]
tool_node = ToolNode(tools)


def code(state, config):
    """Write and improve code with configuration support."""
    # Get config from state and merge with passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)

    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Coder Agent. Your task is to write, debug, "
                            "and improve code.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Find coding examples and docs.\n"
                            "- python_repl: Execute and test Python code.\n"
                            "- read_file: Retrieve code from files.\n"
                            "- write_file: Save code modifications to files.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code or coding request.\n"
                            "2. Provide solutions, test code, and explain your "
                            "reasoning.\n"
                            "3. Use the available tools to execute code and verify "
                            "fixes as necessary."
                        ),
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Processes tool outputs and formats FINAL user response"""
    # Add handoff command detection
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }

    last_message = state["messages"][-1]
    tool_outputs = []

    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })

        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }
    return state


coder_graph.add_node("code", code)
coder_graph.add_node("tools", tool_node)
coder_graph.add_node("process_results", process_tool_results)
coder_graph.set_entry_point("code")
coder_graph.add_edge(START, "code")

coder_graph.add_conditional_edges(
    "code",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else "END"
    ),
    {"tools": "tools", "END": END}
)

coder_graph.add_edge("tools", "process_results")
coder_graph.add_edge("process_results", "code")

coder_graph = coder_graph.compile()

__all__ = ["coder_graph"]

================
File: langstuff_multi_agent/agents/context_manager.py
================
# langstuff_multi_agent/agents/context_manager.py
"""
Context Manager Agent module for tracking conversation context.

This module provides a workflow for managing conversation history
and maintaining context across interactions.
"""
import json
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    read_file,
    write_file,
    has_tool_calls,
    save_memory,
    search_memories
)
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage, AIMessage

# 1. Initialize workflow FIRST
context_manager_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for context management
tools = [search_web, read_file, write_file]
tool_node = ToolNode(tools)


def save_context(state):
    """Saves conversation history to a file"""
    with open("context.json", "w") as f:
        json.dump(state["messages"], f)


def load_context():
    """Loads previous conversation history"""
    try:
        with open("context.json", "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return []


def manage_context(state, config):
    """Manage context using memory system"""
    user_id = config.get("configurable", {}).get("user_id", "global")
    
    # Save conversation history to memory
    save_memory.invoke([{
        "subject": "conversation",
        "predicate": "history",
        "object_": msg.content
    } for msg in state["messages"]], {"configurable": config.get("configurable", {})})
    
    # Retrieve relevant memories
    memories = search_memories.invoke(
        state["messages"][-1].content,
        {"configurable": config.get("configurable", {})}
    )
    
    return {
        "messages": state["messages"] + [AIMessage(
            content=f"Contextual memories: {memories}"
        )]
    }


def process_tool_results(state, config):
    """Process tool outputs and generate final response."""
    # Add handoff command detection
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }

    tool_outputs = []

    for msg in state["messages"]:
        if tool_calls := getattr(msg, "tool_calls", None):
            for tc in tool_calls:
                try:
                    output = f"Tool {tc['name']} result: {tc['output']}"
                    tool_outputs.append({
                        "tool_call_id": tc["id"],
                        "output": output
                    })
                except Exception as e:
                    tool_outputs.append({
                        "tool_call_id": tc["id"],
                        "error": f"Tool execution failed: {str(e)}"
                    })

    return {
        "messages": state["messages"] + [
            {
                "role": "tool",
                "content": to["output"],
                "tool_call_id": to["tool_call_id"]
            } 
            for to in tool_outputs
        ]
    }


# 2. Add nodes BEFORE compiling
context_manager_workflow.add_node("manage_context", manage_context)
context_manager_workflow.add_node("tools", tool_node)
context_manager_workflow.add_node("process_results", process_tool_results)

# 3. Set entry point explicitly
context_manager_workflow.set_entry_point("manage_context")

# 4. Add edges in sequence
context_manager_workflow.add_edge(START, "manage_context")
context_manager_workflow.add_conditional_edges(
    "manage_context",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else END
    ),
    {"tools": "tools", END: END}
)
context_manager_workflow.add_edge("tools", "process_results")
context_manager_workflow.add_edge("process_results", "manage_context")

# 5. Compile ONCE at the end
context_manager_graph = context_manager_workflow.compile()

__all__ = ["context_manager_graph"]

================
File: langstuff_multi_agent/agents/creative_content.py
================
# langstuff_multi_agent/agents/creative_content.py
"""
Creative Content Agent module for generating creative writing, marketing copy, social media posts, or brainstorming ideas.

This module provides a workflow for generating creative content using various tools and a creative prompt.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, calc_tool, has_tool_calls
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import AIMessage, ToolMessage, SystemMessage, HumanMessage

# Create state graph for the Creative Content Agent
creative_content_graph = StateGraph(MessagesState, ConfigSchema)

# Define the tools available for the creative content agent.
# Here we include search_web (to gather inspiration) and calc_tool (if needed).
tools = [search_web, calc_tool]
tool_node = ToolNode(tools)


def creative_content(state, config):
    """Generate creative content based on the user's query with configuration support."""
    # Merge configuration from state and passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)
    # Invoke the LLM with a creative system prompt
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Creative Content Agent. Your task is to generate creative writing, marketing copy, "
                            "social media posts, or brainstorming ideas. Use vivid, imaginative, and engaging language to "
                            "craft content that inspires and captivates.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Use this tool to look up trends or inspiration from online sources.\n"
                            "- calc_tool: Use this for any quick calculations if needed (though it is secondary in this role).\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's creative query.\n"
                            "2. Draw upon your creative instincts (and any tool data if helpful) to generate an inspiring draft.\n"
                            "3. Produce a final piece of creative content that directly addresses the query."
                        ),
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Process tool outputs and integrate them into a final creative content draft."""
    # Check for handoff commands first (if any)
    for msg in state["messages"]:
        if isinstance(msg, AIMessage) and msg.tool_calls:
            for tc in msg.tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }
    # Collect outputs from ToolMessages, if any
    tool_outputs = []
    for msg in state["messages"]:
        if isinstance(msg, ToolMessage):
            try:
                output = f"Tool {msg.tool_call_id} result: {msg.content}"
                tool_outputs.append(output)
            except Exception as e:
                tool_outputs.append(f"Error processing tool result: {str(e)}")
    # If we have tool outputs, use the LLM to synthesize them into a creative draft
    if tool_outputs:
        llm = get_llm(config.get("configurable", {}))
        summary = llm.invoke([
            SystemMessage(content="Synthesize the following inspirations into a creative draft:"),
            HumanMessage(content="\n".join(tool_outputs))
        ])
        return {"messages": [summary]}

    # If no tool outputs were collected, just return the current state
    return state

# Configure the state graph for the creative content agent
creative_content_graph.add_node("creative_content", creative_content)
creative_content_graph.add_node("tools", tool_node)
creative_content_graph.add_node("process_results", process_tool_results)
creative_content_graph.set_entry_point("creative_content")
creative_content_graph.add_edge(START, "creative_content")

creative_content_graph.add_conditional_edges(
    "creative_content",
    lambda state: ("tools" if has_tool_calls(state.get("messages", [])) else "END"),
    {"tools": "tools", "END": END}
)

creative_content_graph.add_edge("tools", "process_results")
creative_content_graph.add_edge("process_results", "creative_content")

creative_content_graph = creative_content_graph.compile()

__all__ = ["creative_content_graph"]

================
File: langstuff_multi_agent/agents/customer_support.py
================
# langstuff_multi_agent/agents/customer_support.py
"""
Customer Support Agent module for handling customer inquiries, troubleshooting, and FAQs.

This module provides a workflow for addressing common customer support issues.
It uses tools to search for support documentation and perform any necessary calculations.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, calc_tool, has_tool_calls
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage

customer_support_graph = StateGraph(MessagesState, ConfigSchema)

# Define tools for the Customer Support Agent
tools = [search_web, calc_tool]
tool_node = ToolNode(tools)


def support(state, config):
    """Conduct customer support interaction with configuration support."""
    # Merge state configuration with passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)
    # Invoke the LLM with a tailored system prompt for customer support
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Customer Support Agent. Your task is to address customer inquiries, provide troubleshooting steps, and answer frequently asked questions.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up support documentation and FAQs.\n"
                            "- calc_tool: Perform calculations if needed.\n\n"
                            "Instructions:\n"
                            "1. Analyze the customer's query.\n"
                            "2. Use tools to retrieve accurate support information.\n"
                            "3. Provide a clear, concise response to help the customer."
                        )
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Processes tool outputs and formats the final customer support response."""
    # Check for handoff commands
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }
    last_message = state["messages"][-1]
    tool_outputs = []
    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })
        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }
    return state


customer_support_graph.add_node("support", support)
customer_support_graph.add_node("tools", tool_node)
customer_support_graph.add_node("process_results", process_tool_results)
customer_support_graph.set_entry_point("support")
customer_support_graph.add_edge(START, "support")

customer_support_graph.add_conditional_edges(
    "support",
    lambda state: ("tools" if has_tool_calls(state.get("messages", [])) else "END"),
    {"tools": "tools", "END": END}
)

customer_support_graph.add_edge("tools", "process_results")
customer_support_graph.add_edge("process_results", "support")

customer_support_graph = customer_support_graph.compile()

__all__ = ["customer_support_graph"]

================
File: langstuff_multi_agent/agents/debugger.py
================
# langstuff_multi_agent/agents/debugger.py
"""
Debugger Agent module for analyzing code and identifying errors.

This module provides a workflow for debugging code using various tools
and LLM-based analysis.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    python_repl,
    read_file,
    write_file,
    has_tool_calls,
    calc_tool
)
from langstuff_multi_agent.config import get_llm
from langchain_core.messages import ToolMessage

debugger_workflow = StateGraph(MessagesState)

# Define the tools available to the Debugger Agent
tools = [search_web, python_repl, read_file, write_file, calc_tool]
tool_node = ToolNode(tools)


def analyze_code(state):
    """Analyze code and identify errors."""
    messages = state.get("messages", [])
    config = state.get("config", {})

    llm = get_llm(config.get("configurable", {}))
    response = llm.invoke(messages)

    return {"messages": messages + [response]}


def process_tool_results(state, config):
    """Processes tool outputs and formats FINAL user response"""
    # Add handoff command detection
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }

    last_message = state["messages"][-1]
    tool_outputs = []

    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })

        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }
    return state


# Initialize and configure the debugger workflow
debugger_workflow.add_node("analyze_code", analyze_code)
debugger_workflow.add_node("tools", tool_node)
debugger_workflow.add_node("process_results", process_tool_results)
debugger_workflow.set_entry_point("analyze_code")
debugger_workflow.add_edge(START, "analyze_code")

debugger_workflow.add_conditional_edges(
    "analyze_code",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

debugger_workflow.add_edge("tools", "process_results")
debugger_workflow.add_edge("process_results", "analyze_code")

debugger_graph = debugger_workflow.compile()

__all__ = ["debugger_graph"]

================
File: langstuff_multi_agent/agents/financial_analyst.py
================
# langstuff_multi_agent/agents/financial_analyst.py
"""
Financial Analyst Agent module for analyzing market data, forecasting trends,
and providing investment insights.

This module provides a workflow for gathering financial news and data,
analyzing stock performance or economic indicators, and synthesizing a concise
summary with actionable investment insights.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    news_tool,
    calc_tool,
    has_tool_calls
)
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage, AIMessage, SystemMessage, HumanMessage
import json
import logging

# Configure logger
logger = logging.getLogger(__name__)

# Create state graph for the financial analyst agent
financial_analyst_graph = StateGraph(MessagesState, ConfigSchema)

# Define the tools available for the financial analyst.
# We assume that search_web and news_tool can be used to retrieve market data and news,
# and calc_tool can be used for any necessary calculations.
tools = [search_web, news_tool, calc_tool]
tool_node = ToolNode(tools)


def financial_analysis(state, config):
    """Conduct financial analysis with configuration support."""
    # Merge configuration from state and the passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)
    # Invoke the LLM with a system prompt tailored for financial analysis
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Financial Analyst Agent. Your task is to analyze current market data, stock "
                            "performance, economic indicators, and forecast trends. You have access to the following tools:\n"
                            "- search_web: Use this tool to look up up-to-date financial news and data.\n"
                            "- news_tool: Retrieve the latest financial headlines and market insights.\n"
                            "- calc_tool: Perform any necessary calculations to support your analysis.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's query about market conditions or investment opportunities.\n"
                            "2. Use the available tools to gather accurate and relevant financial information.\n"
                            "3. Synthesize a clear, concise summary that highlights market trends and actionable insights."
                        )
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Process tool outputs and format the final financial analysis report."""
    # Check for handoff commands (if any)
    for msg in state["messages"]:
        if isinstance(msg, AIMessage) and getattr(msg, "tool_calls", []):
            for tc in msg.tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }
    # Collect tool outputs from ToolMessages
    tool_outputs = []
    for msg in state["messages"]:
        if isinstance(msg, ToolMessage):
            try:
                # Attempt to parse JSON responses if applicable
                content = msg.content.strip()
                if content and content[0] in ('{', '['):
                    data = json.loads(content)
                    # For financial news or data, we assume a list of items with "title" and "source"
                    if isinstance(data, list):
                        for item in data:
                            title = item.get('title', 'No title')
                            source = item.get('source', {}).get('name', 'Unknown')
                            tool_outputs.append(f"{title} ({source})")
                    else:
                        title = data.get('title', 'No title')
                        source = data.get('source', {}).get('name', 'Unknown')
                        tool_outputs.append(f"{title} ({source})")
                else:
                    # Fallback: use raw text split by newline (each line as a data point)
                    for line in content.split("\n"):
                        if line.strip():
                            tool_outputs.append(line.strip())
            except Exception as e:
                tool_outputs.append(f"Error processing financial data: {str(e)}")

    # Generate final summary only if we have outputs
    if tool_outputs:
        llm = get_llm(config.get("configurable", {}))
        summary = llm.invoke([
            SystemMessage(content="Synthesize the following financial data into a concise analysis with key insights:"),
            HumanMessage(content="\n".join(tool_outputs))
        ])
        return {"messages": [summary]}

    # Fallback if no results
    return {
        "messages": [AIMessage(
            content="Could not retrieve current financial data. Please try again later."
        )]
    }


# Configure the state graph for the financial analyst agent
financial_analyst_graph.add_node("financial_analysis", financial_analysis)
financial_analyst_graph.add_node("tools", tool_node)
financial_analyst_graph.add_node("process_results", process_tool_results)
financial_analyst_graph.set_entry_point("financial_analysis")
financial_analyst_graph.add_edge(START, "financial_analysis")

financial_analyst_graph.add_conditional_edges(
    "financial_analysis",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else "END"
    ),
    {"tools": "tools", "END": END}
)

financial_analyst_graph.add_edge("tools", "process_results")
financial_analyst_graph.add_edge("process_results", "financial_analysis")

financial_analyst_graph = financial_analyst_graph.compile()

__all__ = ["financial_analyst_graph"]

================
File: langstuff_multi_agent/agents/general_assistant.py
================
# langstuff_multi_agent/agents/general_assistant.py
"""
General Assistant Agent module for handling diverse queries.

This module provides a workflow for addressing general user requests
using a variety of tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather, has_tool_calls, news_tool
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

general_assistant_graph = StateGraph(MessagesState, ConfigSchema)

# Define general assistant tools
tools = [search_web, get_current_weather, news_tool]
tool_node = ToolNode(tools)


def assist(state, config):
    """Provide general assistance with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a General Assistant Agent. Your task is to assist with a variety of general queries and tasks.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Provide general information and answer questions.\n"
                            "- get_current_weather: Retrieve current weather updates.\n"
                            "- news_tool: Retrieve news headlines and articles.\n\n"
                            "Instructions:\n"
                            "1. Understand the user's request.\n"
                            "2. Use the available tools to gather relevant information when needed.\n"
                            "3. Provide clear, concise, and helpful responses to assist the user."
                        ),
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Processes tool outputs and formats FINAL user response"""
    last_message = state["messages"][-1]
    tool_outputs = []

    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })

        # Create messages with tool outputs
        updated_messages = state["messages"] + [
            {
                "role": "tool",
                "content": to["output"],
                "tool_call_id": to["tool_call_id"]
            } for to in tool_outputs
        ]

        llm = get_llm(config.get("configurable", {}))
        final_response = llm.invoke(updated_messages)

        return {
            "messages": updated_messages + [
                {
                    "role": "assistant",
                    "content": final_response.content
                }
            ]
        }

    return state


general_assistant_graph.add_node("assist", assist)
general_assistant_graph.add_node("tools", tool_node)
general_assistant_graph.add_node("process_results", process_tool_results)
general_assistant_graph.set_entry_point("assist")
general_assistant_graph.add_edge(START, "assist")

general_assistant_graph.add_conditional_edges(
    "assist",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

general_assistant_graph.add_edge("tools", "process_results")
general_assistant_graph.add_edge("process_results", "assist")

general_assistant_graph = general_assistant_graph.compile()

__all__ = ["general_assistant_graph"]

================
File: langstuff_multi_agent/agents/life_coach.py
================
# langstuff_multi_agent/agents/life_coach.py
"""
Life Coach Agent module for personal advice and guidance.

This module provides a workflow for offering lifestyle tips and
personal development advice using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    get_current_weather,
    calendar_tool,
    has_tool_calls,
    save_memory,
    search_memories
)
from langstuff_multi_agent.config import get_llm
from langchain_core.messages import ToolMessage

life_coach_graph = StateGraph(MessagesState)

# Define tools for life coaching
tools = [search_web, get_current_weather, calendar_tool, save_memory, search_memories]
tool_node = ToolNode(tools)


def life_coach(state):
    """Provide life coaching and personal advice."""
    messages = state.get("messages", [])
    config = state.get("config", {})

    llm = get_llm(config.get("configurable", {}))
    response = llm.invoke(messages)

    # Add lifestyle preferences from memory
    if 'user_id' in state.get("configurable", {}):
        preferences = search_memories.invoke(
            "lifestyle preferences", 
            {"configurable": state["configurable"]}
        )
        if preferences:
            messages.append({
                "role": "system",
                "content": f"User preferences: {preferences}"
            })

    return {"messages": messages + [response]}


def process_tool_results(state, config):
    """Processes tool outputs and formats FINAL user response"""
    # Add handoff command detection
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }

    last_message = state["messages"][-1]
    tool_outputs = []

    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })

        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }
    return state


# Initialize and configure the life coach graph
life_coach_graph.add_node("life_coach", life_coach)
life_coach_graph.add_node("tools", tool_node)
life_coach_graph.add_node("process_results", process_tool_results)
life_coach_graph.set_entry_point("life_coach")
life_coach_graph.add_edge(START, "life_coach")

life_coach_graph.add_conditional_edges(
    "life_coach",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else "END"
    ),
    {"tools": "tools", "END": END}
)

life_coach_graph.add_edge("tools", "process_results")
life_coach_graph.add_edge("process_results", "life_coach")

life_coach_graph = life_coach_graph.compile()

__all__ = ["life_coach_graph"]

================
File: langstuff_multi_agent/agents/marketing_strategist.py
================
# langstuff_multi_agent/agents/marketing_strategist.py
"""
Marketing Strategist Agent module for analyzing trends, planning campaigns, and providing social media strategy insights.

This module provides a workflow for gathering market data, identifying trends, and delivering actionable marketing strategies.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, news_tool, calc_tool, has_tool_calls
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage

marketing_strategist_graph = StateGraph(MessagesState, ConfigSchema)

# Define tools for the Marketing Strategist Agent
tools = [search_web, news_tool, calc_tool]
tool_node = ToolNode(tools)


def marketing(state, config):
    """Conduct marketing strategy analysis with configuration support."""
    # Merge configuration from state and passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)
    # Invoke the LLM with a tailored system prompt for marketing strategy
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Marketing Strategist Agent. Your task is to analyze current trends, plan marketing campaigns, and provide social media strategy insights.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Gather market and trend information.\n"
                            "- news_tool: Retrieve the latest news and social media trends.\n"
                            "- calc_tool: Perform quantitative analysis if needed.\n\n"
                            "Instructions:\n"
                            "1. Analyze the customer's marketing query.\n"
                            "2. Use tools to gather accurate market data and trend information.\n"
                            "3. Provide detailed, actionable marketing strategies and social media insights."
                        )
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Processes tool outputs and formats the final marketing strategy response."""
    # Check for handoff commands
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }
    last_message = state["messages"][-1]
    tool_outputs = []
    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })
        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }
    return state


marketing_strategist_graph.add_node("marketing", marketing)
marketing_strategist_graph.add_node("tools", tool_node)
marketing_strategist_graph.add_node("process_results", process_tool_results)
marketing_strategist_graph.set_entry_point("marketing")
marketing_strategist_graph.add_edge(START, "marketing")

marketing_strategist_graph.add_conditional_edges(
    "marketing",
    lambda state: ("tools" if has_tool_calls(state.get("messages", [])) else "END"),
    {"tools": "tools", "END": END}
)

marketing_strategist_graph.add_edge("tools", "process_results")
marketing_strategist_graph.add_edge("process_results", "marketing")

marketing_strategist_graph = marketing_strategist_graph.compile()

__all__ = ["marketing_strategist_graph"]

================
File: langstuff_multi_agent/agents/news_reporter.py
================
# langstuff_multi_agent/agents/news_reporter.py
"""
News Reporter Agent module for gathering and summarizing news reports.

This module provides a workflow for gathering and reporting the latest news using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    news_tool,
    calc_tool,
    has_tool_calls,
    save_memory,
    search_memories
)
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage, AIMessage, SystemMessage, HumanMessage
import json
import logging

# Create state graph for the news reporter agent
news_reporter_graph = StateGraph(MessagesState, ConfigSchema)

# Define the tools available for the news reporter
tools = [search_web, news_tool, calc_tool, save_memory, search_memories]
tool_node = ToolNode(tools)

# Configure logger
logger = logging.getLogger(__name__)

def final_response(state, config):
    """Directly return last ToolMessage for immediate responses"""
    for msg in reversed(state["messages"]):
        if isinstance(msg, ToolMessage):
            return {"messages": [msg]}
    return {"messages": state["messages"]}

def news_should_continue(state):
    """Enhanced conditional routing with direct return check"""
    messages = state.get("messages", [])
    if not messages:
        return "END"
        
    last_message = messages[-1]
    if not getattr(last_message, "tool_calls", []):
        return "END"

    # Check first tool call for return_direct flag
    args = last_message.tool_calls[0].get("args", {})
    return "final" if args.get("return_direct", False) else "tools"

def news_report(state, config):
    """Conduct news reporting with configuration support."""
    # Merge the configuration from the state and the passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)
    # Invoke the LLM with a system prompt tailored for a news reporter agent
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a News Reporter Agent. Your task is to gather and report "
                            "the latest news, headlines, and summaries from reliable sources.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up recent info and data.\n"
                            "- news_tool: Retrieve the latest news articles and headlines.\n"
                            "- calc_tool: Perform calculations if necessary.\n"
                            "- save_memory: Save information for future reference.\n"
                            "- search_memories: Retrieve saved information.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's news query.\n"
                            "2. Use the available tools to gather accurate and up-to-date news.\n"
                            "3. Provide a clear and concise summary of your findings."
                        ),
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Process tool outputs with hybrid JSON/text parsing"""
    # Clean previous error messages
    state["messages"] = [msg for msg in state["messages"]
                        if not (isinstance(msg, ToolMessage) and "⚠️" in msg.content)]

    try:
        # Get last tool message with content validation
        last_tool_msg = next(msg for msg in reversed(state["messages"]) 
                            if isinstance(msg, ToolMessage))
        
        # Null byte removal and encoding cleanup
        raw_content = last_tool_msg.content
        if not isinstance(raw_content, str):
            raise ValueError("Non-string tool response")
            
        clean_content = raw_content.replace('\0', '').replace('\ufeff', '').strip()
        if not clean_content:
            raise ValueError("Empty content after cleaning")

        # Attempt JSON parsing first
        if clean_content[0] in ('{', '['):
            articles = json.loads(clean_content, strict=False)
        else:
            # NEW: Handle non-JSON responses using text parsing
            articles = [
                {"title": line.split(" (")[0], "source": line.split("(")[-1].rstrip(")")}
                for line in clean_content.split("\n") if " (" in line and ")" in line
            ]
            logger.info(f"Converted {len(articles)} text entries to structured format")

        # Convert single article to list
        if not isinstance(articles, list):
            articles = [articles]

        # Process articles with validation
        valid_articles = [
            art for art in articles[:5]
            if validate_article(art)
        ]
        
        if not valid_articles:
            raise ValueError("No valid articles after filtering")

        # Add memory context to articles
        if 'user_id' in config.get("configurable", {}):
            memories = search_memories.invoke(
                "news preferences", 
                {"configurable": config["configurable"]}
            )
            if memories:
                state["messages"].append(AIMessage(
                    content=f"User preferences context: {memories}"
                ))

        # Generate summary
        tool_outputs = []
        for art in valid_articles:
            title = art.get('title', 'Untitled')[:100]
            # Handle both string and dict source formats
            source = (
                art['source'].get('name', 'Unknown') 
                if isinstance(art.get('source'), dict)
                else str(art.get('source', 'Unknown'))
            )[:50]
            tool_outputs.append(f"{title} ({source})")

        llm = get_llm(config.get("configurable", {}))
        summary = llm.invoke([
            SystemMessage(content="Create concise bullet points from these articles:"),
            HumanMessage(content="\n".join(tool_outputs))
        ])
        
        return {"messages": [summary]}

    except json.JSONDecodeError as e:
        logger.error(f"JSON Error: {e}\nFirst 200 chars: {clean_content[:200]}")
        # NEW: Attempt text fallback
        if "\n" in clean_content:
            return handle_text_fallback(clean_content, config)
        return {"messages": [AIMessage(
            content=f"⚠️ News format error: {str(e)[:100]}",
            additional_kwargs={"error": True, "raw_content": clean_content[:200]}
        )]}
        
    except ValueError as e:
        logger.error(f"Validation Error: {str(e)}")
        return {"messages": [AIMessage(
            content=f"⚠️ Invalid news data: {str(e)[:100]}",
            additional_kwargs={"error": True}
        )]}

def handle_text_fallback(content: str, config: dict) -> dict:
    """Process text-based news format with source validation"""
    articles = []
    for line in content.split("\n"):
        if " (" in line and line.endswith(")"):
            title, source = line.rsplit(" (", 1)
            articles.append({
                "title": title.strip(),
                "source": source.rstrip(")").strip()  # Store source as string
            })
    
    # Validate at least 1 article has both fields
    if not any(validate_article(art) for art in articles):
        raise ValueError("No valid articles in text fallback")
    
    # Generate summary from parsed text
    tool_outputs = [f"{art['title']} ({art['source']})" for art in articles[:5]]
    llm = get_llm(config.get("configurable", {}))
    summary = llm.invoke([
        SystemMessage(content="Create concise bullet points from these articles:"),
        HumanMessage(content="\n".join(tool_outputs))
    ])
    return {"messages": [summary]}

def validate_article(article: dict) -> bool:
    """Strict validation for news article structure"""
    return all(
        key in article and isinstance(article[key], str)
        for key in ['title', 'source']
    ) and len(article.get('title', '')) >= 10


# Configure the state graph for the news reporter agent
news_reporter_graph.add_node("news_report", news_report)
news_reporter_graph.add_node("tools", tool_node)
news_reporter_graph.add_node("process_results", process_tool_results)
news_reporter_graph.add_node("final", final_response)
news_reporter_graph.set_entry_point("news_report")
news_reporter_graph.add_edge(START, "news_report")

news_reporter_graph.add_conditional_edges(
    "news_report",
    news_should_continue,
    {"tools": "tools", "final": "final", "END": END}
)

news_reporter_graph.add_edge("final", END)
news_reporter_graph.add_edge("tools", "process_results")
news_reporter_graph.add_edge("process_results", "news_report")

news_reporter_graph = news_reporter_graph.compile()

__all__ = ["news_reporter_graph"]

================
File: langstuff_multi_agent/agents/professional_coach.py
================
# langstuff_multi_agent/agents/professional_coach.py
"""
Professional Coach Agent module for career guidance.

This module provides a workflow for offering career advice and
job search strategies using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    job_search_tool,
    has_tool_calls,
    get_current_weather,
    calendar_tool,
    save_memory,
    search_memories
)
from langstuff_multi_agent.config import get_llm
from langchain_core.messages import ToolMessage

professional_coach_graph = StateGraph(MessagesState)

# Define the tools for professional coaching
tools = [search_web, job_search_tool, get_current_weather, calendar_tool, save_memory, search_memories]
tool_node = ToolNode(tools)


def coach(state):
    """Provide professional coaching and career advice."""
    messages = state.get("messages", [])
    config = state.get("config", {})

    llm = get_llm(config.get("configurable", {}))
    response = llm.invoke(messages)

    # Add memory context
    if 'user_id' in state.get("configurable", {}):
        career_goals = search_memories.invoke(
            "career goals", 
            {"configurable": state["configurable"]}
        )
        if career_goals:
            messages.append({
                "role": "system",
                "content": f"User career history: {career_goals}"
            })

    return {"messages": messages + [response]}


def process_tool_results(state, config):
    """Processes tool outputs and formats FINAL user response"""
    # Add handoff command detection
    for msg in state["messages"]:
        if tool_calls := getattr(msg, 'tool_calls', None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {
                        "messages": [ToolMessage(
                            goto=tc['name'].replace('transfer_to_', ''),
                            graph=ToolMessage.PARENT
                        )]
                    }

    last_message = state["messages"][-1]
    tool_outputs = []

    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })

        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }
    return state


# Initialize and configure the professional coach graph
professional_coach_graph.add_node("coach", coach)
professional_coach_graph.add_node("tools", tool_node)
professional_coach_graph.add_node("process_results", process_tool_results)
professional_coach_graph.set_entry_point("coach")
professional_coach_graph.add_edge(START, "coach")

professional_coach_graph.add_conditional_edges(
    "coach",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else "END"
    ),
    {"tools": "tools", "END": END}
)

professional_coach_graph.add_edge("tools", "process_results")
professional_coach_graph.add_edge("process_results", "coach")

professional_coach_graph = professional_coach_graph.compile()

__all__ = ["professional_coach_graph"]

================
File: langstuff_multi_agent/agents/project_manager.py
================
# langstuff_multi_agent/agents/project_manager.py
"""
Project Manager Agent module for task and timeline management.

This module provides a workflow for overseeing project schedules
and coordinating tasks using various tools.
"""

from langgraph.graph import END, START, StateGraph
from typing import TypedDict, Dict, Any

from langstuff_multi_agent.utils.tools import get_tool_node, search_web, python_repl
from langstuff_multi_agent.config import get_llm
from langstuff_multi_agent.utils.tools import has_tool_calls


def manage(state):
    """Project management agent that coordinates tasks and timelines."""
    messages = state.get("messages", [])
    config = state.get("configurable", {})

    llm = get_llm(config)
    response = llm.invoke(messages)

    return {"messages": messages + [response]}


def process_tool_results(state, config):  # Add config parameter
    """Processes tool outputs and formats FINAL user response"""
    last_message = state["messages"][-1]
    tool_outputs = []

    if tool_calls := getattr(last_message, 'tool_calls', None):
        for tc in tool_calls:
            try:
                output = f"Tool {tc['name']} result: {tc['output']}"
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "output": output
                })
            except Exception as e:
                tool_outputs.append({
                    "tool_call_id": tc["id"],
                    "error": f"Tool execution failed: {str(e)}"
                })

        # Use configurable LLM
        return {
            "messages": state["messages"] + [
                {
                    "role": "tool",
                    "content": to["output"],
                    "tool_call_id": to["tool_call_id"]
                } for to in tool_outputs
            ]
        }

    return state


# Define state schema properly
class ProjectState(TypedDict):
    tasks: Dict[str, Any]
    current_step: str
    artifacts: Dict[str, Any]


def planning_node(state: ProjectState) -> dict:
    """Generates initial project plan"""
    return {"tasks": ["research", "prototype"], "current_step": "planning"}


def execution_node(state: ProjectState) -> dict:
    """Executes planned tasks"""
    return {"current_step": "executing", "artifacts": {"result": "prototype_v1"}}


# Correct initialization pattern from @Web examples
project_manager_graph = StateGraph(ProjectState)  # Pass state schema class

# Add ALL required nodes first
project_manager_graph.add_node("planning", planning_node)
project_manager_graph.add_node("execution", execution_node)
project_manager_graph.add_node("manage", manage)
project_manager_graph.add_node("tools", get_tool_node([search_web, python_repl]))
project_manager_graph.add_node("process_results", process_tool_results)

# Then define edges
project_manager_graph.add_edge("planning", "execution")
project_manager_graph.add_edge("execution", "manage")

# Conditional edges must point to REGISTERED nodes
project_manager_graph.add_conditional_edges(
    "manage",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else END,
    {"tools": "tools", "END": END}
)

project_manager_graph.add_edge("tools", "process_results")
project_manager_graph.add_edge("process_results", "manage")

# Set entry point AFTER all nodes exist
project_manager_graph.set_entry_point("planning")

project_manager_graph = project_manager_graph.compile()


__all__ = ["project_manager_graph"]

================
File: langstuff_multi_agent/agents/researcher.py
================
# langstuff_multi_agent/agents/researcher.py
"""
Researcher Agent module for gathering and summarizing information.

This module provides a workflow for gathering and summarizing news and research 
information using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import (
    search_web,
    news_tool,
    calc_tool,
    has_tool_calls,
    news_tool,
    save_memory,
    search_memories
)
from langstuff_multi_agent.config import ConfigSchema, get_llm
from langchain_core.messages import ToolMessage
import json
from langchain.schema import SystemMessage, HumanMessage, AIMessage

researcher_graph = StateGraph(MessagesState, ConfigSchema)

# Define research tools
tools = [search_web, news_tool, calc_tool, news_tool, save_memory, search_memories]
tool_node = ToolNode(tools)


def research(state, config):
    """Conduct research with configuration support."""
    # Get config from state and merge with passed config
    state_config = state.get("configurable", {})
    if config:
        state_config.update(config.get("configurable", {}))
    llm = get_llm(state_config)
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Researcher Agent. Your task is to gather "
                            "and summarize news and research information.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up recent info and data.\n"
                            "- news_tool: Get latest news and articles.\n"
                            "- calc_tool: Perform calculations.\n"
                            "- save_memory: Save information to memory.\n"
                            "- search_memories: Search for information in memory.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's research query.\n"
                            "2. Use tools to gather accurate and relevant info.\n"
                            "3. Provide a clear summary of your findings."
                        ),
                    }
                ]
            )
        ]
    }


def process_tool_results(state, config):
    """Processes tool outputs with enhanced error handling"""
    # Clean previous error messages
    state["messages"] = [msg for msg in state["messages"]
                        if not (isinstance(msg, ToolMessage) and "⚠️" in msg.content)]

    try:
        # Get last tool message with content validation
        last_tool_msg = next(msg for msg in reversed(state["messages"]) 
                            if isinstance(msg, ToolMessage))
        
        # Null byte removal and encoding cleanup
        raw_content = last_tool_msg.content
        if not isinstance(raw_content, str):
            raise ValueError("Non-string tool response")
            
        clean_content = raw_content.replace('\0', '').replace('\ufeff', '').strip()
        if not clean_content:
            raise ValueError("Empty content after cleaning")

        # Hybrid JSON/text parsing
        if clean_content[0] in ('{', '['):
            results = json.loads(clean_content, strict=False)
        else:
            results = [{"content": line} for line in clean_content.split("\n") if line.strip()]
        
        # Validate results structure
        if not isinstance(results, list):
            results = [results]
            
        valid_results = [
            res for res in results[:5]
            if isinstance(res, dict) and res.get("content")
        ]
        
        if not valid_results:
            raise ValueError("No valid research results")

        # Generate summary
        tool_outputs = [f"{res.get('title', 'Result')}: {res['content'][:200]}" for res in valid_results]
        llm = get_llm(config.get("configurable", {}))
        summary = llm.invoke([
            SystemMessage(content="Synthesize these research findings:"),
            HumanMessage(content="\n".join(tool_outputs))
        ])
        
        # Add memory handling
        tool_calls = [msg for msg in state["messages"] if isinstance(msg, ToolMessage)]
        memory_operations = [tc for tc in tool_calls if tc['name'] in ('save_memory', 'search_memories')]
        if memory_operations:
            return handle_memory_operations(state, memory_operations, config)
        
        return {"messages": [summary]}

    except (json.JSONDecodeError, ValueError) as e:
        # Fallback to raw content display
        return {"messages": [AIMessage(
            content=f"Research summary:\n{clean_content[:500]}",
            additional_kwargs={"raw_data": True}
        )]}


def validate_result(result: dict) -> bool:
    """Ensure research result has minimum required fields"""
    return isinstance(result, dict) and "content" in result


# Add new memory handling function
def handle_memory_operations(state, tool_calls, config):
    outputs = []
    for tc in tool_calls:
        try:
            if tc['name'] == 'save_memory':
                result = save_memory.invoke(
                    tc['args'], 
                    {"configurable": config.get("configurable", {})}
                )
            elif tc['name'] == 'search_memories':
                result = search_memories.invoke(
                    tc['args'], 
                    {"configurable": config.get("configurable", {})}
                )
            outputs.append({
                "tool_call_id": tc["id"],
                "output": result
            })
        except Exception as e:
            outputs.append({
                "tool_call_id": tc["id"],
                "error": str(e)
            })
    
    return {"messages": [ToolMessage(
        content=json.dumps([o["output"] for o in outputs]),
        tool_call_id=[o["tool_call_id"] for o in outputs]
    )]}


researcher_graph.add_node("research", research)
researcher_graph.add_node("tools", tool_node)
researcher_graph.add_node("process_results", process_tool_results)
researcher_graph.set_entry_point("research")
researcher_graph.add_edge(START, "research")

researcher_graph.add_conditional_edges(
    "research",
    lambda state: (
        "tools" if has_tool_calls(state.get("messages", [])) else "END"
    ),
    {"tools": "tools", "END": END}
)

researcher_graph.add_edge("tools", "process_results")
researcher_graph.add_edge("process_results", "research")

researcher_graph = researcher_graph.compile()

__all__ = ["researcher_graph"]

================
File: langstuff_multi_agent/agents/supervisor.py
================
# langstuff_multi_agent/agents/supervisor.py
"""
Supervisor Agent module for integrating and routing individual LangGraph agent workflows.
"""

from langgraph.graph import StateGraph
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langstuff_multi_agent.config import get_llm
from typing import Literal, Optional
from pydantic import BaseModel, Field
import re
import uuid
from langchain_community.tools import tool
from langchain_core.messages import ToolCall
from langchain_core.tools import BaseTool
from typing_extensions import Annotated
from langchain_core.tools import InjectedToolCallId
from langstuff_multi_agent.utils.tools import search_memories

# Import individual workflows.
from langstuff_multi_agent.agents.debugger import debugger_graph
from langstuff_multi_agent.agents.context_manager import context_manager_graph
from langstuff_multi_agent.agents.project_manager import project_manager_graph
from langstuff_multi_agent.agents.professional_coach import professional_coach_graph
from langstuff_multi_agent.agents.life_coach import life_coach_graph
from langstuff_multi_agent.agents.coder import coder_graph
from langstuff_multi_agent.agents.analyst import analyst_graph
from langstuff_multi_agent.agents.researcher import researcher_graph
from langstuff_multi_agent.agents.general_assistant import general_assistant_graph
from langstuff_multi_agent.agents.news_reporter import news_reporter_graph
from langstuff_multi_agent.agents.customer_support import customer_support_graph
from langstuff_multi_agent.agents.marketing_strategist import marketing_strategist_graph
from langstuff_multi_agent.agents.creative_content import creative_content_graph
from langstuff_multi_agent.agents.financial_analyst import financial_analyst_graph
import logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Define valid agent destinations at the top of the file
AVAILABLE_AGENTS = [
    'debugger',
    'context_manager',
    'project_manager',
    'professional_coach',
    'life_coach',
    'coder',
    'analyst',
    'researcher',
    'general_assistant',
    'news_reporter',
    'customer_support',
    'marketing_strategist',
    'creative_content',
    'financial_analyst'
]


def log_agent_failure(agent_name, query):
    """Logs agent failures for better debugging"""
    logger.error(f"Agent '{agent_name}' failed to process query: {query}")


# ======================
# Handoff Implementation
# ======================
WHITESPACE_RE = re.compile(r"\s+")


def _normalize_agent_name(agent_name: str) -> str:
    return WHITESPACE_RE.sub("_", agent_name.strip()).lower()


def create_handoff_tool(*, agent_name: str) -> BaseTool:
    tool_name = f"transfer_to_{_normalize_agent_name(agent_name)}"

    @tool(tool_name)
    def handoff_to_agent(
        tool_call_id: Annotated[str, InjectedToolCallId],
    ):
        tool_message = ToolMessage(
            content=f"Successfully transferred to {agent_name}",
            name=tool_name,
            tool_call_id=tool_call_id,
        )
        return ToolMessage(
            goto=agent_name,
            graph=ToolMessage.PARENT,
            update={"messages": [tool_message]},
        )
    return handoff_to_agent


def create_handoff_back_messages(agent_name: str, supervisor_name: str):
    tool_call_id = str(uuid.uuid4())
    tool_name = f"transfer_back_to_{_normalize_agent_name(supervisor_name)}"
    tool_calls = [ToolCall(name=tool_name, args={}, id=tool_call_id)]
    return (
        AIMessage(
            content=f"Transferring back to {supervisor_name}",
            tool_calls=tool_calls,
            name=agent_name,
        ),
        ToolMessage(
            content=f"Successfully transferred back to {supervisor_name}",
            name=tool_name,
            tool_call_id=tool_call_id,
        ),
    )


# ======================
# Core Supervisor Logic
# ======================
class RouterInput(BaseModel):
    messages: list[HumanMessage] = Field(..., description="User messages to route")
    last_route: Optional[str] = Field(None, description="Previous routing destination")


class RouteDecision(BaseModel):
    """Routing decision with chain-of-thought reasoning"""
    reasoning: str = Field(..., description="Step-by-step routing logic")
    destination: Literal[tuple(AVAILABLE_AGENTS)] = Field(..., description="Target agent")


class RouterState(RouterInput):
    """Combined state for routing workflow"""
    reasoning: Optional[str] = Field(None, description="Routing decision rationale")
    destination: Optional[str] = Field(None, description="Selected agent target")


def route_query(state: RouterState):
    """Classifies and routes user queries using structured LLM output."""
    # Get config from state and add structured output method
    config = getattr(state, "configurable", {})
    config["structured_output_method"] = "json_mode"
    llm = get_llm(config)
    system = """You are an expert router for a multi-agent system. Analyze the user's query 
    and route to ONE specialized agent. Consider these specialties:
    - Debugger: Code errors solutions, troubleshooting
    - Coder: Writing/explaining code
    - Analyst: Data analysis requests
    - Researcher: Fact-finding, web research, news research
    - Project Manager: Task planning
    - Life Coach: Personal life strategies and advice
    - Professional Coach: Professional career strategies and advice
    - General Assistant: General purpose assistant for generic requests
    - News Reporter: News searching, reporting and summaries
    - Customer Support: Customer support queries
    - Marketing Strategist: Marketing strategy, insights, trends, and planning
    - Creative Content: Creative writing, marketing copy, social media posts, or brainstorming ideas
    - Financial Analyst: Financial analysis, market data, forecasting, and investment insights"""

    structured_llm = llm.with_structured_output(RouteDecision)

    decision = structured_llm.invoke([{
        "role": "user",
        "content": f"Route this query: {state.messages[-1].content}"
    }], config={"system": system})

    # Use the defined constant for validation
    if decision.destination not in AVAILABLE_AGENTS:
        log_agent_failure(decision.destination, state.messages[-1].content)
        return RouterState(
            messages=state.messages,
            reasoning="Fallback due to failure",
            destination="general_assistant"
        )
    else:
        return RouterState(
            messages=state.messages,
            reasoning=decision.reasoning,
            destination=decision.destination
        )


def process_tool_results(state, config):
    """Updated to preserve final assistant messages"""
    tool_outputs = []
    final_messages = []

    for msg in state["messages"]:
        if isinstance(msg, AIMessage) and not msg.tool_calls:
            final_messages.append(msg)  # Capture final assistant response
        if tool_calls := getattr(msg, "tool_calls", None):
            for tc in tool_calls:
                if tc['name'].startswith('transfer_to_'):
                    return {"messages": [ToolMessage(
                        goto=tc['name'].replace('transfer_to_', ''),
                        graph=ToolMessage.PARENT
                    )]}
                # Existing tool processing logic
                try:
                    output = f"Tool {tc['name']} result: {tc['output']}"
                    tool_outputs.append({
                        "tool_call_id": tc["id"],
                        "output": output
                    })
                except Exception as e:
                    tool_outputs.append({
                        "tool_call_id": tc["id"],
                        "error": str(e)
                    })

    return {
        "messages": [
            *final_messages,  # Preserve final responses
            *[ToolMessage(content=to["output"], tool_call_id=to["tool_call_id"]) 
              for to in tool_outputs]
        ]
    }


def should_continue(state: dict) -> bool:
    """
    Determine if the workflow should continue processing.

    Returns True if there are pending tool calls or no final assistant message.
    """
    messages = state.get("messages", [])
    if not messages:
        return True

    last_message = messages[-1]
    # Continue if not an AI message or has tool calls
    return not isinstance(last_message, AIMessage) or bool(getattr(last_message, "tool_calls", None))


def end_state(state: RouterState):
    """Terminal node that returns the final state."""
    return state


# ======================
# Workflow Construction
# ======================
def create_supervisor(agent_graphs=None, configurable=None, supervisor_name=None):
    """Create supervisor workflow with enhanced configurability"""
    builder = StateGraph(RouterState)

    # Add new memory loading node
    builder.add_node("load_memories", lambda state: {
        "memories": search_memories.invoke(
            state["messages"][-1].content,
            {"configurable": state.get("configurable", {})}
        )
    })
    
    # Modify existing route_query to use memories
    def route_query(state):
        memories = state.get("memories", [])
        # Add memories to context
        return {"messages": state["messages"] + memories}
    
    builder.add_node("route_query", route_query)
    builder.add_node("debugger", debugger_graph)
    builder.add_node("context_manager", context_manager_graph)
    builder.add_node("project_manager", project_manager_graph)
    builder.add_node("professional_coach", professional_coach_graph)
    builder.add_node("life_coach", life_coach_graph)
    builder.add_node("coder", coder_graph)
    builder.add_node("analyst", analyst_graph)
    builder.add_node("researcher", researcher_graph)
    builder.add_node("general_assistant", general_assistant_graph)
    builder.add_node("news_reporter", news_reporter_graph)
    builder.add_node("customer_support", customer_support_graph)
    builder.add_node("marketing_strategist", marketing_strategist_graph)
    builder.add_node("creative_content", creative_content_graph)
    builder.add_node("financial_analyst", financial_analyst_graph)
    builder.add_node("process_results", process_tool_results)
    builder.add_node("end", end_state)  # Add terminal node

    # Conditional edges
    builder.add_conditional_edges(
        "route_query",
        lambda s: s.destination if s.destination in AVAILABLE_AGENTS else "general_assistant",
        {agent: agent for agent in AVAILABLE_AGENTS}
    )

    # Add conditional edge from process_results to either end or route_query
    builder.add_conditional_edges(
        "process_results",
        lambda state: "route_query" if should_continue(state) else "end",
        {"route_query": "route_query", "end": "end"}
    )

    builder.add_edge("load_memories", "route_query")
    builder.set_entry_point("route_query")
    return builder.compile()


supervisor_workflow = create_supervisor()

__all__ = ["create_supervisor", "supervisor_workflow"]

================
File: langstuff_multi_agent/config.py
================
# config.py
"""
Configuration settings for the LangGraph multi-agent AI project.

This file reads critical configuration values from environment variables,
provides default settings, initializes logging, sets up a persistent checkpoint
instance using MemorySaver, and exposes configuration and factory functions for LangGraph.

Supported providers:
  - "anthropic": Uses ChatAnthropic with the key from ANTHROPIC_API_KEY.
  - "openai": Uses ChatOpenAI with the key from OPENAI_API_KEY.
  - "grok" (or "xai"): Uses ChatOpenAI as an interface to Grok with the key from XAI_API_KEY.

Note: The LLM instances returned by get_llm() support structured output via the 
.with_structured_output() method. This is essential for our supervisor routing
logic and agent structured responses.
"""

import os
import logging
import json
from functools import lru_cache, wraps
from langgraph.checkpoint.memory import MemorySaver
from typing import Optional, Dict, Any, Literal, Callable
from typing_extensions import TypedDict
from langchain_core.messages import SystemMessage
from langchain_core.runnables.config import RunnableConfig
from pydantic import BaseModel, ValidationError

# Import provider libraries.
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel


class ConfigSchema(TypedDict):
    """Enhanced configuration schema for assistant nodes"""
    model: Optional[str]
    system_message: Optional[str]
    temperature: Optional[float]
    top_p: Optional[float]
    max_tokens: Optional[int]
    provider: Literal['openai', 'anthropic', 'grok']  # Required provider field


class Config:
    # API Keys
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    XAI_API_KEY = os.environ.get("XAI_API_KEY")

    # Default model settings
    DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "gpt-4o-mini")
    DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", 0.4))
    DEFAULT_PROVIDER = os.environ.get("AI_PROVIDER", "openai").lower()

    # Cache settings
    LLM_CACHE_SIZE = int(os.environ.get("LLM_CACHE_SIZE", "8"))

    # Track API key states for cache invalidation
    _api_key_states = {
        "anthropic": ANTHROPIC_API_KEY,
        "openai": OPENAI_API_KEY,
        "grok": XAI_API_KEY
    }

    @classmethod
    def api_keys_changed(cls) -> bool:
        """Check if any API keys have changed since last check."""
        current_states = {
            "anthropic": cls.ANTHROPIC_API_KEY,
            "openai": cls.OPENAI_API_KEY,
            "grok": cls.XAI_API_KEY
        }
        changed = current_states != cls._api_key_states
        cls._api_key_states = current_states.copy()
        return changed

    # Model configurations
    MODEL_CONFIGS = {
        "anthropic": {
            "model_name": "claude-3-5-sonnet-20240620",
            "temperature": 0.0,
            "top_p": 0.9,
            "max_tokens": 4000,
        },
        "openai": {
            "model_name": "gpt-4o-mini",  # Preferred openai model
            "temperature": 0.4,
            "top_p": 0.9,
            "max_tokens": 4000,
        },
        "grok": {
            "model_name": "grok-2-1212",  # Fallback to latest Grok model - this model name is accurate
            "temperature": 0.4,
            "top_p": 0.9,
            "max_tokens": 4000,
        }
    }

    # Logging configuration
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Persistent checkpointer instance
    PERSISTENT_CHECKPOINTER = MemorySaver()

    @classmethod
    def init_logging(cls):
        """Initialize logging with configured settings."""
        logging.basicConfig(level=cls.LOG_LEVEL, format=cls.LOG_FORMAT)
        logging.info("Logging initialized at level: %s", cls.LOG_LEVEL)

    @classmethod
    def get_api_key(cls, provider: str) -> str:
        """Get the API key for the specified provider."""
        key_map = {
            "openai": ("OPENAI_API_KEY", cls.OPENAI_API_KEY),
            "anthropic": ("ANTHROPIC_API_KEY", cls.ANTHROPIC_API_KEY),
            "grok": ("XAI_API_KEY", cls.XAI_API_KEY),
        }

        env_var, key = key_map.get(provider, (None, None))
        if not key:
            raise ValueError(f"{env_var} environment variable not set")
        return key


# Initialize logging immediately
Config.init_logging()


class ModelConfig(BaseModel):
    """Validation schema for LLM configurations"""
    provider: Literal['openai', 'anthropic', 'grok', 'azure_openai']
    model_name: str
    temperature: float = 0.7
    max_tokens: int = 2048
    streaming: bool = True
    structured_output_method: Optional[str] = None


def get_model_instance(provider: str, **kwargs):
    # Validate provider first
    if not provider or provider not in Config.MODEL_CONFIGS:
        available = list(Config.MODEL_CONFIGS.keys())
        raise ValueError(f"Invalid LLM provider: {provider}. Available: {available}")

    try:
        # Validate configuration against schema
        config_data = {**Config.MODEL_CONFIGS[provider], **kwargs}
        config_obj = ModelConfig(
            provider=provider,
            **config_data
        )
    except ValidationError as e:
        error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
        raise ValueError(
            "Invalid model configuration:" + "\n" + "\n".join(error_messages)
        )

    # Exclude structured_output_method from model params
    model_params = config_obj.model_dump(exclude={'provider', 'structured_output_method'})

    logging.info("Creating model instance for provider: %s with params: %s",
                 provider, model_params)

    if provider == "anthropic":
        return ChatAnthropic(
            api_key=Config.get_api_key("anthropic"),
            **model_params
        )
    elif provider in ["openai", "grok"]:
        return ChatOpenAI(
            api_key=Config.get_api_key(provider),
            **model_params
        )
    else:
        raise ValueError(f"Unsupported provider: {provider}")


class LLMCacheStats:
    """Tracks statistics for the LLM cache."""
    def __init__(self):
        self.hits = 0
        self.misses = 0
        self.invalidations = 0

    def __str__(self):
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        return (
            f"Cache Stats - Hits: {self.hits}, Misses: {self.misses}, "
            f"Hit Rate: {hit_rate:.1f}%, Invalidations: {self.invalidations}"
        )


# Global cache statistics
llm_cache_stats = LLMCacheStats()


def safe_json_dumps(obj: Any) -> str:
    """Safely convert object to JSON string, handling edge cases."""
    try:
        return json.dumps(obj, sort_keys=True)
    except (TypeError, ValueError, OverflowError) as e:
        logging.warning(f"JSON serialization failed: {e}. Using str representation.")
        return str(obj)


@lru_cache(maxsize=None)  # Size controlled by wrapper
def _get_cached_llm_inner(provider: str, model_kwargs_json: str):
    """Internal cached function for LLM instantiation."""
    model_kwargs = json.loads(model_kwargs_json)
    return get_model_instance(provider, **model_kwargs)


def _cache_wrapper(func: Callable) -> Callable:
    """Wrapper to add cache statistics and invalidation."""
    cache = lru_cache(maxsize=Config.LLM_CACHE_SIZE)(func)

    @wraps(func)
    def wrapper(*args, **kwargs):
        # Check for API key changes
        if Config.api_keys_changed():
            logging.info("API keys changed - invalidating LLM cache")
            cache.cache_clear()
            llm_cache_stats.invalidations += 1

        # Track cache statistics
        result = cache(*args, **kwargs)
        if hasattr(cache, 'cache_info'):
            info = cache.cache_info()
            llm_cache_stats.hits = info.hits
            llm_cache_stats.misses = info.misses

        return result

    # Expose cache clear method
    wrapper.cache_clear = cache.cache_clear
    return wrapper


@_cache_wrapper
def _get_cached_llm(provider: str, model_kwargs_json: str):
    """
    Helper function that creates and caches LLM instances based on configuration.
    Uses JSON string of model_kwargs as a hashable cache key.

    Args:
        provider: The LLM provider name
        model_kwargs_json: JSON string of model configuration parameters

    Returns:
        Cached LLM instance
    """
    try:
        return _get_cached_llm_inner(provider, model_kwargs_json)
    except json.JSONDecodeError as e:
        logging.error(f"Failed to decode model_kwargs_json: {e}")
        raise ValueError(f"Invalid model configuration JSON: {e}")


def get_llm(configurable: dict = {}) -> BaseChatModel:
    """
    Factory function to create a language model instance based on configuration.
    Uses caching to avoid re-instantiating the LLM if the configuration hasn't changed.

    The cache size can be configured via the LLM_CACHE_SIZE environment variable.
    Cache statistics are available via the llm_cache_stats global variable.

    Args:
        configurable: Optional configuration dictionary that can include:
               - provider: Provider name ("anthropic", "openai", "grok", etc.)
               - model_kwargs: Additional keyword arguments for the model

    Returns:
        A cached instance of BaseChatModel configured according to the specified parameters.

    Raises:
        ValueError: If the configuration is invalid or JSON serialization fails
    """
    provider = configurable.get('provider', 'openai')
    model_kwargs = configurable.get('model_kwargs', {})

    try:
        # Convert model_kwargs to a JSON string for hashing
        model_kwargs_json = safe_json_dumps(model_kwargs)
        return _get_cached_llm(provider, model_kwargs_json)
    except Exception as e:
        logging.error(f"Failed to create LLM instance: {e}")
        raise ValueError(f"Failed to create LLM instance: {e}")


def create_model_config(
    model: Optional[str] = None,
    system_message: Optional[str] = None,
    **kwargs
) -> RunnableConfig:
    """
    Updated config creator with validation

    :param model: Model identifier (e.g. "gpt-4o")
    :param system_message: Role definition for the assistant
    :param **kwargs: Additional config parameters
    """
    validated = ConfigSchema(
        model=model or Config.DEFAULT_MODEL,
        system_message=system_message,
        temperature=kwargs.get('temperature', Config.DEFAULT_TEMPERATURE),
        provider=kwargs.get('provider', Config.DEFAULT_PROVIDER),
        top_p=kwargs.get('top_p', 0.1),
        max_tokens=kwargs.get('max_tokens', 4000)
    )
    return {"configurable": validated}

================
File: langstuff_multi_agent/utils/memory.py
================
from typing import List, TypedDict
import os
from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
import uuid


class MemoryTriple(TypedDict):
    subject: str
    predicate: str
    object_: str


class MemoryManager:
    def __init__(self, persist_path="memory_store"):
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = Chroma(
            collection_name="agent_memories",
            embedding_function=self.embeddings,
            persist_directory=persist_path
        )

    def save_memory(self, user_id: str, memories: List[MemoryTriple]):
        docs = [
            Document(
                page_content=f"{m['subject']} {m['predicate']} {m['object_']}",
                metadata={"user_id": user_id, **m}
            ) for m in memories
        ]
        self.vector_store.add_documents(docs)

    def search_memories(self, user_id: str, query: str, k=3) -> List[str]:
        results = self.vector_store.similarity_search(
            query, k=k,
            filter={"user_id": user_id}
        )
        return [f"{doc.metadata['subject']} {doc.metadata['predicate']} {doc.metadata['object_']}" 
                for doc in results]

================
File: langstuff_multi_agent/utils/tools.py
================
# langstuff_multi_agent/utils/tools.py
"""
This module defines various utility tools for the LangGraph multi-agent AI project.
Each tool is decorated with @tool from langchain_core.tools to ensure compatibility
with LangGraph. These tools are now implemented to be fully functional.

The tools include:
  - search_web: Perform an actual web search via SerpAPI.
  - python_repl: Execute Python code in a restricted environment.
  - read_file: Read file contents from disk.
  - write_file: Write content to a file on disk.
  - calendar_tool: Append event details to a local calendar file.
  - task_tracker_tool: Insert tasks into a local SQLite database.
  - job_search_tool: Perform job search queries via SerpAPI.
  - get_current_weather: Retrieve weather data via OpenWeatherMap.
  - calc_tool: Evaluate mathematical expressions safely.
  - news_tool: Retrieve news headlines using NewsAPI.
"""

import os
import requests
import sqlite3
import io
import contextlib
from langchain_core.tools import tool
from typing import Dict, Any, List
from langstuff_multi_agent.config import get_llm
from langgraph.prebuilt import ToolNode
from .memory import MemoryManager


def has_tool_calls(message: Dict[str, Any]) -> bool:
    """
    Check if a message contains tool calls.

    Args:
        message: A dictionary containing message data that might have tool calls

    Returns:
        bool: True if the message contains tool calls, False otherwise
    """
    if not isinstance(message, dict):
        return False

    # Check for tool_calls in the message
    tool_calls = message.get("tool_calls", [])
    if tool_calls and isinstance(tool_calls, list):
        return True

    # Check for function_call in the message (older format)
    function_call = message.get("function_call")
    if function_call and isinstance(function_call, dict):
        return True

    return False


# ---------------------------
# REAL WEB SEARCH TOOL
# ---------------------------
@tool(return_direct=True)
def search_web(query: str) -> str:
    """
    Performs a real web search using SerpAPI.
    Requires SERPAPI_API_KEY to be set as an environment variable.

    :param query: The search query.
    :return: A string with a summary of top search results.
    """
    api_key = os.environ.get("SERPAPI_API_KEY")
    if not api_key:
        raise ValueError("SERPAPI_API_KEY environment variable not set")
    params = {
        "engine": "google",
        "q": query,
        "api_key": api_key,
        "num": 5,
    }
    response = requests.get("https://serpapi.com/search", params=params)
    if response.status_code != 200:
        return f"Error performing web search: {response.text}"
    data = response.json()
    results = []
    for result in data.get("organic_results", []):
        title = result.get("title", "No title")
        snippet = result.get("snippet", "")
        link = result.get("link", "")
        results.append(f"{title}: {snippet} ({link})")
    return "\n".join(results)


# ---------------------------
# PYTHON REPL TOOL
# ---------------------------
@tool
def python_repl(code: str) -> str:
    """
    Executes Python code in a restricted environment.

    WARNING: Executing arbitrary code can be dangerous. This implementation uses a
    limited set of safe built-ins. In production, consider a proper sandbox.

    :param code: The Python code to execute.
    :return: The output produced by the code.
    """
    try:
        # Define a restricted set of safe built-ins.
        safe_builtins = {
            "print": print,
            "range": range,
            "len": len,
            "str": str,
            "int": int,
            "float": float,
            "bool": bool,
            "list": list,
            "dict": dict,
            "set": set,
            "min": min,
            "max": max,
            "sum": sum,
        }
        restricted_globals = {"__safe_builtins__": safe_builtins}
        restricted_locals = {}
        output = io.StringIO()

        with contextlib.redirect_stdout(output):
            exec(code, restricted_globals, restricted_locals)

        return output.getvalue()
    except SyntaxError as e:
        return f"Syntax error: {e}"
    except Exception as e:
        return f"Execution error: {e}"


def code(state, config):
    """Enhanced Coder Agent with Fix Suggestions"""
    llm = get_llm(config.get("configurable", {}))
    code_snippet = state["messages"][-1]["content"]

    result = python_repl(code_snippet)

    if "error" in result.lower():
        suggestion = llm.invoke([{
            "role": "user",
            "content": f"The following code produced an error:\n{code_snippet}\nSuggest a fix."
        }])
        return {"messages": [result, suggestion]}

    return {"messages": [result]}


# ---------------------------
# READ FILE TOOL
# ---------------------------
@tool
def read_file(filepath: str) -> str:
    """
    Reads the content of a file from disk.

    :param filepath: The path to the file.
    :return: The file's content or an error message.
    """
    try:
        with open(filepath, 'r', encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        return f"Error reading file '{filepath}': {str(e)}"


# ---------------------------
# WRITE FILE TOOL
# ---------------------------
@tool
def write_file(params: dict) -> str:
    """
    Writes content to a file on disk.

    Expects a dictionary with the keys:
      - 'filepath': The path to the file.
      - 'content': The content to write.

    :param params: Dictionary containing file path and content.
    :return: Success message or an error message.
    """
    try:
        filepath = params.get("filepath")
        content = params.get("content", "")
        with open(filepath, 'w', encoding="utf-8") as file:
            file.write(content)
        return f"Successfully wrote to '{filepath}'."
    except Exception as e:
        return f"Error writing to file '{filepath}': {str(e)}"


# ---------------------------
# CALENDAR TOOL
# ---------------------------
@tool
def calendar_tool(event_details: str) -> str:
    """
    Adds an event to a local calendar file.

    This implementation appends the event details to a file named 'calendar.txt'.

    :param event_details: Details of the event.
    :return: Confirmation message.
    """
    try:
        with open("calendar.txt", "a", encoding="utf-8") as f:
            f.write(event_details + "\n")
        return f"Event added to calendar: {event_details}"
    except Exception as e:
        return f"Error updating calendar: {str(e)}"


# ---------------------------
# TASK TRACKER TOOL (using SQLite)
# ---------------------------
# Initialize the SQLite database for task tracking.
def init_task_db():
    conn = sqlite3.connect("tasks.db")
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS tasks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_details TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()


init_task_db()


@tool
def task_tracker_tool(task_details: str) -> str:
    """
    Adds a new task to the task tracker using a local SQLite database.

    :param task_details: Details of the task.
    :return: Confirmation message.
    """
    try:
        conn = sqlite3.connect("tasks.db")
        c = conn.cursor()
        c.execute("INSERT INTO tasks (task_details) VALUES (?)", (task_details,))
        conn.commit()
        task_id = c.lastrowid
        conn.close()
        return f"Task {task_id} added: {task_details}"
    except Exception as e:
        return f"Error adding task: {str(e)}"


# ---------------------------
# JOB SEARCH TOOL (using SerpAPI for Google Jobs)
# ---------------------------
@tool(return_direct=True)
def job_search_tool(query: str) -> str:
    """
    Performs a job search using the SerpAPI Google Jobs engine.

    Requires SERPAPI_API_KEY to be set as an environment variable.

    :param query: The job search query.
    :return: A summary string of job listings.
    """
    api_key = os.environ.get("SERPAPI_API_KEY")
    if not api_key:
        raise ValueError("SERPAPI_API_KEY environment variable not set")
    params = {
        "engine": "google_jobs",
        "q": query,
        "api_key": api_key,
    }
    response = requests.get("https://serpapi.com/search", params=params)
    if response.status_code != 200:
        return f"Error performing job search: {response.text}"
    data = response.json()
    results = []
    for job in data.get("job_results", []):
        title = job.get("title", "No title")
        company = job.get("company", "Unknown")
        location = job.get("location", "Unknown")
        snippet = job.get("snippet", "")
        results.append(f"{title} at {company} in {location}: {snippet}")
    return "\n".join(results)


# ---------------------------
# CURRENT WEATHER TOOL (using OpenWeatherMap)
# ---------------------------
@tool(return_direct=True)
def get_current_weather(location: str) -> str:
    """
    Retrieves current weather information for a given location using the OpenWeatherMap API.

    Requires OPENWEATHER_API_KEY to be set as an environment variable.

    :param location: The city name for which to retrieve weather.
    :return: Weather details as a string.
    """
    api_key = os.environ.get("OPENWEATHER_API_KEY")
    if not api_key:
        raise ValueError("OPENWEATHER_API_KEY environment variable not set")
    params = {
        "q": location,
        "appid": api_key,
        "units": "imperial"  # Fahrenheit
    }
    response = requests.get("http://api.openweathermap.org/data/2.5/weather", params=params)
    if response.status_code != 200:
        return f"Error fetching weather: {response.text}"
    data = response.json()
    temp = data["main"]["temp"]
    wind_speed = data["wind"]["speed"]
    wind_direction = data["wind"].get("deg", "N/A")
    return f"Current weather in {location}: {temp}°F, wind {wind_speed} mph at {wind_direction}°"


# ---------------------------
# CALCULATION TOOL
# ---------------------------
@tool
def calc_tool(expression: str) -> str:
    """
    Evaluates a simple mathematical expression safely.

    Uses eval with a restricted __builtins__.

    :param expression: A string containing the mathematical expression.
    :return: The result as a string or an error message.
    """
    try:
        safe_builtins = {
            "abs": abs,
            "round": round,
            "min": min,
            "max": max,
            "sum": sum,
        }
        result = eval(expression, {"__builtins__": safe_builtins}, {})
        return str(result)
    except Exception as e:
        return f"Error evaluating expression: {str(e)}"


# ---------------------------
# NEWS TOOL (using NewsAPI)
# ---------------------------
@tool(return_direct=True)
def news_tool(topic: str) -> str:
    """
    Retrieves news headlines for a given topic using the NewsAPI.

    Requires NEWSAPI_API_KEY to be set as an environment variable.

    :param topic: The news topic.
    :return: A summary string of news headlines.
    """
    api_key = os.environ.get("NEWSAPI_API_KEY")
    if not api_key:
        raise ValueError("NEWSAPI_API_KEY environment variable not set")
    params = {
        "q": topic,
        "apiKey": api_key,
        "pageSize": 5,
        "sortBy": "relevancy",
    }
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    if response.status_code != 200:
        return f"Error fetching news: {response.text}"
    data = response.json()
    results = []
    for article in data.get("articles", []):
        title = article.get("title", "No title")
        source = article.get("source", {}).get("name", "Unknown source")
        results.append(f"{title} ({source})")
    return "\n".join(results)


# ---------------------------
# TOOL NODE IMPLEMENTATION (UPDATED)
# ---------------------------

def get_tool_node(tools: List[Any]) -> ToolNode:
    """
    Creates and returns a configured ToolNode instance for LangGraph workflows.

    Args:
        tools: List of @tool-decorated functions to include in the node

    Returns:
        Properly configured ToolNode instance
    """
    try:
        return ToolNode(tools)
    except NameError as e:
        if "ToolNode" in str(e):
            raise ImportError(
                "LangGraph version >=0.1.2 required. Install with: "
                "pip install langgraph>=0.1.2"
            ) from e
        raise

memory = MemoryManager()

@tool
def save_memory(memories: List[dict], config: RunnableConfig) -> str:
    """Save important facts about users or conversations"""
    user_id = config.get("configurable", {}).get("user_id", "global")
    memory.save_memory(user_id, memories)
    return "Memories saved successfully"

@tool 
def search_memories(query: str, config: RunnableConfig) -> List[str]:
    """Search long-term conversation memories"""
    user_id = config.get("configurable", {}).get("user_id", "global")
    return memory.search_memories(user_id, query)

================
File: requirements.txt
================
langgraph>=0.0.20
langchain-anthropic>=0.0.10
langchain-core
langchain-openai
python-dotenv
tavily-python
langchain_community>=0.3.17
chromadb
pypika
embedchain
