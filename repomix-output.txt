This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-11T10:50:03.511Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
langgraph.json
langstuff_multi_agent/__init__.py
langstuff_multi_agent/agent.py
langstuff_multi_agent/agents/analyst.py
langstuff_multi_agent/agents/coder.py
langstuff_multi_agent/agents/context_manager.py
langstuff_multi_agent/agents/debugger.py
langstuff_multi_agent/agents/general_assistant.py
langstuff_multi_agent/agents/life_coach.py
langstuff_multi_agent/agents/professional_coach.py
langstuff_multi_agent/agents/project_manager.py
langstuff_multi_agent/agents/researcher.py
langstuff_multi_agent/agents/supervisor.py
langstuff_multi_agent/config.py
langstuff_multi_agent/utils/tools.py
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
.env

================
File: langgraph.json
================
{
    "version": "1.0",
    "project": "LangGraph Multi-Agent AI",
    "description": "Configuration for deploying the LangGraph multi-agent AI project via LangGraph Studio.",
    "entry_point": "./langstuff_multi_agent/agent.py:graph",
    "graphs": {
        "main": "./langstuff_multi_agent/agent.py:graph",
        "debugger": "./langstuff_multi_agent/agent.py:debugger_graph",
        "context_manager": "./langstuff_multi_agent/agent.py:context_manager_graph",
        "project_manager": "./langstuff_multi_agent/agent.py:project_manager_graph",
        "professional_coach": "./langstuff_multi_agent/agent.py:professional_coach_graph",
        "life_coach": "./langstuff_multi_agent/agent.py:life_coach_graph",
        "coder": "./langstuff_multi_agent/agent.py:coder_graph",
        "analyst": "./langstuff_multi_agent/agent.py:analyst_graph",
        "researcher": "./langstuff_multi_agent/agent.py:researcher_graph",
        "general_assistant": "./langstuff_multi_agent/agent.py:general_assistant_graph"
    },
    "agents": [
        {
            "name": "SUPERVISOR",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "main",
            "description": "Main supervisor agent that routes requests to specialized agents."
        },
        {
            "name": "DEBUGGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "debugger",
            "description": "Agent responsible for debugging code."
        },
        {
            "name": "CONTEXT_MANAGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "context_manager",
            "description": "Agent responsible for managing conversation context."
        },
        {
            "name": "PROJECT_MANAGER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "project_manager",
            "description": "Agent responsible for managing project timelines and tasks."
        },
        {
            "name": "PROFESSIONAL_COACH",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "professional_coach",
            "description": "Agent providing professional and career guidance."
        },
        {
            "name": "LIFE_COACH",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "life_coach",
            "description": "Agent providing lifestyle and personal advice."
        },
        {
            "name": "CODER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "coder",
            "description": "Agent that assists with coding tasks."
        },
        {
            "name": "ANALYST",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "analyst",
            "description": "Agent specializing in data analysis."
        },
        {
            "name": "RESEARCHER",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "researcher",
            "description": "Agent that gathers and summarizes research and news."
        },
        {
            "name": "GENERAL_ASSISTANT",
            "file": "./langstuff_multi_agent/agent.py",
            "graph": "general_assistant",
            "description": "Agent for general queries and assistance."
        }
    ],
    "dependencies": [
        "langgraph>=0.0.20",
        "langchain-anthropic>=0.0.10",
        "langchain-core>=0.1.20",
        "langchain-openai>=0.0.5",
        "python-dotenv>=1.0.0",
        "tavily-python>=0.5.1",
        "langchain_community>=0.3.17",
        "./langstuff_multi_agent"
    ],
    "configuration": {
        "xai_api_key": "${XAI_API_KEY}",
        "anthropic_api_key": "${ANTHROPIC_API_KEY}",
        "openai_api_key": "${OPENAI_API_KEY}",
        "tavily_api_key": "${TAVILY_API_KEY}",
        "model_settings": {
            "default_provider": "openai",
            "default_model": "gpt-4-turbo-2024-04-09",
            "default_temperature": 0.0,
            "default_top_p": 0.1,
            "default_max_tokens": 4000
        },
        "checkpointer": "MemorySaver",
        "logging": {
            "level": "INFO",
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        }
    }
}

================
File: langstuff_multi_agent/__init__.py
================
"""
LangStuff Multi-Agent package.
A multi-agent system built with LangGraph for handling various 
specialized tasks.
"""

__version__ = "0.1.0"

================
File: langstuff_multi_agent/agent.py
================
# langstuff_multi_agent/agent.py
"""
Main agent module that exports the graph for LangGraph Studio.

This module serves as the entry point for LangGraph Studio, exporting only
the primary supervisor workflow. This avoids potential MultipleSubgraphsError
by isolating internal subgraphs.
"""

import logging
from langgraph.graph import Graph
from langstuff_multi_agent.agents.supervisor import supervisor_workflow

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Initializing primary supervisor workflow...")

# Compile only the supervisor workflow as the entry point.
graph = supervisor_workflow.compile()

__all__ = ["graph"]

logger.info("Primary supervisor workflow successfully initialized.")

================
File: langstuff_multi_agent/agents/analyst.py
================
# langstuff_multi_agent/agents/analyst.py
"""
Analyst Agent module for data analysis and interpretation.

This module provides a workflow for analyzing data and performing
calculations using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, calc_tool, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

analyst_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for analysis tasks
tools = [search_web, python_repl, calc_tool]
tool_node = ToolNode(tools)


def analyze_data(state, config):
    """Analyze data with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are an Analyst Agent. Your task is to analyze data, perform calculations, and interpret results.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Retrieve background information and data.\n"
                            "- python_repl: Run Python code to perform calculations and tests.\n"
                            "- calc_tool: Execute specific calculations and numerical analysis.\n\n"
                            "Instructions:\n"
                            "1. Review the data or query provided by the user.\n"
                            "2. Perform necessary calculations and analyze the results.\n"
                            "3. Summarize your findings in clear, concise language."
                        ),
                    }
                ]
            )
        ]
    }


analyst_workflow.add_node("analyze_data", analyze_data)
analyst_workflow.add_node("tools", tool_node)

analyst_workflow.add_edge(START, "analyze_data")

analyst_workflow.add_conditional_edges(
    "analyze_data",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

analyst_workflow.add_edge("tools", "analyze_data")

__all__ = ["analyst_workflow"]

================
File: langstuff_multi_agent/agents/coder.py
================
# langstuff_multi_agent/agents/coder.py
"""
Coder Agent module for writing and improving code.

This module provides a workflow for code generation, debugging,
and optimization using various development tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, read_file, write_file, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

coder_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for coding tasks.
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)


def code(state, config):
    """Write and improve code with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)

    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Coder Agent. Your task is to write, debug, and improve code.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Find coding examples and documentation.\n"
                            "- python_repl: Execute and test Python code snippets.\n"
                            "- read_file: Retrieve code from files.\n"
                            "- write_file: Save code modifications to files.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code or coding request.\n"
                            "2. Provide solutions, test code, and explain your reasoning.\n"
                            "3. Use the available tools to execute code and verify fixes as necessary."
                        ),
                    }
                ]
            )
        ]
    }


coder_workflow.add_node("code", code)
coder_workflow.add_node("tools", tool_node)

coder_workflow.add_edge(START, "code")

coder_workflow.add_conditional_edges(
    "code",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {
        "tools": "tools",
        "END": END
    }
)

coder_workflow.add_edge("tools", "code")

__all__ = ["coder_workflow"]

================
File: langstuff_multi_agent/agents/context_manager.py
================
# langstuff_multi_agent/agents/context_manager.py
"""
Context Manager Agent module for tracking conversation context.

This module provides a workflow for managing conversation history
and maintaining context across interactions.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, read_file, write_file, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

context_manager_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for context management
tools = [search_web, read_file, write_file]
tool_node = ToolNode(tools)


def manage_context(state, config):
    """Manage conversation context with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Context Manager Agent. Your task is to track and manage conversation context.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Keep track of key information and topics discussed in the conversation.\n"
                            "2. Summarize important points and decisions made.\n"
                            "3. Use read_file and write_file to store and retrieve context information.\n"
                            "4. If necessary, use search_web to gather additional context.\n"
                            "5. Ensure that the conversation stays focused and relevant."
                        ),
                    }
                ]
            )
        ]
    }


context_manager_workflow.add_node("manage_context", manage_context)
context_manager_workflow.add_node("tools", tool_node)

context_manager_workflow.add_edge(START, "manage_context")

context_manager_workflow.add_conditional_edges(
    "manage_context",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

context_manager_workflow.add_edge("tools", "manage_context")

__all__ = ["context_manager_workflow"]

================
File: langstuff_multi_agent/agents/debugger.py
================
# langstuff_multi_agent/agents/debugger.py
"""
Debugger Agent module for analyzing code and identifying errors.

This module provides a workflow for debugging code using various tools
and LLM-based analysis.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, python_repl, read_file, write_file, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

debugger_workflow = StateGraph(MessagesState, ConfigSchema)

# Define the tools available to the Debugger Agent
tools = [search_web, python_repl, read_file, write_file]
tool_node = ToolNode(tools)


def analyze_code(state, config):
    """Analyze code and identify errors with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Debugger Agent. Your task is to identify and analyze code errors.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for general information and recent content.\n"
                            "- python_repl: Execute Python code.\n"
                            "- read_file: Read the contents of a file.\n"
                            "- write_file: Write content to a file.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's code and identify potential errors.\n"
                            "2. Use the search_web tool to find relevant information about the error or related debugging techniques.\n"
                            "3. Use the python_repl tool to execute code snippets and test potential fixes.\n"
                            "4. If necessary, use read_file and write_file to modify the code.\n"
                            "5. Provide clear and concise explanations of the error and the debugging process."
                        ),
                    }
                ]
            )
        ]
    }


debugger_workflow.add_node("analyze_code", analyze_code)
debugger_workflow.add_node("tools", tool_node)

debugger_workflow.add_edge(START, "analyze_code")

debugger_workflow.add_conditional_edges(
    "analyze_code",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

debugger_workflow.add_edge("tools", "analyze_code")

__all__ = ["debugger_workflow"]

================
File: langstuff_multi_agent/agents/general_assistant.py
================
# langstuff_multi_agent/agents/general_assistant.py
"""
General Assistant Agent module for handling diverse queries.

This module provides a workflow for addressing general user requests
using a variety of tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

general_assistant_workflow = StateGraph(MessagesState, ConfigSchema)

# Define general assistant tools
tools = [search_web, get_current_weather]
tool_node = ToolNode(tools)


def assist(state, config):
    """Provide general assistance with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a General Assistant Agent. Your task is to assist with a variety of general queries and tasks.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Provide general information and answer questions.\n"
                            "- get_current_weather: Retrieve current weather updates.\n\n"
                            "Instructions:\n"
                            "1. Understand the user's request.\n"
                            "2. Use the available tools to gather relevant information when needed.\n"
                            "3. Provide clear, concise, and helpful responses to assist the user."
                        ),
                    }
                ]
            )
        ]
    }


general_assistant_workflow.add_node("assist", assist)
general_assistant_workflow.add_node("tools", tool_node)

general_assistant_workflow.add_edge(START, "assist")

general_assistant_workflow.add_conditional_edges(
    "assist",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

general_assistant_workflow.add_edge("tools", "assist")

__all__ = ["general_assistant_workflow"]

================
File: langstuff_multi_agent/agents/life_coach.py
================
# langstuff_multi_agent/agents/life_coach.py
"""
Life Coach Agent module for personal advice and guidance.

This module provides a workflow for offering lifestyle tips and
personal development advice using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, get_current_weather, calendar_tool, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

life_coach_workflow = StateGraph(MessagesState, ConfigSchema)

# Define tools for life coaching
tools = [search_web, get_current_weather, calendar_tool]
tool_node = ToolNode(tools)


def life_coach(state, config):
    """Provide life coaching with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Life Coach Agent. Your task is to provide personal advice and lifestyle tips.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up general lifestyle tips and motivational content.\n"
                            "- get_current_weather: Provide weather updates to help plan outdoor activities.\n"
                            "- calendar_tool: Assist in scheduling and planning daily routines.\n\n"
                            "Instructions:\n"
                            "1. Listen to the user's personal queries and lifestyle challenges.\n"
                            "2. Offer practical advice and motivational support.\n"
                            "3. Use the available tools to supply additional context when necessary.\n"
                            "4. Maintain an empathetic and encouraging tone throughout the conversation."
                        ),
                    }
                ]
            )
        ]
    }


life_coach_workflow.add_node("life_coach", life_coach)
life_coach_workflow.add_node("tools", tool_node)

life_coach_workflow.add_edge(START, "life_coach")

life_coach_workflow.add_conditional_edges(
    "life_coach",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

life_coach_workflow.add_edge("tools", "life_coach")

__all__ = ["life_coach_workflow"]

================
File: langstuff_multi_agent/agents/professional_coach.py
================
# langstuff_multi_agent/agents/professional_coach.py
"""
Professional Coach Agent module for career guidance.

This module provides a workflow for offering career advice and
job search strategies using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, job_search_tool, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

professional_coach_workflow = StateGraph(MessagesState, ConfigSchema)

# Define the tools for professional coaching
tools = [search_web, job_search_tool]
tool_node = ToolNode(tools)


def coach(state, config):
    """Provide professional coaching with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Professional Coach Agent. Your task is to provide career advice and job search strategies.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search for career advice and job market trends.\n"
                            "- job_search_tool: Retrieve job listings and career opportunities.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's career-related queries.\n"
                            "2. Offer actionable advice and strategies for job searching.\n"
                            "3. Use the available tools to provide up-to-date information and resources.\n"
                            "4. Communicate in a supportive and motivational tone."
                        ),
                    }
                ]
            )
        ]
    }


professional_coach_workflow.add_node("coach", coach)
professional_coach_workflow.add_node("tools", tool_node)

professional_coach_workflow.add_edge(START, "coach")

professional_coach_workflow.add_conditional_edges(
    "coach",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

professional_coach_workflow.add_edge("tools", "coach")

__all__ = ["professional_coach_workflow"]

================
File: langstuff_multi_agent/agents/project_manager.py
================
# langstuff_multi_agent/agents/project_manager.py
"""
Project Manager Agent module for task and timeline management.

This module provides a workflow for overseeing project schedules
and coordinating tasks using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, calendar_tool, task_tracker_tool, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

project_manager_workflow = StateGraph(MessagesState, ConfigSchema)

# Define project management tools
tools = [search_web, calendar_tool, task_tracker_tool]
tool_node = ToolNode(tools)


def manage_project(state, config):
    """Manage project with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Project Manager Agent. Your task is to oversee project timelines, tasks, and scheduling.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Search the web for project management best practices.\n"
                            "- calendar_tool: Access and update project calendars.\n"
                            "- task_tracker_tool: Manage and update project task lists.\n\n"
                            "Instructions:\n"
                            "1. Review project details and timelines.\n"
                            "2. Update project schedules and task lists as needed.\n"
                            "3. Use search_web for additional project management information.\n"
                            "4. Provide clear instructions and updates regarding project progress."
                        ),
                    }
                ]
            )
        ]
    }


project_manager_workflow.add_node("manage_project", manage_project)
project_manager_workflow.add_node("tools", tool_node)

project_manager_workflow.add_edge(START, "manage_project")

project_manager_workflow.add_conditional_edges(
    "manage_project",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

project_manager_workflow.add_edge("tools", "manage_project")

__all__ = ["project_manager_workflow"]

================
File: langstuff_multi_agent/agents/researcher.py
================
# langstuff_multi_agent/agents/researcher.py
"""
Researcher Agent module for gathering and summarizing information.

This module provides a workflow for gathering and summarizing news and research 
information using various tools.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langstuff_multi_agent.utils.tools import search_web, news_tool, has_tool_calls
from langchain_anthropic import ChatAnthropic
from langstuff_multi_agent.config import ConfigSchema, get_llm

researcher_workflow = StateGraph(MessagesState, ConfigSchema)

# Define research tools
tools = [search_web, news_tool]
tool_node = ToolNode(tools)


def research(state, config):
    """Conduct research with configuration support."""
    llm = get_llm(config.get("configurable", {}))
    llm = llm.bind_tools(tools)
    return {
        "messages": [
            llm.invoke(
                state["messages"] + [
                    {
                        "role": "system",
                        "content": (
                            "You are a Researcher Agent. Your task is to gather and summarize news and research information.\n\n"
                            "You have access to the following tools:\n"
                            "- search_web: Look up recent information and background data.\n"
                            "- news_tool: Retrieve the latest news headlines and articles.\n\n"
                            "Instructions:\n"
                            "1. Analyze the user's research query.\n"
                            "2. Use the available tools to gather accurate and relevant information.\n"
                            "3. Provide a clear summary of your findings."
                        ),
                    }
                ]
            )
        ]
    }


researcher_workflow.add_node("research", research)
researcher_workflow.add_node("tools", tool_node)

researcher_workflow.add_edge(START, "research")

researcher_workflow.add_conditional_edges(
    "research",
    lambda state: "tools" if has_tool_calls(state.get("messages", [])) else "END",
    {"tools": "tools", "END": END}
)

researcher_workflow.add_edge("tools", "research")

__all__ = ["researcher_workflow"]

================
File: langstuff_multi_agent/agents/supervisor.py
================
# langstuff_multi_agent/agents/supervisor.py
"""
Supervisor Agent module for integrating and routing individual LangGraph agent workflows.
"""

from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langstuff_multi_agent.config import get_llm, ConfigSchema
from typing_extensions import TypedDict, Literal

# Import individual workflows.
from langstuff_multi_agent.agents.debugger import debugger_workflow
from langstuff_multi_agent.agents.context_manager import context_manager_workflow
from langstuff_multi_agent.agents.project_manager import project_manager_workflow
from langstuff_multi_agent.agents.professional_coach import professional_coach_workflow
from langstuff_multi_agent.agents.life_coach import life_coach_workflow
from langstuff_multi_agent.agents.coder import coder_workflow
from langstuff_multi_agent.agents.analyst import analyst_workflow
from langstuff_multi_agent.agents.researcher import researcher_workflow
from langstuff_multi_agent.agents.general_assistant import general_assistant_workflow


# Define a structured output schema for routing.
class RouteResponse(TypedDict):
    next: Literal[
        "DEBUGGER",
        "CONTEXT_MANAGER",
        "PROJECT_MANAGER",
        "PROFESSIONAL_COACH",
        "LIFE_COACH",
        "CODER",
        "ANALYST",
        "RESEARCHER",
        "GENERAL_ASSISTANT"
    ]


supervisor_workflow = StateGraph(MessagesState, ConfigSchema)

AGENT_OPTIONS = {
    "DEBUGGER": "Code debugging and error analysis",
    "CONTEXT_MANAGER": "Conversation context tracking and management",
    "PROJECT_MANAGER": "Project timeline and task management",
    "PROFESSIONAL_COACH": "Career advice and job search strategies",
    "LIFE_COACH": "Personal development and lifestyle guidance",
    "CODER": "Code writing and improvement",
    "ANALYST": "Data analysis and interpretation",
    "RESEARCHER": "Information gathering and research",
    "GENERAL_ASSISTANT": "General purpose assistance"
}

workflow_map = {
    "DEBUGGER": debugger_workflow,
    "CONTEXT_MANAGER": context_manager_workflow,
    "PROJECT_MANAGER": project_manager_workflow,
    "PROFESSIONAL_COACH": professional_coach_workflow,
    "LIFE_COACH": life_coach_workflow,
    "CODER": coder_workflow,
    "ANALYST": analyst_workflow,
    "RESEARCHER": researcher_workflow,
    "GENERAL_ASSISTANT": general_assistant_workflow
}


def format_agent_list():
    """Format the agent list with descriptions for the prompt."""
    return "\n".join(f"- {name}: {desc}" for name, desc in AGENT_OPTIONS.items())


def route_request(state, config):
    """Route the user request to the appropriate agent workflow using structured LLM output."""
    messages = state.get("messages", [])
    if not messages:
        return {
            "messages": [
                AIMessage(content="Welcome! I'm your AI assistant. How can I help you today?")
            ]
        }

    request = messages[-1].content

    prompt = (
        "You are a Supervisor Agent tasked with routing user requests to the most appropriate specialized agent.\n\n"
        "Available agents and their specialties:\n"
        f"{format_agent_list()}\n\n"
        f"User Request: '{request}'\n\n"
        "Instructions:\n"
        "1. Analyze the user's request carefully.\n"
        "2. Select the most appropriate agent based on their specialties.\n"
        "3. Respond with exactly one agent name from the list (case-insensitive) in JSON format, e.g. {\"next\": \"CODER\"}.\n"
        "4. If unsure, respond with {\"next\": \"GENERAL_ASSISTANT\"}.\n"
        "Selected agent:"
    )

    try:
        supervisor_llm = get_llm(config.get("configurable", {}))
        # Use structured output to enforce JSON formatting.
        route_response = supervisor_llm.with_structured_output(RouteResponse).invoke([SystemMessage(content=prompt)])
        agent_key = route_response["next"].strip().upper()

        if agent_key not in AGENT_OPTIONS:
            agent_key = "GENERAL_ASSISTANT"

        workflow = workflow_map[agent_key].compile()
        result = workflow.invoke(
            {"messages": [
                SystemMessage(content=f"You are the {agent_key} agent, specialized in {AGENT_OPTIONS[agent_key]}."), 
                HumanMessage(content=request)
            ]},
            config=config
        )

        return {
            "messages": [
                AIMessage(content=f"[Routing to {agent_key} - {AGENT_OPTIONS[agent_key]}]\n\n"),
                *result["messages"]
            ]
        }

    except Exception as e:
        return {
            "messages": [
                AIMessage(content=f"I apologize, but I encountered an error while processing your request: {str(e)}\n\n"
                                "Please try rephrasing your request or contact support if the issue persists.")
            ]
        }


supervisor_workflow.add_node("route", route_request)


def create_agent_node(agent_workflow, agent_name):
    def agent_node(state, config):
        compiled_workflow = agent_workflow.compile()
        result = compiled_workflow.invoke(state, config=config)
        return result
    return agent_node


for agent_name, workflow in workflow_map.items():
    supervisor_workflow.add_node(
        agent_name.lower(),
        create_agent_node(workflow, agent_name)
    )

supervisor_workflow.add_edge(START, "route")
for agent_name in workflow_map.keys():
    supervisor_workflow.add_edge("route", agent_name.lower())
    supervisor_workflow.add_edge(agent_name.lower(), END)

__all__ = ["supervisor_workflow"]

================
File: langstuff_multi_agent/config.py
================
# config.py
"""
Configuration settings for the LangGraph multi-agent AI project.

This file reads critical configuration values from environment variables,
provides default settings, initializes logging, sets up a persistent checkpoint
instance using MemorySaver, and exposes configuration and factory functions for LangGraph.

Supported providers:
  - "anthropic": Uses ChatAnthropic with the key from ANTHROPIC_API_KEY.
  - "openai": Uses ChatOpenAI with the key from OPENAI_API_KEY.
  - "grok" (or "xai"): Uses ChatOpenAI as an interface to Grok with the key from XAI_API_KEY.

Note: The LLM instances returned by get_llm() support structured output via the 
.with_structured_output() method. This is essential for our supervisor routing
logic and agent structured responses.
"""

import os
import logging
from langgraph.checkpoint.memory import MemorySaver
from typing import Optional, Dict, Any, Literal
from typing_extensions import TypedDict
from langchain_core.messages import SystemMessage
from langchain_core.runnables.config import RunnableConfig
from pydantic import BaseModel, ValidationError

# Import provider libraries.
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel


class ConfigSchema(TypedDict):
    """Schema for LangGraph runtime configuration."""
    model: Optional[str]
    system_message: Optional[str]
    temperature: Optional[float]
    top_p: Optional[float]
    max_tokens: Optional[int]


class Config:
    # API Keys
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    XAI_API_KEY = os.environ.get("XAI_API_KEY")

    # Default model settings
    DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "gpt-4o")
    DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", 0))
    DEFAULT_PROVIDER = os.environ.get("AI_PROVIDER", "openai").lower()

    # Model configurations
    MODEL_CONFIGS = {
        "anthropic": {
            "model_name": "claude-3-opus-20240229",
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        },
        "openai": {
            "model_name": "gpt-4o",  # Latest GA model
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        },
        "grok": {
            "model_name": "gpt-4o",  # Fallback to latest OpenAI model
            "temperature": 0.0,
            "top_p": 0.1,
            "max_tokens": 4000,
        }
    }

    # Logging configuration
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Persistent checkpointer instance
    PERSISTENT_CHECKPOINTER = MemorySaver()

    @classmethod
    def init_logging(cls):
        """Initialize logging with configured settings."""
        logging.basicConfig(level=cls.LOG_LEVEL, format=cls.LOG_FORMAT)
        logging.info("Logging initialized at level: %s", cls.LOG_LEVEL)

    @classmethod
    def get_api_key(cls, provider: str) -> str:
        """Get the API key for the specified provider."""
        key_map = {
            "openai": ("OPENAI_API_KEY", cls.OPENAI_API_KEY),
            "anthropic": ("ANTHROPIC_API_KEY", cls.ANTHROPIC_API_KEY),
            "grok": ("XAI_API_KEY", cls.XAI_API_KEY),
        }

        env_var, key = key_map.get(provider, (None, None))
        if not key:
            raise ValueError(f"{env_var} environment variable not set")
        return key


# Initialize logging immediately
Config.init_logging()


class ModelConfig(BaseModel):
    """Validation schema for LLM configurations"""
    provider: Literal['openai', 'anthropic', 'grok', 'azure_openai']
    model_name: str
    api_key: str
    temperature: float = 0.7
    max_tokens: int = 2048
    streaming: bool = True
    # Optional parameter for specifying the structured output method (e.g., "json_schema", "tool_calling")
    structured_output_method: Optional[str] = None


def get_model_instance(provider: str, **kwargs):
    # Validate provider first
    if not provider or provider not in Config.MODEL_CONFIGS:
        available = list(Config.MODEL_CONFIGS.keys())
        raise ValueError(f"Invalid LLM provider: {provider}. Available: {available}")

    try:
        # Validate configuration against schema
        config_obj = ModelConfig(
            provider=provider,
            **{**Config.MODEL_CONFIGS[provider], **kwargs}
        )
    except ValidationError as e:
        error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
        raise ValueError(
            "Invalid model configuration:" + "\n" + "\n".join(error_messages)
        )

    # Pop structured_output_method if provided, so it's not passed to the LLM constructor.
    structured_output_method = kwargs.pop("structured_output_method", None)
    if structured_output_method:
        logging.info("Structured output method specified: %s", structured_output_method)

    logging.info("Creating model instance for provider: %s with model: %s, temperature: %s, max_tokens: %s",
                 provider, config_obj.model_name, config_obj.temperature, config_obj.max_tokens)

    if provider == "anthropic":
        return ChatAnthropic(
            api_key=Config.get_api_key("anthropic"),
            **config_obj.model_dump(exclude={"structured_output_method"})
        )
    elif provider in ["openai"]:
        return ChatOpenAI(
            api_key=Config.get_api_key("openai"),
            **config_obj.model_dump(exclude={"structured_output_method"})
        )
    elif provider in ["grok"]:
        return ChatOpenAI(
            api_key=Config.get_api_key("grok"),
            **config_obj.model_dump(exclude={"structured_output_method"})
        )
    else:
        raise ValueError(f"Unsupported provider: {provider}")


def get_llm(configurable: dict = {}):
    """
    Factory function to create a language model instance based on configuration.

    Args:
        configurable: Optional configuration dictionary that can include:
               - provider: Provider name ("anthropic", "openai", "grok", etc.)
               - system_message: Optional system message to prepend
               - temperature: Temperature parameter for generation
               - top_p: Top-p parameter for generation
               - max_tokens: Maximum tokens to generate
               - model_kwargs: Additional keyword arguments for the model (e.g., structured_output_method)

    Returns:
        An instance of BaseChatModel configured according to the specified parameters.
        Note: The returned LLM instance supports structured output via .with_structured_output().
    """
    provider = configurable.get('provider', 'openai')  # Set default provider
    model_kwargs = configurable.get('model_kwargs', {})
    return get_model_instance(provider, **model_kwargs)


def create_model_config(
    model: Optional[str] = None,
    system_message: Optional[str] = None,
    **kwargs
) -> RunnableConfig:
    """
    Create a RunnableConfig for LangGraph workflow configuration.

    Args:
        model: Optional model provider to use.
        system_message: Optional system message to prepend.
        **kwargs: Additional configuration parameters.

    Returns:
        RunnableConfig with the specified configuration.
    """
    config = {"model": model} if model else {}
    if system_message:
        config["system_message"] = system_message
    config.update(kwargs)
    return {"configurable": config}

================
File: langstuff_multi_agent/utils/tools.py
================
# langstuff_multi_agent/utils/tools.py
"""
This module defines various utility tools for the LangGraph multi-agent AI project.
Each tool is decorated with @tool from langchain_core.tools to ensure compatibility
with LangGraph. These tools are now implemented to be fully functional.

The tools include:
  - search_web: Perform an actual web search via SerpAPI.
  - python_repl: Execute Python code in a restricted environment.
  - read_file: Read file contents from disk.
  - write_file: Write content to a file on disk.
  - calendar_tool: Append event details to a local calendar file.
  - task_tracker_tool: Insert tasks into a local SQLite database.
  - job_search_tool: Perform job search queries via SerpAPI.
  - get_current_weather: Retrieve weather data via OpenWeatherMap.
  - calc_tool: Evaluate mathematical expressions safely.
  - news_tool: Retrieve news headlines using NewsAPI.
"""

import os
import requests
import sqlite3
import io
import contextlib
from langchain_core.tools import tool
from typing import Dict, Any, Optional, List


def has_tool_calls(message: Dict[str, Any]) -> bool:
    """
    Check if a message contains tool calls.

    Args:
        message: A dictionary containing message data that might have tool calls

    Returns:
        bool: True if the message contains tool calls, False otherwise
    """
    if not isinstance(message, dict):
        return False

    # Check for tool_calls in the message
    tool_calls = message.get("tool_calls", [])
    if tool_calls and isinstance(tool_calls, list):
        return True

    # Check for function_call in the message (older format)
    function_call = message.get("function_call")
    if function_call and isinstance(function_call, dict):
        return True

    return False


# ---------------------------
# REAL WEB SEARCH TOOL
# ---------------------------
@tool
def search_web(query: str) -> str:
    """
    Performs a real web search using SerpAPI.
    Requires SERPAPI_API_KEY to be set as an environment variable.

    :param query: The search query.
    :return: A string with a summary of top search results.
    """
    api_key = os.environ.get("SERPAPI_API_KEY")
    if not api_key:
        raise ValueError("SERPAPI_API_KEY environment variable not set")
    params = {
        "engine": "google",
        "q": query,
        "api_key": api_key,
        "num": 5,
    }
    response = requests.get("https://serpapi.com/search", params=params)
    if response.status_code != 200:
        return f"Error performing web search: {response.text}"
    data = response.json()
    results = []
    for result in data.get("organic_results", []):
        title = result.get("title", "No title")
        snippet = result.get("snippet", "")
        link = result.get("link", "")
        results.append(f"{title}: {snippet} ({link})")
    return "\n".join(results)


# ---------------------------
# PYTHON REPL TOOL
# ---------------------------
@tool
def python_repl(code: str) -> str:
    """
    Executes Python code in a restricted environment.

    WARNING: Executing arbitrary code can be dangerous. This implementation uses a
    limited set of safe built-ins. In production, consider a proper sandbox.

    :param code: The Python code to execute.
    :return: The output produced by the code.
    """
    try:
        # Define a restricted set of safe built-ins.
        safe_builtins = {
            "print": print,
            "range": range,
            "len": len,
            "str": str,
            "int": int,
            "float": float,
            "bool": bool,
            "list": list,
            "dict": dict,
            "set": set,
            "min": min,
            "max": max,
            "sum": sum,
        }
        restricted_globals = {"__builtins__": safe_builtins}
        restricted_locals = {}
        # Capture the output of the exec call.
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            exec(code, restricted_globals, restricted_locals)
        return output.getvalue()
    except Exception as e:
        return f"Error during code execution: {str(e)}"


# ---------------------------
# READ FILE TOOL
# ---------------------------
@tool
def read_file(filepath: str) -> str:
    """
    Reads the content of a file from disk.

    :param filepath: The path to the file.
    :return: The file's content or an error message.
    """
    try:
        with open(filepath, 'r', encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        return f"Error reading file '{filepath}': {str(e)}"


# ---------------------------
# WRITE FILE TOOL
# ---------------------------
@tool
def write_file(params: dict) -> str:
    """
    Writes content to a file on disk.

    Expects a dictionary with the keys:
      - 'filepath': The path to the file.
      - 'content': The content to write.

    :param params: Dictionary containing file path and content.
    :return: Success message or an error message.
    """
    try:
        filepath = params.get("filepath")
        content = params.get("content", "")
        with open(filepath, 'w', encoding="utf-8") as file:
            file.write(content)
        return f"Successfully wrote to '{filepath}'."
    except Exception as e:
        return f"Error writing to file '{filepath}': {str(e)}"


# ---------------------------
# CALENDAR TOOL
# ---------------------------
@tool
def calendar_tool(event_details: str) -> str:
    """
    Adds an event to a local calendar file.

    This implementation appends the event details to a file named 'calendar.txt'.

    :param event_details: Details of the event.
    :return: Confirmation message.
    """
    try:
        with open("calendar.txt", "a", encoding="utf-8") as f:
            f.write(event_details + "\n")
        return f"Event added to calendar: {event_details}"
    except Exception as e:
        return f"Error updating calendar: {str(e)}"


# ---------------------------
# TASK TRACKER TOOL (using SQLite)
# ---------------------------
# Initialize the SQLite database for task tracking.
def init_task_db():
    conn = sqlite3.connect("tasks.db")
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS tasks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_details TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()


init_task_db()


@tool
def task_tracker_tool(task_details: str) -> str:
    """
    Adds a new task to the task tracker using a local SQLite database.

    :param task_details: Details of the task.
    :return: Confirmation message.
    """
    try:
        conn = sqlite3.connect("tasks.db")
        c = conn.cursor()
        c.execute("INSERT INTO tasks (task_details) VALUES (?)", (task_details,))
        conn.commit()
        task_id = c.lastrowid
        conn.close()
        return f"Task {task_id} added: {task_details}"
    except Exception as e:
        return f"Error adding task: {str(e)}"


# ---------------------------
# JOB SEARCH TOOL (using SerpAPI for Google Jobs)
# ---------------------------
@tool
def job_search_tool(query: str) -> str:
    """
    Performs a job search using the SerpAPI Google Jobs engine.

    Requires SERPAPI_API_KEY to be set as an environment variable.

    :param query: The job search query.
    :return: A summary string of job listings.
    """
    api_key = os.environ.get("SERPAPI_API_KEY")
    if not api_key:
        raise ValueError("SERPAPI_API_KEY environment variable not set")
    params = {
        "engine": "google_jobs",
        "q": query,
        "api_key": api_key,
    }
    response = requests.get("https://serpapi.com/search", params=params)
    if response.status_code != 200:
        return f"Error performing job search: {response.text}"
    data = response.json()
    results = []
    for job in data.get("job_results", []):
        title = job.get("title", "No title")
        company = job.get("company", "Unknown")
        location = job.get("location", "Unknown")
        snippet = job.get("snippet", "")
        results.append(f"{title} at {company} in {location}: {snippet}")
    return "\n".join(results)


# ---------------------------
# CURRENT WEATHER TOOL (using OpenWeatherMap)
# ---------------------------
@tool
def get_current_weather(location: str) -> str:
    """
    Retrieves current weather information for a given location using the OpenWeatherMap API.

    Requires OPENWEATHER_API_KEY to be set as an environment variable.

    :param location: The city name for which to retrieve weather.
    :return: Weather details as a string.
    """
    api_key = os.environ.get("OPENWEATHER_API_KEY")
    if not api_key:
        raise ValueError("OPENWEATHER_API_KEY environment variable not set")
    params = {
        "q": location,
        "appid": api_key,
        "units": "imperial"  # Fahrenheit
    }
    response = requests.get("http://api.openweathermap.org/data/2.5/weather", params=params)
    if response.status_code != 200:
        return f"Error fetching weather: {response.text}"
    data = response.json()
    temp = data["main"]["temp"]
    wind_speed = data["wind"]["speed"]
    wind_direction = data["wind"].get("deg", "N/A")
    return f"Current weather in {location}: {temp}°F, wind {wind_speed} mph at {wind_direction}°"


# ---------------------------
# CALCULATION TOOL
# ---------------------------
@tool
def calc_tool(expression: str) -> str:
    """
    Evaluates a simple mathematical expression safely.

    Uses eval with a restricted __builtins__.

    :param expression: A string containing the mathematical expression.
    :return: The result as a string or an error message.
    """
    try:
        safe_builtins = {
            "abs": abs,
            "round": round,
            "min": min,
            "max": max,
            "sum": sum,
        }
        result = eval(expression, {"__builtins__": safe_builtins}, {})
        return str(result)
    except Exception as e:
        return f"Error evaluating expression: {str(e)}"


# ---------------------------
# NEWS TOOL (using NewsAPI)
# ---------------------------
@tool
def news_tool(topic: str) -> str:
    """
    Retrieves news headlines for a given topic using the NewsAPI.

    Requires NEWSAPI_API_KEY to be set as an environment variable.

    :param topic: The news topic.
    :return: A summary string of news headlines.
    """
    api_key = os.environ.get("NEWSAPI_API_KEY")
    if not api_key:
        raise ValueError("NEWSAPI_API_KEY environment variable not set")
    params = {
        "q": topic,
        "apiKey": api_key,
        "pageSize": 5,
        "sortBy": "relevancy",
    }
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    if response.status_code != 200:
        return f"Error fetching news: {response.text}"
    data = response.json()
    results = []
    for article in data.get("articles", []):
        title = article.get("title", "No title")
        source = article.get("source", {}).get("name", "Unknown source")
        results.append(f"{title} ({source})")
    return "\n".join(results)

================
File: requirements.txt
================
langgraph>=0.0.20
langchain-anthropic>=0.0.10
langchain-core>=0.1.20
langchain-openai>=0.0.5
python-dotenv>=1.0.0
tavily-python>=0.5.1
langchain_community>=0.3.17
